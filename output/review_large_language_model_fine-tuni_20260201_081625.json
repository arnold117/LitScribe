{
  "research_question": "large language model fine-tuning",
  "search_results": {
    "query": "large language model fine-tuning",
    "expanded_queries": [
      "large language model fine-tuning",
      "parameter-efficient fine-tuning large language models",
      "instruction tuning reinforcement learning from human feedback",
      "LLM adaptation biomedical text generation",
      "neural network adaptation prompt engineering",
      "supervised fine-tuning domain adaptation pretrained transformers"
    ],
    "papers": [
      {
        "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models",
        "authors": [
          "Renxi Wang",
          "Haonan Li",
          "Minghao Wu",
          "Yuxia Wang",
          "Xudong Han",
          "Chiyu Zhang",
          "Timothy Baldwin"
        ],
        "abstract": "Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.",
        "year": 2023,
        "sources": {
          "arxiv": "2312.10793v3"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2312.10793v3"
        ],
        "relevance_score": 1.0,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2312.10793v3",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "keywords": [],
        "comment": "Instruction Tuning, Large Language Model, Alignment",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "Differentially Private Fine-tuning of Language Models",
        "authors": [
          "Da Yu",
          "Saurabh Naik",
          "Arturs Backurs",
          "Sivakanth Gopi",
          "Huseyin A. Inan",
          "Gautam Kamath",
          "Janardhan Kulkarni",
          "Yin Tat Lee",
          "Andre Manoel",
          "Lukas Wutschitz",
          "Sergey Yekhanin",
          "Huishuai Zhang"
        ],
        "abstract": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commonly studied datasets, the utility of private models approaches that of non-private models. For example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using RoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of $ε= 6.7$. In comparison, absent privacy constraints, RoBERTa-Large achieves an accuracy of $90.2\\%$. Our findings are similar for natural language generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8 respectively (privacy budget of $ε= 6.8,δ=$ 1e-5) whereas the non-private baseline is $48.1$. All our experiments suggest that larger models are better suited for private fine-tuning: while they are well known to achieve superior accuracy non-privately, we find that they also better maintain their accuracy when privacy is introduced.",
        "year": 2021,
        "sources": {
          "arxiv": "2110.06500v2"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2110.06500v2"
        ],
        "relevance_score": 0.9583333333333334,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2110.06500v2",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.LG",
          "cs.CL",
          "cs.CR",
          "stat.ML"
        ],
        "keywords": [],
        "comment": "ICLR 2022. Code available at https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
        "authors": [
          "Renxi Wang",
          "Haonan Li",
          "Xudong Han",
          "Yixuan Zhang",
          "Timothy Baldwin"
        ],
        "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agent-tunning scenarios. Our findings offer guidance for developing better agent-tuning methods and low-resource data usage techniques.",
        "year": 2024,
        "sources": {
          "arxiv": "2402.11651v2"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2402.11651v2"
        ],
        "relevance_score": 0.9166666666666666,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2402.11651v2",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL"
        ],
        "keywords": [],
        "comment": "Agent, LLM, Large Language Model",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
        "authors": [
          "Can Xu",
          "Qingfeng Sun",
          "Kai Zheng",
          "Xiubo Geng",
          "Pu Zhao",
          "Jiazhan Feng",
          "Chongyang Tao",
          "Qingwei Lin",
          "Daxin Jiang"
        ],
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
        "year": 2023,
        "sources": {
          "arxiv": "2304.12244v3"
        },
        "venue": "The Twelfth International Conference on Learning Representations (ICLR 2024)",
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2304.12244v3"
        ],
        "relevance_score": 0.8333333333333334,
        "completeness_score": 0.8,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2304.12244v3",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "keywords": [],
        "comment": "large language model, instruction fine-tune",
        "journal_ref": "The Twelfth International Conference on Learning Representations (ICLR 2024)",
        "url": null
      },
      {
        "title": "Fine-tuning with Very Large Dropout",
        "authors": [
          "Jianyu Zhang",
          "Léon Bottou"
        ],
        "abstract": "It is impossible today to pretend that the practice of machine learning is always compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.\n  This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods such as model soups.\n  This result has practical significance because the importance of the fine-tuning scenario has considerably grown in recent years. This result also provides interesting insights on the nature of rich representations and on the intrinsically linear nature of fine-tuning a large network using a comparatively small dataset.",
        "year": 2024,
        "sources": {
          "arxiv": "2403.00946v3"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2403.00946v3"
        ],
        "relevance_score": 0.875,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2403.00946v3",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.LG",
          "cs.CV"
        ],
        "keywords": [],
        "comment": "Fine-tuning with very large dropout outperforms weight-averaging and ensemble on ResNet and large vision transformer",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning",
        "authors": [
          "Chang Tian",
          "Matthew B. Blaschko",
          "Mingzhe Xing",
          "Xiuxing Li",
          "Yinliang Yue",
          "Marie-Francine Moens"
        ],
        "abstract": "Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs. We formally define and evaluate these challenging scenarios. We fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM) using RL with a representative policy-gradient algorithm and then test their performance on eight public datasets. Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities. Although we propose a scenario-specific remediation method, our results suggest current methods leave these reasoning deficits largely unresolved. This work highlights that the reasoning abilities of large models are often overstated and underscores the importance of evaluating models under non-ideal scenarios. The code and data will be released at XXXX.",
        "year": 2025,
        "sources": {
          "arxiv": "2508.04848v1"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2508.04848v1"
        ],
        "relevance_score": 0.7916666666666666,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2508.04848v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.AI"
        ],
        "keywords": [],
        "comment": "large language models, large vision-language model, reasoning, non-ideal conditions, reinforcement learning",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "authors": [
          "Rui Pan",
          "Xiang Liu",
          "Shizhe Diao",
          "Renjie Pi",
          "Jipeng Zhang",
          "Chi Han",
          "Tong Zhang"
        ],
        "abstract": "The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freezes most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "c739eb7f0302e85e935d1e2fdb903fe01b812804"
        },
        "venue": "Neural Information Processing Systems",
        "citations": 94,
        "pdf_urls": [],
        "relevance_score": 0.75,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2403.17919",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2403.17919",
        "scholar_id": "c739eb7f0302e85e935d1e2fdb903fe01b812804",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science",
          "Mathematics"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/c739eb7f0302e85e935d1e2fdb903fe01b812804"
      },
      {
        "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
        "authors": [
          "Teo Sušnjak",
          "Peter Hwang",
          "N. Reyes",
          "A. Barczak",
          "Timothy R. Mcintosh",
          "Surangika Ranathunga"
        ],
        "abstract": "This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "7dfbe4882188dbac938306be93bc58b34450b87f"
        },
        "venue": "ACM Transactions on Knowledge Discovery from Data",
        "citations": 64,
        "pdf_urls": [],
        "relevance_score": 0.7083333333333333,
        "completeness_score": 0.7,
        "doi": "10.1145/3715964",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2404.08680",
        "scholar_id": "7dfbe4882188dbac938306be93bc58b34450b87f",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/7dfbe4882188dbac938306be93bc58b34450b87f"
      },
      {
        "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
        "authors": [
          "Yixin Liu",
          "Avi Singh",
          "C. D. Freeman",
          "John D. Co-Reyes",
          "Peter J. Liu"
        ],
        "abstract": "Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "8868a6d452b06bf4ad33237d0f3952d895ca20e7"
        },
        "venue": "arXiv.org",
        "citations": 65,
        "pdf_urls": [],
        "relevance_score": 0.625,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2310.10047",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2310.10047",
        "scholar_id": "8868a6d452b06bf4ad33237d0f3952d895ca20e7",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/8868a6d452b06bf4ad33237d0f3952d895ca20e7"
      },
      {
        "title": "Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning",
        "authors": [
          "Wenke Huang",
          "Jian Liang",
          "Zekun Shi",
          "Didi Zhu",
          "Guancheng Wan",
          "He Li",
          "Bo Du",
          "Dacheng Tao",
          "Mang Ye"
        ],
        "abstract": "Multimodal Large Language Model (MLLM) have demonstrated strong generalization capabilities across diverse distributions and tasks, largely due to extensive pre-training datasets. Fine-tuning MLLM has become a common practice to improve performance on specific downstream tasks. However, during fine-tuning, MLLM often faces the risk of forgetting knowledge acquired during pre-training, which can result in a decline in generalization abilities. To balance the trade-off between generalization and specialization, we propose measuring the parameter importance for both pre-trained and fine-tuning distributions, based on frozen pre-trained weight magnitude and accumulated fine-tuning gradient values. We further apply an importance-aware weight allocation strategy, selectively updating relatively important parameters for downstream tasks. We conduct empirical evaluations on both image captioning and visual question-answering tasks using various MLLM architectures. The comprehensive experimental analysis demonstrates the effectiveness of the proposed solution, highlighting the efficiency of the crucial modules in enhancing downstream specialization performance while mitigating generalization degradation in MLLM Fine-Tuning.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "4c03c30f1d39c578ff6fa2a8de6913b387dd21fa"
        },
        "venue": "arXiv.org",
        "citations": 19,
        "pdf_urls": [],
        "relevance_score": 0.5833333333333333,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2411.10928",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2411.10928",
        "scholar_id": "4c03c30f1d39c578ff6fa2a8de6913b387dd21fa",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/4c03c30f1d39c578ff6fa2a8de6913b387dd21fa"
      },
      {
        "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
        "authors": [
          "Jiaru Zou",
          "Mengyu Zhou",
          "Tao Li",
          "Shi Han",
          "Dongmei Zhang"
        ],
        "abstract": "Recent advances in fine-tuning large language models (LLMs) have greatly enhanced their usage in domain-specific tasks. Despite the success, fine-tuning continues to rely on repeated and lengthy prompts, which escalate computational expenses, require more resources, and lead to slower inference. In this paper, we present a novel approach, PromptIntern, which internalizes prompt knowledge during model fine-tuning to achieve efficient inference and save costs. Instead of compressing the prompts for a vanilla model, PromptIntern aims to embed the recurrent prompt directly into the model parameters. We design a fine-tuning pipeline that includes instruction template compression, few-shot example absorption, and a progressive internalization strategy, effectively diminishing the need for intricate prompts during inference. Comprehensive experiments on challenging NL2Code tasks demonstrate that our method reduces input tokens by more than 90%, accelerates inference by 4.2 times, and reduces monetary inference costs by 88.3%.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "e7e35bf7e359535b75344e9ba7fb9c6224ffd77b"
        },
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citations": 19,
        "pdf_urls": [],
        "relevance_score": 0.5416666666666667,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2407.02211",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2407.02211",
        "scholar_id": "e7e35bf7e359535b75344e9ba7fb9c6224ffd77b",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/e7e35bf7e359535b75344e9ba7fb9c6224ffd77b"
      },
      {
        "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Model Fine-Tuning",
        "authors": [
          "Yuexiang Zhai",
          "Shengbang Tong",
          "Xiao Li",
          "Mu Cai",
          "Qing Qu",
          "Yong Jae Lee",
          "Yi Ma"
        ],
        "abstract": null,
        "year": 2024,
        "sources": {
          "semantic_scholar": "8a6580a0d894c05075fde1dbe3e1aede23d236f9"
        },
        "venue": "CPAL",
        "citations": 59,
        "pdf_urls": [],
        "relevance_score": 0.6666666666666667,
        "completeness_score": 0.4,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": null,
        "scholar_id": "8a6580a0d894c05075fde1dbe3e1aede23d236f9",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/8a6580a0d894c05075fde1dbe3e1aede23d236f9"
      },
      {
        "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
        "authors": [
          "Zhiqiang Hu",
          "Yihuai Lan",
          "Lei Wang",
          "Wanyu Xu",
          "Ee-Peng Lim",
          "Roy Ka-Wei Lee",
          "Lidong Bing",
          "Soujanya Poria"
        ],
        "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "bdb68c5e2369633b20e733774ac66eb4600c34d1"
        },
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citations": 390,
        "pdf_urls": [
          "https://arxiv.org/pdf/2304.01933"
        ],
        "relevance_score": 0.75,
        "completeness_score": 0.9,
        "doi": "10.48550/arXiv.2304.01933",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2304.01933",
        "scholar_id": "bdb68c5e2369633b20e733774ac66eb4600c34d1",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/bdb68c5e2369633b20e733774ac66eb4600c34d1"
      },
      {
        "title": "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models",
        "authors": [
          "Yiwen Tang",
          "Ray Zhang",
          "Zoey Guo",
          "Dong Wang",
          "Zhigang Wang",
          "Bin Zhao",
          "Xuelong Li"
        ],
        "abstract": "The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggregate point cloud features within spatial neighborhoods to capture fine-grained geometric information through local interactions. Extensive experiments indicate that our Point-PEFT can achieve better performance than the full fine-tuning on various downstream tasks, while using only 5% of the trainable parameters, demonstrating the efficiency and effectiveness of our approach. Code is released at https://github.com/Ivan-Tang-3D/Point-PEFT.",
        "year": 2023,
        "sources": {
          "arxiv": "2310.03059v8"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2310.03059v8"
        ],
        "relevance_score": 0.8333333333333334,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2310.03059v8",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "keywords": [],
        "comment": "The specialized PEFT framework for 3D pre-trained models, which achieves competitive performance to full fine-tuning, and significantly reduces the computational resources. Project page: https://github.com/Ivan-Tang-3D/Point-PEFT",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning",
        "authors": [
          "Jun Lu",
          "Lei Yu",
          "Xiaojia Li",
          "Li Yang",
          "Chun Zuo"
        ],
        "abstract": "The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "d955956378b40b23fa4b34098662c54a3b1fd64d"
        },
        "venue": "IEEE International Symposium on Software Reliability Engineering",
        "citations": 108,
        "pdf_urls": [
          "https://arxiv.org/pdf/2308.11148"
        ],
        "relevance_score": 0.6666666666666667,
        "completeness_score": 0.9,
        "doi": "10.1109/ISSRE59848.2023.00026",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2308.11148",
        "scholar_id": "d955956378b40b23fa4b34098662c54a3b1fd64d",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/d955956378b40b23fa4b34098662c54a3b1fd64d"
      },
      {
        "title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning",
        "authors": [
          "Zhaoxuan Tan",
          "Qingkai Zeng",
          "Yijun Tian",
          "Zheyuan Liu",
          "Bing Yin",
          "Meng Jiang"
        ],
        "abstract": "Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs’ interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU’s enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "72d0be6c3e6db156530c73d750cf436b0c983be4"
        },
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citations": 72,
        "pdf_urls": [],
        "relevance_score": 0.7083333333333333,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2402.04401",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2402.04401",
        "scholar_id": "72d0be6c3e6db156530c73d750cf436b0c983be4",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/72d0be6c3e6db156530c73d750cf436b0c983be4"
      },
      {
        "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models",
        "authors": [
          "Wei Ni",
          "Xuanjie Hu",
          "Yue Gao",
          "Zhe Chen",
          "Jun Luo",
          "Xianhao Chen",
          "Praneeth Vepakomma",
          "Yu-xin Zhang",
          "Zheng Lin",
          "Ang Li",
          "Zihan Fang"
        ],
        "abstract": "The scalability of large language models (LLMs) in handling high-complexity models and large-scale datasets has led to tremendous successes in pivotal domains. While there is an urgent need to acquire more training data for LLMs, a concerning reality is the depletion of high-quality public datasets within a few years. In view of this, the federated learning (FL) LLM fine-tuning paradigm recently has been proposed to facilitate collaborative LLM fine-tuning on distributed private data, where multiple data owners collaboratively fine-tune a shared LLM without sharing raw data. However, the staggering model size of LLMs imposes heavy computing and communication burdens on clients, posing significant barriers to the democratization of the FL LLM fine-tuning paradigm. To address this issue, split learning (SL) has emerged as a promising solution by offloading the primary training workload to a server via model partitioning while exchanging activation/activation's gradients with smaller data sizes rather than the entire LLM. Unfortunately, research on the SL LLM fine-tuning paradigm is still in its nascent stage. To fill this gap, in this paper, we propose the first SL LLM fine-tuning framework, named SplitLoRA. SplitLoRA is built on the split federated learning (SFL) framework, amalgamating the advantages of parallel training from FL and model splitting from SL and thus greatly enhancing the training efficiency. It is worth noting that SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning, providing a foundation for research efforts dedicated to advancing SL LLM fine-tuning. Extensive simulations validate that SplitLoRA achieves target accuracy in significantly less time than state-of-the-art LLM fine-tuning frameworks, demonstrating the superior training performance of SplitLoRA. The project page is available at https://fduinc.github.io/splitlora/.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "28660d983370b06442bf6cc856327f3278f53599"
        },
        "venue": "arXiv.org",
        "citations": 70,
        "pdf_urls": [],
        "relevance_score": 0.625,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2407.00952",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2407.00952",
        "scholar_id": "36f708fa17b9a096223d234565be16ad8ee83a35",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/36f708fa17b9a096223d234565be16ad8ee83a35"
      },
      {
        "title": "Parameter-efficient fine-tuning in large language models: a survey of methodologies",
        "authors": [
          "Luping Wang",
          "Sheng Chen",
          "Linnan Jiang",
          "Shu Pan",
          "Runze Cai",
          "Sen Yang",
          "Fei Yang"
        ],
        "abstract": "The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large language models require substantial computational resources and GPU memory to operate. When adapting large language models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, parameter-efficient fine-tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large language models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "d5c4c77fa4504873e7dad8f9e640b81a7161d5aa"
        },
        "venue": "Artificial Intelligence Review",
        "citations": 58,
        "pdf_urls": [],
        "relevance_score": 0.5416666666666667,
        "completeness_score": 0.7,
        "doi": "10.1007/s10462-025-11236-4",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2410.19878",
        "scholar_id": "d5c4c77fa4504873e7dad8f9e640b81a7161d5aa",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/d5c4c77fa4504873e7dad8f9e640b81a7161d5aa"
      },
      {
        "title": "RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback",
        "authors": [
          "Yannick Metz",
          "David Lindner",
          "Raphaël Baur",
          "Daniel Keim",
          "Mennatallah El-Assady"
        ],
        "abstract": "To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RLHF-Blender. More information is available at https://rlhfblender.info/.",
        "year": 2023,
        "sources": {
          "arxiv": "2308.04332v1"
        },
        "venue": "ICML2023 Interactive Learning from Implicit Human Feedback Workshop",
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2308.04332v1"
        ],
        "relevance_score": 1.0,
        "completeness_score": 0.8,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2308.04332v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.LG",
          "cs.HC"
        ],
        "keywords": [],
        "comment": "14 pages, 3 figures",
        "journal_ref": "ICML2023 Interactive Learning from Implicit Human Feedback Workshop",
        "url": null
      },
      {
        "title": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values, Whose Perspectives?",
        "authors": [
          "Kristian González Barman",
          "Simon Lohse",
          "Henk de Regt"
        ],
        "abstract": "We argue for the epistemic and ethical advantages of pluralism in Reinforcement Learning from Human Feedback (RLHF) in the context of Large Language Models (LLM). Drawing on social epistemology and pluralist philosophy of science, we suggest ways in which RHLF can be made more responsive to human needs and how we can address challenges along the way. The paper concludes with an agenda for change, i.e. concrete, actionable steps to improve LLM development.",
        "year": 2024,
        "sources": {
          "arxiv": "2407.17482v2"
        },
        "venue": "González Barman, K., Lohse, S. & de Regt, H.W. Reinforcement Learning from Human Feedback in LLMs: Whose Culture, Whose Values, Whose Perspectives?. Philos. Technol. 38, 35 (2025)",
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2407.17482v2"
        ],
        "relevance_score": 0.875,
        "completeness_score": 0.9,
        "doi": "10.1007/s13347-025-00861-0",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2407.17482v2",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CY",
          "cs.AI",
          "cs.CL",
          "cs.HC"
        ],
        "keywords": [],
        "comment": null,
        "journal_ref": "González Barman, K., Lohse, S. & de Regt, H.W. Reinforcement Learning from Human Feedback in LLMs: Whose Culture, Whose Values, Whose Perspectives?. Philos. Technol. 38, 35 (2025)",
        "url": null
      },
      {
        "title": "ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning",
        "authors": [
          "Jannis Becktepe",
          "Julian Dierkes",
          "Carolin Benjamins",
          "Aditya Mohan",
          "David Salinas",
          "Raghu Rajan",
          "Frank Hutter",
          "Holger Hoos",
          "Marius Lindauer",
          "Theresa Eimer"
        ],
        "abstract": "Hyperparameters are a critical factor in reliably training well-performing reinforcement learning (RL) agents. Unfortunately, developing and evaluating automated approaches for tuning such hyperparameters is both costly and time-consuming. As a result, such approaches are often only evaluated on a single domain or algorithm, making comparisons difficult and limiting insights into their generalizability. We propose ARLBench, a benchmark for hyperparameter optimization (HPO) in RL that allows comparisons of diverse HPO approaches while being highly efficient in evaluation. To enable research into HPO in RL, even in settings with low compute resources, we select a representative subset of HPO tasks spanning a variety of algorithm and environment combinations. This selection allows for generating a performance profile of an automated RL (AutoRL) method using only a fraction of the compute previously necessary, enabling a broader range of researchers to work on HPO in RL. With the extensive and large-scale dataset on hyperparameter landscapes that our selection is based on, ARLBench is an efficient, flexible, and future-oriented foundation for research on AutoRL. Both the benchmark and the dataset are available at https://github.com/automl/arlbench.",
        "year": 2024,
        "sources": {
          "arxiv": "2409.18827v1"
        },
        "venue": "17th European Workshop on Reinforcement Learning 2024",
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2409.18827v1"
        ],
        "relevance_score": 0.9166666666666666,
        "completeness_score": 0.8,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2409.18827v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.LG"
        ],
        "keywords": [],
        "comment": "Accepted at the 17th European Workshop on Reinforcement Learning",
        "journal_ref": "17th European Workshop on Reinforcement Learning 2024",
        "url": null
      },
      {
        "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
        "authors": [
          "Zhiyang Xu",
          "Chao Feng",
          "Rulin Shao",
          "Trevor Ashby",
          "Ying Shen",
          "Di Jin",
          "Yu Cheng",
          "Qifan Wang",
          "Lifu Huang"
        ],
        "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.",
        "year": 2024,
        "sources": {
          "arxiv": "2402.11690v1"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2402.11690v1"
        ],
        "relevance_score": 0.9583333333333334,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2402.11690v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "cs.CV"
        ],
        "keywords": [],
        "comment": "8 Pages, visual instruction tuning",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
        "authors": [
          "Viet Dac Lai",
          "C. Nguyen",
          "Nghia Trung Ngo",
          "Thuat Nguyen",
          "Franck Dernoncourt",
          "Ryan A. Rossi",
          "Thien Huu Nguyen"
        ],
        "abstract": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "fc84f5b58e68871f3d6889dc2a93dffa7e107be2"
        },
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citations": 207,
        "pdf_urls": [
          "https://arxiv.org/pdf/2307.16039"
        ],
        "relevance_score": 0.7083333333333333,
        "completeness_score": 0.9,
        "doi": "10.48550/arXiv.2307.16039",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2307.16039",
        "scholar_id": "fc84f5b58e68871f3d6889dc2a93dffa7e107be2",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/fc84f5b58e68871f3d6889dc2a93dffa7e107be2"
      },
      {
        "title": "Directed Policy Gradient for Safe Reinforcement Learning with Human Advice",
        "authors": [
          "Hélène Plisnier",
          "Denis Steckelmacher",
          "Tim Brys",
          "Diederik M. Roijers",
          "Ann Nowé"
        ],
        "abstract": "Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.",
        "year": 2018,
        "sources": {
          "arxiv": "1808.04096v1"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/1808.04096v1"
        ],
        "relevance_score": 0.7916666666666666,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "1808.04096v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "keywords": [],
        "comment": "Accepted at the European Workshop on Reinforcement Learning 2018 (EWRL14)",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "Reinforcement Learning from Human Feedback",
        "authors": [
          "Nathan Lambert"
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some level of quantitative background. The book starts with the origins of RLHF -- both in recent literature and in a convergence of disparate fields of science in economics, philosophy, and optimal control. We then set the stage with definitions, problem formulation, data collection, and other common math used in the literature. The core of the book details every optimization stage in using RLHF, from starting with instruction tuning to training a reward model and finally all of rejection sampling, reinforcement learning, and direct alignment algorithms. The book concludes with advanced topics -- understudied research questions in synthetic data and evaluation -- and open questions for the field.",
        "year": 2025,
        "sources": {
          "semantic_scholar": "18dc78d3f247f75aafca5422fe540f20b3cd455d"
        },
        "venue": "arXiv.org",
        "citations": 60,
        "pdf_urls": [],
        "relevance_score": 0.75,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2504.12501",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2504.12501",
        "scholar_id": "18dc78d3f247f75aafca5422fe540f20b3cd455d",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/18dc78d3f247f75aafca5422fe540f20b3cd455d"
      },
      {
        "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning",
        "authors": [
          "Yufei Zhan",
          "Yousong Zhu",
          "Shurong Zheng",
          "Hongyin Zhao",
          "Fan Yang",
          "Ming Tang",
          "Jinqiao Wang"
        ],
        "abstract": "Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model.",
        "year": 2025,
        "sources": {
          "semantic_scholar": "1d9430bae9b3ac93ad8e0955e2a5d57745a91ccf"
        },
        "venue": "arXiv.org",
        "citations": 53,
        "pdf_urls": [],
        "relevance_score": 0.625,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2503.18013",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2503.18013",
        "scholar_id": "1d9430bae9b3ac93ad8e0955e2a5d57745a91ccf",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/1d9430bae9b3ac93ad8e0955e2a5d57745a91ccf"
      },
      {
        "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback",
        "authors": [
          "Aniruddha Roy",
          "Pretam Ray",
          "Abhilash Nandy",
          "Somak Aditya",
          "Pawan Goyal"
        ],
        "abstract": "Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.",
        "year": 2025,
        "sources": {
          "semantic_scholar": "ed49718f59d9854668b4b39bfe39e56f4da44a0b"
        },
        "venue": "arXiv.org",
        "citations": 0,
        "pdf_urls": [],
        "relevance_score": 0.6666666666666667,
        "completeness_score": 0.6,
        "doi": "10.48550/arXiv.2505.06548",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2505.06548",
        "scholar_id": "ed49718f59d9854668b4b39bfe39e56f4da44a0b",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/ed49718f59d9854668b4b39bfe39e56f4da44a0b"
      },
      {
        "title": "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback",
        "authors": [
          "Fatemeh Pesaran Zadeh",
          "Juyeon Kim",
          "Jin-Hwa Kim",
          "Gunhee Kim"
        ],
        "abstract": "Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "fab9078698495dc2d5fdbce955a0970a34f7f7be"
        },
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citations": 13,
        "pdf_urls": [],
        "relevance_score": 0.5833333333333333,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2410.04064",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2410.04064",
        "scholar_id": "fab9078698495dc2d5fdbce955a0970a34f7f7be",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/fab9078698495dc2d5fdbce955a0970a34f7f7be"
      },
      {
        "title": "AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection",
        "authors": [
          "Arezo Bodaghi",
          "Benjamin C. M. Fung",
          "Ketra A. Schmitt"
        ],
        "abstract": "Addressing the challenge of toxic language in online discussions is crucial for the development of effective toxicity detection models. This pioneering work focuses on addressing imbalanced datasets in toxicity detection by introducing a novel approach to augment toxic language data. We create a balanced dataset by instructing fine-tuning of Large Language Models (LLMs) using Reinforcement Learning with Human Feedback (RLHF). Recognizing the challenges in collecting sufficient toxic samples from social media platforms for building a balanced dataset, our methodology involves sentence-level text data augmentation through paraphrasing existing samples using optimized generative LLMs. Leveraging generative LLM, we utilize the Proximal Policy Optimizer (PPO) as the RL algorithm to fine-tune the model further and align it with human feedback. In other words, we start by fine-tuning a LLM using an instruction dataset, specifically tailored for the task of paraphrasing while maintaining semantic consistency. Next, we apply PPO and a reward function, to further fine-tune (optimize) the instruction-tuned LLM. This RL process guides the model in generating toxic responses. We utilize the Google Perspective API as a toxicity evaluator to assess generated responses and assign rewards/penalties accordingly. This approach guides LLMs through PPO and the reward function, transforming minority class samples into augmented versions. The primary goal of our methodology is to create a balanced and diverse dataset to enhance the accuracy and performance of classifiers in identifying instances from the minority class. Utilizing two publicly available toxic datasets, we compared various techniques with our proposed method for generating toxic samples, demonstrating that our approach outperforms all others in producing a higher number of toxic samples. Starting with an initial 16,225 toxic prompts, our method successfully generated 122,951 toxic samples with a toxicity score exceeding 30%. Subsequently, we developed various classifiers using the generated balanced datasets and applied a cost-sensitive learning approach to the original imbalanced dataset. The findings highlight the superior performance of classifiers trained on data generated using our proposed method. These results highlight the importance of employing RL and a data-agnostic model as a reward mechanism for augmenting toxic data, thereby enhancing the robustness of toxicity detection models.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "b662c5f269ab64f164efcd6a4f6f9aae693f16ff"
        },
        "venue": "ACM Transactions on the Web",
        "citations": 5,
        "pdf_urls": [],
        "relevance_score": 0.5416666666666667,
        "completeness_score": 0.6,
        "doi": "10.1145/3700791",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": null,
        "scholar_id": "b662c5f269ab64f164efcd6a4f6f9aae693f16ff",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/b662c5f269ab64f164efcd6a4f6f9aae693f16ff"
      }
    ],
    "source_counts": {
      "arxiv": 0,
      "semantic_scholar": 0
    },
    "total_found": 29,
    "search_timestamp": "2026-02-01T08:16:48.926561"
  },
  "analyzed_papers": [
    {
      "paper_id": "2304.01933",
      "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
      "authors": [
        "Zhiqiang Hu",
        "Yihuai Lan",
        "Lei Wang",
        "Wanyu Xu",
        "Ee-Peng Lim",
        "Roy Ka-Wei Lee",
        "Lidong Bing",
        "Soujanya Poria"
      ],
      "year": 2023,
      "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization",
      "key_findings": [
        "Finding 1: The optimal placement for adapter modules varies by type: series adapters perform best after MLP layers, parallel adapters work best parallel to MLP layers, and LoRA (a reparameterization method) achieves optimal performance when placed after both Attention and MLP layers simultaneously.",
        "Finding 2: Smaller language models (7B-13B parameters) with parameter-efficient fine-tuning (PEFT) can achieve comparable or superior performance to much larger models (175B+ parameters) on specific reasoning tasks, with LLaMA-13B+LoRA outperforming GPT-3.5 (>175B) on three arithmetic reasoning datasets (MultiArith, AddSub, SingleEq).",
        "Finding 3: In-distribution fine-tuned LLaMA-13B with adapters outperforms ChatGPT on commonsense reasoning tasks, demonstrating that smaller models can surpass larger ones on specific tasks when fine-tuned with relevant data.",
        "Finding 4: The framework successfully integrates multiple PEFT methods (Series adapters, Parallel adapters, Prompt-based learning, Reparametrization-based methods) with three major open-source LLMs (LLaMA, BLOOM, GPT-J), enabling systematic comparison across 14 datasets from two reasoning domains."
      ],
      "methodology": "Study design and approach: The paper employs an empirical research design focused on systematically evaluating different parameter-efficient fine-tuning (PEFT) methods applied to large language models. The study investigates three main research questions regarding optimal adapter placement, performance across tasks, and in-distribution vs. out-of-distribution performance. The authors developed the LLM-Adapters framework as both a research tool and contribution, enabling integration of various adapters with multiple LLMs. The experimental design involves controlled comparisons of adapter types, placement locations, and hyperparameters across consistent task sets.\n\nData collection methods and sources: The study uses fourteen datasets from two reasoning domains: Arithmetic Reasoning (including GSM8K, MultiArith, AddSub, SingleEq, AQuA, SVAMP) and Commonsense Reasoning (including CommonsenseQA, OpenBookQA, ARC-Easy, ARC-Challenge, BoolQ, PIQA, SIQA, HellaSwag). The authors mention constructing 'two high-quality training datasets' to enhance PEFT performance in these reasoning tasks, though specific details about these constructed datasets are not provided in the abstract and introduction sections. The models evaluated include open-source LLMs (LLaMA 7B/13B, BLOOM, GPT-J) and comparisons are made against proprietary models (GPT-3.5, ChatGPT).\n\nAnalysis techniques and key parameters: The analysis involves systematic experimentation with key parameters including adapter type (Series, Parallel, Prompt-based, Reparametrization-based), placement location (after Attention layers, after MLP layers, parallel to MLP layers, or combinations), and hyperparameter configurations. Performance is evaluated using task-specific metrics on reasoning datasets. The study compares ID (in-distribution) versus OOD (out-of-distribution) scenarios and analyzes the parameter efficiency (number of trainable parameters) versus performance trade-offs. The framework enables standardized comparison across different adapter-LLM combinations.",
      "strengths": [
        "Comprehensive empirical study covering multiple adapter types, placement strategies, and model architectures across diverse reasoning tasks",
        "Development of a practical, open-source framework (LLM-Adapters) that facilitates reproducible research and application of PEFT methods",
        "Important demonstration that parameter-efficient methods enable smaller models to compete with much larger models, potentially democratizing access to LLM capabilities",
        "Systematic investigation of design choices (placement, hyperparameters) that provides actionable insights for practitioners implementing PEFT"
      ],
      "limitations": [
        "Limited task diversity - only evaluates reasoning tasks (arithmetic and commonsense) without covering other important NLP domains like generation, translation, or summarization",
        "Incomplete methodological details in provided sections - missing information about the constructed training datasets, specific hyperparameter settings, and evaluation protocols",
        "Potential selection bias in adapter methods - focuses on established methods but may miss emerging or hybrid approaches",
        "Lack of theoretical analysis - the paper is primarily empirical without exploring why certain placements work better or the theoretical limits of PEFT approaches"
      ],
      "relevance_score": 0.75,
      "citations": 390,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2307.16039",
      "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
      "authors": [
        "Viet Dac Lai",
        "C. Nguyen",
        "Nghia Trung Ngo",
        "Thuat Nguyen",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Thien Huu Nguyen"
      ],
      "year": 2023,
      "abstract": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef",
      "key_findings": [
        "Finding 1: The Okapi framework successfully implements the first RLHF-based instruction tuning pipeline for open-source LLMs across 26 diverse languages, demonstrating the technical feasibility of extending RLHF beyond English-centric models.",
        "Finding 2: RLHF-based instruction tuning consistently outperforms supervised fine-tuning (SFT) alone across multiple base models and datasets in multilingual settings, indicating that RLHF provides significant performance benefits even for lower-resource languages.",
        "Finding 3: The authors created and released substantial new resources including: 106K additional English instructions generated via Self-Instruct, translations of instructions into 26 languages via ChatGPT, and response-ranked datasets for RLHF training in all supported languages.",
        "Finding 4: The study introduces new benchmark datasets specifically designed for evaluating generative LLMs in multiple languages, addressing a critical gap in multilingual LLM evaluation methodology."
      ],
      "methodology": "The study employs a multi-stage experimental design to develop and evaluate RLHF-based instruction tuning for multilingual LLMs. First, the authors expanded the Alpaca dataset by generating 106K additional English instructions using Self-Instruct methodology. They then used ChatGPT to translate these instructions into 26 diverse languages, employing specialized prompts to handle programming code and maintain translation quality. For RLHF implementation, they followed the standard three-stage pipeline: supervised fine-tuning (SFT) on instruction-response pairs, training a reward model on human preference data (simulated via ChatGPT rankings), and proximal policy optimization (PPO) reinforcement learning.\n\nData collection involved both generation and curation approaches. The primary source was the Alpaca dataset (52K instructions), which was expanded using Self-Instruct. For multilingual adaptation, ChatGPT served as the translation engine with careful prompt engineering to preserve instruction semantics across languages. Response ranking data for RLHF training was generated by having ChatGPT rank multiple model outputs for the same instruction, simulating human preference judgments. The 26 languages were selected to include both high-resource and lower-resource languages across different language families.\n\nAnalysis techniques included comparative evaluation between SFT-only and RLHF-tuned models across multiple base architectures. The authors employed both automatic metrics (likely including BLEU, ROUGE, or similar generation metrics) and human evaluation where feasible. Key experimental parameters included testing different base models (though specific models aren't detailed in the provided text), varying dataset sizes, and comparing performance across language groups. The evaluation focused on instruction-following capability, response quality, and alignment with human preferences.",
      "strengths": [
        "Addresses a significant research gap by being the first to implement RLHF for multilingual instruction tuning of open-source LLMs",
        "Creates and releases valuable resources including datasets, benchmarks, and code framework that will facilitate future research",
        "Employs a comprehensive methodology that combines SFT, reward modeling, and PPO in a complete RLHF pipeline",
        "Focuses on language diversity by including 26 languages rather than just high-resource languages"
      ],
      "limitations": [
        "Relies heavily on ChatGPT for both translation and preference ranking, introducing potential biases from a single proprietary model's capabilities and limitations",
        "Limited discussion of the specific evaluation metrics and statistical significance of reported improvements",
        "Does not address potential issues with translation quality for low-resource languages or specialized domains like programming code",
        "The paper provides insufficient details about the base models used, dataset sizes per language, and computational requirements"
      ],
      "relevance_score": 0.7083333333333333,
      "citations": 207,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2308.11148",
      "title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning",
      "authors": [
        "Jun Lu",
        "Lei Yu",
        "Xiaojia Li",
        "Li Yang",
        "Chun Zuo"
      ],
      "year": 2023,
      "abstract": "The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smal",
      "key_findings": [
        "Finding 1: The LLaMA-Reviewer framework, using a 6.7B parameter LLaMA base model with parameter-efficient fine-tuning (PEFT), achieved performance comparable to existing domain-specific code review models while training less than 1% of the total parameters.",
        "Finding 2: The study demonstrates that unified Large Language Models (LLMs), primarily pre-trained on natural language, can be effectively adapted for specialized software engineering tasks like code review without requiring resource-intensive domain-specific pre-training from scratch.",
        "Finding 3: The proposed 'unified model + PEFT' paradigm offers a computationally efficient alternative for code review automation, with the PEFT-weight plugins optimizing storage requirements, a claimed first application of this paradigm in the software engineering domain.",
        "Finding 4: Ablation experiments provided insights into the impact of fine-tuning components, including input representation and instruction tuning, on the model's performance for the three core code review tasks: necessity prediction, comment generation, and code refinement."
      ],
      "methodology": "The study employed an experimental design to develop and evaluate the LLaMA-Reviewer framework. The approach involved fine-tuning the LLaMA large language model using Parameter-Efficient Fine-Tuning (PEFT) methods, specifically LoRA (Low-Rank Adaptation) and prefix-tuning, to adapt it to a pipeline of three automated code review tasks derived from prior work. The framework was built upon an existing pipeline that segments the code review process into review necessity prediction, review comment generation, and code refinement.\n\nData collection involved using two diverse, publicly available datasets for each of the three sub-tasks. The paper references specific datasets but does not name them in the provided text. The data presumably consists of code diffs, review comments, and refined code snippets from real-world software projects, following the modern code review (MCR) cycle involving committers and reviewers.\n\nAnalysis techniques included an extensive evaluation comparing LLaMA-Reviewer's performance against existing code-review-focused models (like CodeT5 and PLBART) on the three tasks. Key experimental parameters included using the smallest LLaMA base model (6.7B parameters), a limited number of tuning epochs, and PEFT methods that train less than 1% of parameters. The study also conducted ablation experiments to analyze the influence of input representation, instruction tuning, and the choice of PEFT method on the final results.",
      "strengths": [
        "Addresses a significant resource challenge by applying Parameter-Efficient Fine-Tuning (PEFT) to large language models for a software engineering task, making advanced LLM capabilities more accessible by drastically reducing computational and storage demands.",
        "Provides a comprehensive evaluation framework covering three core code review tasks (prediction, generation, refinement) on multiple datasets, and includes valuable ablation studies to dissect the contribution of different fine-tuning components.",
        "Offers practical and open contributions by releasing code, models, and PEFT-weight plugins, which promotes reproducibility, allows for direct comparison by other researchers, and enables practical application and extension of the work.",
        "Identifies and explores a relevant research gap by systematically investigating the under-explored potential of unified LLMs (vs. domain-specific PLMs) for code review automation, proposing a novel paradigm for the field."
      ],
      "limitations": [
        "The provided text lacks specific, quantitative results and detailed comparisons with baseline models, making it impossible to critically assess the claimed performance parity. Metrics, dataset specifics, and statistical significance of results are not presented.",
        "Potential validity threats (mentioned in Section VII) are not detailed in the excerpt, but common threats in such work could include dataset bias (e.g., language, project domain), the generalizability of the PEFT approach to other LLMs or tasks, and the quality evaluation of generated comments/code.",
        "The reliance on the pipeline from prior work (Li et al. 2022) may inherit its limitations, and the paper does not discuss the potential for error propagation between the sequential tasks (prediction -> generation -> refinement) within the automated cycle.",
        "While promoting an offline, privacy-conscious alternative to API-based LLMs is a strength, the paper does not compare the performance, cost, or ease of use of LLaMA-Reviewer against state-of-the-art closed-source LLMs (e.g., GPT-4) applied to the same tasks."
      ],
      "relevance_score": 0.6666666666666667,
      "citations": 108,
      "venue": "IEEE International Symposium on Software Reliability Engineering",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2403.17919",
      "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
      "authors": [
        "Rui Pan",
        "Xiang Liu",
        "Shizhe Diao",
        "Renjie Pi",
        "Jipeng Zhang",
        "Chi Han",
        "Tong Zhang"
      ],
      "year": 2024,
      "abstract": "The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is dis",
      "key_findings": [
        "Finding 1: The authors discovered a consistent skewness in the layerwise weight norms during LoRA fine-tuning, where the bottom and/or top layers of the model account for the majority of the weight update magnitude, while middle layers contribute minimally.",
        "Finding 2: The proposed LISA method, which applies importance sampling to layers by randomly freezing most middle layers during optimization, achieves superior performance to both LoRA and full parameter fine-tuning (FT) on multiple benchmarks while maintaining memory costs comparable to LoRA.",
        "Finding 3: LISA consistently outperforms LoRA by 10%-35% on the MT-Bench evaluation for instruction-following tasks, while achieving on-par or better performance on knowledge-based (MMLU, AGIEval) and reasoning (WinoGrande) benchmarks.",
        "Finding 4: The performance advantage of LISA over LoRA scales to very large models (e.g., LLaMA-2-70B), demonstrating effectiveness across domains including instruction following (MT-Bench), math (GSM8K), and biomedical QA (PubMedQA).",
        "Finding 5: LISA exhibits better convergence behavior than LoRA, GaLore, and Full Fine-Tuning (FT) on the Alpaca GPT-4 dataset, as evidenced by lower and more stable training loss curves presented in the paper."
      ],
      "methodology": "Study design and approach: The paper follows an empirical research design centered on the observation of a phenomenon (skewed layerwise updates in LoRA) and the subsequent proposal and evaluation of a novel optimization method (LISA). The core approach involves analyzing training dynamics of existing PEFT methods, formulating a hypothesis about layer importance, and designing a training strategy that selectively updates layers based on a sampling probability derived from observed update magnitudes. The evaluation is comparative, pitting LISA against established baselines (Full Fine-Tuning, LoRA, and GaLore) across models of varying scales (7B to 70B parameters) and diverse task domains.\n\nData collection methods and sources: The authors utilize standard, publicly available pre-trained LLMs (LLaMA-2-7B/13B/70B) as base models. For fine-tuning, they employ established instruction-tuning datasets (Alpaca GPT-4) and task-specific datasets relevant to the evaluated benchmarks. Evaluation is performed on a comprehensive suite of standard LLM benchmarks: MT-Bench (instruction following/chat), MMLU and AGIEval (knowledge & reasoning), WinoGrande (commonsense reasoning), GSM8K (math), and PubMedQA (biomedical QA). This reliance on public models and benchmarks ensures reproducibility and facilitates direct comparison with prior work.\n\nAnalysis techniques and key parameters: The primary analysis technique is the measurement and comparison of layerwise Frobenius norms of the weight updates (ΔW) during LoRA training to identify the skewness pattern. Based on this, LISA is implemented by defining a layer sampling probability p_l proportional to the norm of its update, ||ΔW_l||_F. During training, only a subset of layers (K) is activated for updates per iteration, with others frozen. Key hyperparameters include the total number of layers to update (K) and the specific formula for converting update norms to sampling probabilities. Performance is analyzed via quantitative metrics on the aforementioned benchmarks and qualitative analysis of training loss curves.",
      "strengths": [
        "The work is built on a clear, empirical observation (skewed layerwise updates) that is both novel and intuitive, providing a strong motivational foundation for the proposed method.",
        "The proposed LISA algorithm is conceptually simple and easy to implement, requiring minimal modification to existing training loops, which enhances its practical utility and adoption potential.",
        "Evaluation is extensive and rigorous, covering multiple model sizes (7B, 13B, 70B) and a diverse set of benchmarks spanning different capabilities (knowledge, reasoning, instruction following, domain-specific QA).",
        "The paper demonstrates a significant and consistent performance improvement over a strong, widely-used baseline (LoRA) while maintaining its core advantage (low memory footprint), positioning LISA as a compelling alternative."
      ],
      "limitations": [
        "The paper provides limited theoretical justification or analysis for why the observed layerwise skewness occurs or why the proposed importance sampling scheme is optimal. The connection between update magnitude and layer 'importance' for final task performance is asserted but not deeply analyzed.",
        "The computational cost (FLOPs, time per iteration) of LISA compared to LoRA is not discussed. While memory usage is similar, the overhead of calculating layerwise norms and the potential inefficiency of freezing/unfreezing layers could impact total training time.",
        "The exploration of the hyperparameter space for LISA (e.g., the choice of K, the exact probability calculation) seems preliminary. A more systematic ablation study would strengthen the understanding of the method's sensitivity and robustness.",
        "The comparison to full fine-tuning (FT) is somewhat ambiguous. The abstract claims LISA outperforms FT 'in a wide range of settings,' but the evidence in the provided text primarily shows superior loss convergence and mentions outperforming FT 'under certain settings.' A clearer delineation of when LISA surpasses FT is needed."
      ],
      "relevance_score": 0.75,
      "citations": 94,
      "venue": "Neural Information Processing Systems",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2402.04401",
      "title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning",
      "authors": [
        "Zhaoxuan Tan",
        "Qingkai Zeng",
        "Yijun Tian",
        "Zheyuan Liu",
        "Bing Yin",
        "Meng Jiang"
      ],
      "year": 2024,
      "abstract": "Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs’ interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior",
      "key_findings": [
        "Finding 1: The OPPU framework, which assigns each user a personalized Parameter-Efficient Fine-Tuning (PEFT) module, significantly outperforms existing prompt-based personalization methods across all seven diverse tasks in the LaMP benchmark, demonstrating superior personalization capabilities.",
        "Finding 2: OPPU exhibits enhanced robustness to user behavior shifts, where a user's current query may not be directly relevant to their historical data, by leveraging parametric knowledge stored in the PEFT module rather than relying solely on non-parametric retrieval from history.",
        "Finding 3: The framework successfully decouples user-specific parametric knowledge (stored in the PEFT module) from non-parametric knowledge (retrieved from user history), allowing for a more flexible and powerful integration that addresses the limitations of retrieval-only methods.",
        "Finding 4: OPPU maintains strong performance across users with varying activity levels (high vs. low history data) and is robust to different formats of user history data, indicating its generalizability across diverse personalization scenarios.",
        "Finding 5: The approach enables practical model ownership and privacy for individual users, as each user possesses and controls their own lightweight PEFT module (typically <1% of base LLM parameters), which can be 'plugged in' to a shared base model."
      ],
      "methodology": "The study employs a comprehensive experimental design centered on the LaMP (Language Model Personalization) benchmark, which consists of seven diverse text generation tasks requiring user-specific adaptation (e.g., personalized tweet generation, product recommendation, email subject generation). The core approach, OPPU (One PEFT Per User), involves a two-stage process. First, a base LLM (Flan-T5) is task-adapted via fine-tuning on held-out user data to establish general task proficiency. Second, for each target user, a personalized PEFT module (using methods like LoRA or Adapter) is fine-tuned exclusively on that user's behavior history. During inference, the user's personal PEFT parameters are integrated with the base LLM, and the input is optionally augmented with retrieved relevant snippets from the user's history.\n\nData collection and sources are derived entirely from the publicly available LaMP benchmark. This benchmark provides standardized datasets for the seven personalization tasks, each containing user behavior histories (e.g., past tweets, reviewed products, sent emails) and corresponding queries for evaluation. The user history formats vary by task, including query-answer pairs and plain text sequences, providing a testbed for robustness. The benchmark ensures a controlled comparison against established baselines, which include vanilla prompting, retrieval-augmented prompting, and profile-augmented prompting methods.\n\nAnalysis techniques involve quantitative evaluation using standard NLP metrics (e.g., ROUGE, BERTScore) as defined by the LaMP benchmark for each task. Key experimental parameters and analyses include: 1) Ablation studies to isolate the contribution of the PEFT module versus retrieval augmentation, 2) Analysis of performance under simulated 'behavior shift' by manipulating the relevance of retrieved history, 3) Evaluation across user groups segmented by activity level (amount of history data), 4) Testing robustness to different history data formats, and 5) Exploring the versatility of OPPU with different underlying PEFT methods (LoRA, Adapter, Prompt Tuning). Statistical significance of improvements over baselines is reported.",
      "strengths": [
        "Addresses a critical and timely problem: The paper tackles two significant limitations of current LLM personalization—lack of user ownership/privacy and poor handling of behavior shifts—with a novel and practical solution.",
        "Rigorous and comprehensive evaluation: The use of the standardized LaMP benchmark across seven diverse tasks provides strong, comparable evidence for the framework's effectiveness and generalizability beyond a single domain.",
        "Clear conceptual contribution: The paper clearly articulates and validates the value of decoupling parametric (PEFT) and non-parametric (retrieval) user knowledge, offering a new paradigm for personalization architecture.",
        "Practical and scalable design: Leveraging parameter-efficient fine-tuning makes the approach computationally feasible for widespread deployment, as storing and switching per-user modules is lightweight compared to full model copies."
      ],
      "limitations": [
        "Limited exploration of long-term adaptation: The paper evaluates on static benchmark splits but does not deeply study how the PEFT modules are updated incrementally over time as user behavior continuously evolves, which is crucial for real-world deployment.",
        "Potential cold-start problem: While performance for users with low activity is evaluated, the method fundamentally requires some user history for PEFT training. The framework's strategy for completely new users (zero history) is not discussed in detail.",
        "Computational cost during training phase: Although inference is efficient, the need to fine-tune a separate PEFT module for every user (even if lightweight) requires significant upfront computational resources and orchestration for a large user base, which is acknowledged but not fully cost-analyzed.",
        "Contextual limitations of the benchmark: The LaMP tasks, while diverse, may not fully capture the complexity of open-ended, multi-turn conversational personalization or the nuances of highly dynamic preferences. The evaluation is confined to the tasks and metrics of the benchmark."
      ],
      "relevance_score": 0.7083333333333333,
      "citations": 72,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "pdf_available": true,
      "source": "unknown"
    }
  ],
  "knowledge_graph": {
    "entities": {
      "efceda28921b": {
        "entity_id": "efceda28921b",
        "name": "LLM-Adapters",
        "entity_type": "method",
        "aliases": [
          "LLM-Adapters framework"
        ],
        "description": "An easy-to-use framework that integrates various adapters into large language models for parameter-efficient fine-tuning.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "f3c98857888b": {
        "entity_id": "f3c98857888b",
        "name": "Parameter-Efficient Fine-Tuning",
        "entity_type": "method",
        "aliases": [
          "Parameter Efficient Fine-Tuning",
          "PEFT"
        ],
        "description": "A fine-tuning approach that only requires training a small number of external parameters instead of the entire model.",
        "paper_ids": [
          "2308.11148",
          "2304.01933",
          "2403.17919",
          "2402.04401"
        ],
        "frequency": 4
      },
      "69d40ae1a31a": {
        "entity_id": "69d40ae1a31a",
        "name": "Adapter-based Fine-Tuning",
        "entity_type": "method",
        "aliases": [
          "Adapter-based PEFT"
        ],
        "description": "A parameter-efficient fine-tuning method that adds small adapter modules to a pre-trained model.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "97e3689ac212": {
        "entity_id": "97e3689ac212",
        "name": "Series Adapters",
        "entity_type": "method",
        "aliases": [
          "Series adapter"
        ],
        "description": "A type of adapter architecture where adapters are placed sequentially in the model layers.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "d3e20b6aa58d": {
        "entity_id": "d3e20b6aa58d",
        "name": "Parallel Adapter",
        "entity_type": "method",
        "aliases": [
          "Parallel adapters"
        ],
        "description": "A type of adapter architecture where adapters operate in parallel with the main model layers.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "b9583273e461": {
        "entity_id": "b9583273e461",
        "name": "Prompt-based Learning",
        "entity_type": "method",
        "aliases": [
          "Prompt tuning",
          "Prompt-based methods"
        ],
        "description": "A parameter-efficient fine-tuning approach that optimizes continuous prompt embeddings instead of model weights.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "08aef4940efd": {
        "entity_id": "08aef4940efd",
        "name": "Reparametrization",
        "entity_type": "method",
        "aliases": [
          "Reparametrization methods"
        ],
        "description": "A class of parameter-efficient fine-tuning techniques that reparameterize model weights using low-rank approximations.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "828da8381861": {
        "entity_id": "828da8381861",
        "name": "LLaMA",
        "entity_type": "method",
        "aliases": [
          "Large Language Model Meta AI",
          "LLaMA model"
        ],
        "description": "A family of open-access large language models developed by Meta AI.",
        "paper_ids": [
          "2308.11148",
          "2304.01933"
        ],
        "frequency": 2
      },
      "fbd190b84a3a": {
        "entity_id": "fbd190b84a3a",
        "name": "BLOOM",
        "entity_type": "method",
        "aliases": [
          "BigScience Large Open-science Open-access Multilingual Language Model"
        ],
        "description": "An open-access multilingual large language model developed by the BigScience collaboration.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "a59ad3d15dcd": {
        "entity_id": "a59ad3d15dcd",
        "name": "GPT-J",
        "entity_type": "method",
        "aliases": [
          "GPT-J 6B"
        ],
        "description": "An open-source large language model with 6 billion parameters developed by EleutherAI.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "e37a1ec2aed2": {
        "entity_id": "e37a1ec2aed2",
        "name": "GPT-4",
        "entity_type": "method",
        "aliases": [
          "Generative Pre-trained Transformer 4"
        ],
        "description": "A large multimodal language model developed by OpenAI, capable of accepting both text and image inputs.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "c167ca9ace71": {
        "entity_id": "c167ca9ace71",
        "name": "ChatGPT",
        "entity_type": "method",
        "aliases": [
          "Chat Generative Pre-trained Transformer"
        ],
        "description": "A conversational AI system developed by OpenAI based on large language model technology.",
        "paper_ids": [
          "2307.16039",
          "2304.01933"
        ],
        "frequency": 2
      },
      "5d14b2b2abb2": {
        "entity_id": "5d14b2b2abb2",
        "name": "ChatDoctor",
        "entity_type": "method",
        "aliases": [
          "ChatDoctor model"
        ],
        "description": "A medical domain large language model created by fine-tuning an open-access LLM with task-specific data.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "37e613bb2772": {
        "entity_id": "37e613bb2772",
        "name": "Alpaca",
        "entity_type": "method",
        "aliases": [
          "Alpaca model"
        ],
        "description": "An instruction-following language model created by fine-tuning LLaMA with instruction data.",
        "paper_ids": [
          "2307.16039",
          "2304.01933"
        ],
        "frequency": 2
      },
      "45e75bb3c854": {
        "entity_id": "45e75bb3c854",
        "name": "Large Language Models",
        "entity_type": "method",
        "aliases": [
          "LLMs"
        ],
        "description": "Advanced neural network models trained on massive text corpora to understand and generate human language.",
        "paper_ids": [
          "2308.11148",
          "2307.16039",
          "2403.17919",
          "2402.04401"
        ],
        "frequency": 4
      },
      "abdb8e30e8cb": {
        "entity_id": "abdb8e30e8cb",
        "name": "Supervised Fine-Tuning",
        "entity_type": "method",
        "aliases": [
          "SFT"
        ],
        "description": "A training method where models are fine-tuned on labeled instruction-response pairs to improve task performance.",
        "paper_ids": [
          "2307.16039"
        ],
        "frequency": 1
      },
      "ab2a554169c8": {
        "entity_id": "ab2a554169c8",
        "name": "Reinforcement Learning from Human Feedback",
        "entity_type": "method",
        "aliases": [
          "RLHF"
        ],
        "description": "A training approach that uses reinforcement learning with human preferences as rewards to align model outputs.",
        "paper_ids": [
          "2307.16039"
        ],
        "frequency": 1
      },
      "32f72ab2080c": {
        "entity_id": "32f72ab2080c",
        "name": "Vicuna",
        "entity_type": "method",
        "aliases": [],
        "description": "An open-source chatbot trained by fine-tuning LLaMA on user-shared conversations from ShareGPT.",
        "paper_ids": [
          "2307.16039"
        ],
        "frequency": 1
      },
      "de2f1323c590": {
        "entity_id": "de2f1323c590",
        "name": "Okapi",
        "entity_type": "method",
        "aliases": [],
        "description": "An instruction-tuned large language model for multiple languages using reinforcement learning from human feedback.",
        "paper_ids": [
          "2307.16039"
        ],
        "frequency": 1
      },
      "598c5d865f5b": {
        "entity_id": "598c5d865f5b",
        "name": "LLaMA-Reviewer",
        "entity_type": "method",
        "aliases": [
          "LLaMA-Reviewer framework"
        ],
        "description": "An innovative framework that leverages LLaMA for automating code review tasks through parameter-efficient fine-tuning.",
        "paper_ids": [
          "2308.11148"
        ],
        "frequency": 1
      },
      "8c060c621400": {
        "entity_id": "8c060c621400",
        "name": "Low-Rank Adaptation",
        "entity_type": "method",
        "aliases": [
          "LoRA"
        ],
        "description": "A parameter-efficient fine-tuning method that constrains weight updates to low-rank matrices to reduce memory usage.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "f1ad79dd64f2": {
        "entity_id": "f1ad79dd64f2",
        "name": "Layerwise Importance Sampling",
        "entity_type": "method",
        "aliases": [
          "LISA"
        ],
        "description": "A training strategy that leverages observed skewness in weight norms across layers to improve memory efficiency during fine-tuning.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "4b07fcca632d": {
        "entity_id": "4b07fcca632d",
        "name": "Full Parameter Training",
        "entity_type": "method",
        "aliases": [
          "full fine-tuning"
        ],
        "description": "Traditional fine-tuning approach where all model parameters are updated during training, requiring substantial memory resources.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "e13e320a3d75": {
        "entity_id": "e13e320a3d75",
        "name": "One PEFT Per User",
        "entity_type": "method",
        "aliases": [
          "OPPU"
        ],
        "description": "A framework employing personalized PEFT modules to store user-specific behavior patterns and preferences.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "6899e8d5879c": {
        "entity_id": "6899e8d5879c",
        "name": "Behavior History Retrieval",
        "entity_type": "method",
        "aliases": [],
        "description": "A technique that enriches user queries by retrieving relevant information from user behavior history.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "97ad2ddd3e8c": {
        "entity_id": "97ad2ddd3e8c",
        "name": "Textual Profiles",
        "entity_type": "method",
        "aliases": [],
        "description": "User profiles constructed from text data to capture preferences and characteristics for personalization.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "9c8eed5a1221": {
        "entity_id": "9c8eed5a1221",
        "name": "Large Language Models",
        "entity_type": "concept",
        "aliases": [
          "LLMs"
        ],
        "description": "Transformer-based neural networks with billions of parameters trained on massive text corpora.",
        "paper_ids": [
          "2304.01933"
        ],
        "frequency": 1
      },
      "ad0b1e8359f6": {
        "entity_id": "ad0b1e8359f6",
        "name": "Instruction Tuning",
        "entity_type": "concept",
        "aliases": [
          "Instruction-based tuning"
        ],
        "description": "A training approach that aligns language models with human instructions to improve their ability to follow directions.",
        "paper_ids": [
          "2307.16039"
        ],
        "frequency": 1
      },
      "7f766e78a5b9": {
        "entity_id": "7f766e78a5b9",
        "name": "Code Review Automation",
        "entity_type": "concept",
        "aliases": [
          "automating code review tasks"
        ],
        "description": "The automation of code review activities, a long-standing pursuit in software engineering.",
        "paper_ids": [
          "2308.11148"
        ],
        "frequency": 1
      },
      "c9574cbef9d0": {
        "entity_id": "c9574cbef9d0",
        "name": "Domain-Specific Pre-trained Models",
        "entity_type": "concept",
        "aliases": [
          "domain-specific models"
        ],
        "description": "Models pre-trained specifically for particular domains, often requiring extensive resources for training from scratch.",
        "paper_ids": [
          "2308.11148"
        ],
        "frequency": 1
      },
      "c87a6b11867d": {
        "entity_id": "c87a6b11867d",
        "name": "Memory Consumption",
        "entity_type": "concept",
        "aliases": [
          "GPU memory requirements",
          "memory usage"
        ],
        "description": "The amount of GPU memory required for training large models, which poses challenges for resource-constrained environments.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "f0d57b3d4fbd": {
        "entity_id": "f0d57b3d4fbd",
        "name": "Weight Norms",
        "entity_type": "concept",
        "aliases": [
          "parameter norms",
          "layer weight norms"
        ],
        "description": "The magnitude or distribution of weights across different layers in a neural network, which can exhibit systematic patterns.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "62248151bf95": {
        "entity_id": "62248151bf95",
        "name": "Layerwise Properties",
        "entity_type": "concept",
        "aliases": [
          "layer characteristics",
          "layer behavior"
        ],
        "description": "Observable patterns or behaviors that vary systematically across different layers of a neural network during training.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "559bc7357105": {
        "entity_id": "559bc7357105",
        "name": "Low-Rank Subspace",
        "entity_type": "concept",
        "aliases": [
          "low-dimensional subspace"
        ],
        "description": "A constrained parameter space where weight updates are restricted, potentially limiting model adaptation capacity.",
        "paper_ids": [
          "2403.17919"
        ],
        "frequency": 1
      },
      "dd52f37c7dcb": {
        "entity_id": "dd52f37c7dcb",
        "name": "Personalization",
        "entity_type": "concept",
        "aliases": [
          "Personalized LLMs"
        ],
        "description": "The process of aligning language model interactions, content, and recommendations with individual user preferences.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "943fe977d4ec": {
        "entity_id": "943fe977d4ec",
        "name": "Non-parametric Knowledge",
        "entity_type": "concept",
        "aliases": [],
        "description": "Knowledge derived from external sources like behavior history retrieval and textual profiles without modifying model parameters.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "32300b832ab7": {
        "entity_id": "32300b832ab7",
        "name": "Parametric Knowledge",
        "entity_type": "concept",
        "aliases": [],
        "description": "Knowledge encoded within the model's parameters through fine-tuning or training.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "05ab30feed41": {
        "entity_id": "05ab30feed41",
        "name": "Model Ownership",
        "entity_type": "concept",
        "aliases": [],
        "description": "The concept of users having control and ownership over their personalized language model instances.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      },
      "6457c5746d08": {
        "entity_id": "6457c5746d08",
        "name": "Dynamic User Behavior Patterns",
        "entity_type": "concept",
        "aliases": [],
        "description": "Complex, evolving patterns in user interactions and preferences that require adaptive modeling approaches.",
        "paper_ids": [
          "2402.04401"
        ],
        "frequency": 1
      }
    },
    "mentions": [
      {
        "entity_id": "efceda28921b",
        "paper_id": "2304.01933",
        "context": "An easy-to-use framework that integrates various adapters into large language models for parameter-efficient fine-tuning.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "f3c98857888b",
        "paper_id": "2304.01933",
        "context": "A fine-tuning approach that only requires training a small number of external parameters instead of the entire model.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "69d40ae1a31a",
        "paper_id": "2304.01933",
        "context": "A parameter-efficient fine-tuning method that adds small adapter modules to a pre-trained model.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "97e3689ac212",
        "paper_id": "2304.01933",
        "context": "A type of adapter architecture where adapters are placed sequentially in the model layers.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "d3e20b6aa58d",
        "paper_id": "2304.01933",
        "context": "A type of adapter architecture where adapters operate in parallel with the main model layers.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "b9583273e461",
        "paper_id": "2304.01933",
        "context": "A parameter-efficient fine-tuning approach that optimizes continuous prompt embeddings instead of model weights.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "08aef4940efd",
        "paper_id": "2304.01933",
        "context": "A class of parameter-efficient fine-tuning techniques that reparameterize model weights using low-rank approximations.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "828da8381861",
        "paper_id": "2304.01933",
        "context": "A family of open-access large language models developed by Meta AI.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "fbd190b84a3a",
        "paper_id": "2304.01933",
        "context": "An open-access multilingual large language model developed by the BigScience collaboration.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "a59ad3d15dcd",
        "paper_id": "2304.01933",
        "context": "An open-source large language model with 6 billion parameters developed by EleutherAI.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "e37a1ec2aed2",
        "paper_id": "2304.01933",
        "context": "A large multimodal language model developed by OpenAI, capable of accepting both text and image inputs.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c167ca9ace71",
        "paper_id": "2304.01933",
        "context": "A conversational AI system developed by OpenAI based on large language model technology.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "5d14b2b2abb2",
        "paper_id": "2304.01933",
        "context": "A medical domain large language model created by fine-tuning an open-access LLM with task-specific data.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "37e613bb2772",
        "paper_id": "2304.01933",
        "context": "An instruction-following language model created by fine-tuning LLaMA with instruction data.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "9c8eed5a1221",
        "paper_id": "2304.01933",
        "context": "Transformer-based neural networks with billions of parameters trained on massive text corpora.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2307.16039",
        "context": "Advanced neural network models trained on massive text corpora to understand and generate human language.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "ad0b1e8359f6",
        "paper_id": "2307.16039",
        "context": "A training approach that aligns language models with human instructions to improve their ability to follow directions.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "abdb8e30e8cb",
        "paper_id": "2307.16039",
        "context": "A training method where models are fine-tuned on labeled instruction-response pairs to improve task performance.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "ab2a554169c8",
        "paper_id": "2307.16039",
        "context": "A training approach that uses reinforcement learning with human preferences as rewards to align model outputs.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c167ca9ace71",
        "paper_id": "2307.16039",
        "context": "A commercial large language model developed by OpenAI that uses instruction tuning and RLHF.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "37e613bb2772",
        "paper_id": "2307.16039",
        "context": "An open-source instruction-tuned language model based on LLaMA, fine-tuned using supervised learning.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "32f72ab2080c",
        "paper_id": "2307.16039",
        "context": "An open-source chatbot trained by fine-tuning LLaMA on user-shared conversations from ShareGPT.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "de2f1323c590",
        "paper_id": "2307.16039",
        "context": "An instruction-tuned large language model for multiple languages using reinforcement learning from human feedback.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "598c5d865f5b",
        "paper_id": "2308.11148",
        "context": "An innovative framework that leverages LLaMA for automating code review tasks through parameter-efficient fine-tuning.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2308.11148",
        "context": "General-purpose language models with remarkable capabilities that can be adapted to domain-specific tasks.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "828da8381861",
        "paper_id": "2308.11148",
        "context": "A popular large language model used as the foundation for the code review automation framework.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "f3c98857888b",
        "paper_id": "2308.11148",
        "context": "Fine-tuning methods that train only a small subset of model parameters while maintaining high performance.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "7f766e78a5b9",
        "paper_id": "2308.11148",
        "context": "The automation of code review activities, a long-standing pursuit in software engineering.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c9574cbef9d0",
        "paper_id": "2308.11148",
        "context": "Models pre-trained specifically for particular domains, often requiring extensive resources for training from scratch.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2403.17919",
        "context": "Massive neural network models trained on extensive text data, capable of various natural language processing tasks.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "f3c98857888b",
        "paper_id": "2403.17919",
        "context": "Techniques designed to reduce memory consumption during fine-tuning by updating only a subset of model parameters.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "8c060c621400",
        "paper_id": "2403.17919",
        "context": "A parameter-efficient fine-tuning method that constrains weight updates to low-rank matrices to reduce memory usage.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "f1ad79dd64f2",
        "paper_id": "2403.17919",
        "context": "A training strategy that leverages observed skewness in weight norms across layers to improve memory efficiency during fine-tuning.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "4b07fcca632d",
        "paper_id": "2403.17919",
        "context": "Traditional fine-tuning approach where all model parameters are updated during training, requiring substantial memory resources.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c87a6b11867d",
        "paper_id": "2403.17919",
        "context": "The amount of GPU memory required for training large models, which poses challenges for resource-constrained environments.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "f0d57b3d4fbd",
        "paper_id": "2403.17919",
        "context": "The magnitude or distribution of weights across different layers in a neural network, which can exhibit systematic patterns.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "62248151bf95",
        "paper_id": "2403.17919",
        "context": "Observable patterns or behaviors that vary systematically across different layers of a neural network during training.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "559bc7357105",
        "paper_id": "2403.17919",
        "context": "A constrained parameter space where weight updates are restricted, potentially limiting model adaptation capacity.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2402.04401",
        "context": "Advanced AI models trained on massive text corpora to understand and generate human-like language.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "f3c98857888b",
        "paper_id": "2402.04401",
        "context": "A technique for adapting large language models using only a small subset of trainable parameters.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "e13e320a3d75",
        "paper_id": "2402.04401",
        "context": "A framework employing personalized PEFT modules to store user-specific behavior patterns and preferences.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "dd52f37c7dcb",
        "paper_id": "2402.04401",
        "context": "The process of aligning language model interactions, content, and recommendations with individual user preferences.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "943fe977d4ec",
        "paper_id": "2402.04401",
        "context": "Knowledge derived from external sources like behavior history retrieval and textual profiles without modifying model parameters.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "32300b832ab7",
        "paper_id": "2402.04401",
        "context": "Knowledge encoded within the model's parameters through fine-tuning or training.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "6899e8d5879c",
        "paper_id": "2402.04401",
        "context": "A technique that enriches user queries by retrieving relevant information from user behavior history.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "97ad2ddd3e8c",
        "paper_id": "2402.04401",
        "context": "User profiles constructed from text data to capture preferences and characteristics for personalization.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "05ab30feed41",
        "paper_id": "2402.04401",
        "context": "The concept of users having control and ownership over their personalized language model instances.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "6457c5746d08",
        "paper_id": "2402.04401",
        "context": "Complex, evolving patterns in user interactions and preferences that require adaptive modeling approaches.",
        "section": "full_text",
        "confidence": 0.9
      }
    ],
    "edges": [
      {
        "source_id": "2304.01933",
        "target_id": "efceda28921b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "f3c98857888b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "69d40ae1a31a",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "97e3689ac212",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "d3e20b6aa58d",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "b9583273e461",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "08aef4940efd",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "828da8381861",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "fbd190b84a3a",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "a59ad3d15dcd",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "e37a1ec2aed2",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "c167ca9ace71",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "5d14b2b2abb2",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "37e613bb2772",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2304.01933",
        "target_id": "9c8eed5a1221",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "ad0b1e8359f6",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "abdb8e30e8cb",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "ab2a554169c8",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "c167ca9ace71",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "37e613bb2772",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "32f72ab2080c",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2307.16039",
        "target_id": "de2f1323c590",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.11148",
        "target_id": "598c5d865f5b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.11148",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.11148",
        "target_id": "828da8381861",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.11148",
        "target_id": "f3c98857888b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.11148",
        "target_id": "7f766e78a5b9",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.11148",
        "target_id": "c9574cbef9d0",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "f3c98857888b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "8c060c621400",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "f1ad79dd64f2",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "4b07fcca632d",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "c87a6b11867d",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "62248151bf95",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "559bc7357105",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2403.17919",
        "target_id": "2308.11148",
        "edge_type": "likely_cites",
        "weight": 2.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "f3c98857888b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "e13e320a3d75",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "dd52f37c7dcb",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "943fe977d4ec",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "32300b832ab7",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "6899e8d5879c",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "05ab30feed41",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "6457c5746d08",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2402.04401",
        "target_id": "2308.11148",
        "edge_type": "likely_cites",
        "weight": 2.0,
        "paper_ids": []
      },
      {
        "source_id": "efceda28921b",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "efceda28921b",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 2.0,
        "paper_ids": [
          "2308.11148",
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "c9574cbef9d0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 3.0,
        "paper_ids": [
          "2308.11148",
          "2403.17919",
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "7f766e78a5b9",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "598c5d865f5b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "f3c98857888b",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "69d40ae1a31a",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "97e3689ac212",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "d3e20b6aa58d",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "b9583273e461",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "08aef4940efd",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 2.0,
        "paper_ids": [
          "2308.11148",
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "c9574cbef9d0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "7f766e78a5b9",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "828da8381861",
        "target_id": "598c5d865f5b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "fbd190b84a3a",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "a59ad3d15dcd",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "e37a1ec2aed2",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 2.0,
        "paper_ids": [
          "2307.16039",
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "5d14b2b2abb2",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 2.0,
        "paper_ids": [
          "2307.16039",
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "9c8eed5a1221",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "37e613bb2772",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 3.0,
        "paper_ids": [
          "2308.11148",
          "2403.17919",
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "c9574cbef9d0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "7f766e78a5b9",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "598c5d865f5b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "abdb8e30e8cb",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ab2a554169c8",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "32f72ab2080c",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "ad0b1e8359f6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "de2f1323c590",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "598c5d865f5b",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "598c5d865f5b",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "598c5d865f5b",
        "target_id": "c9574cbef9d0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "598c5d865f5b",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "598c5d865f5b",
        "target_id": "7f766e78a5b9",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "8c060c621400",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f1ad79dd64f2",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "4b07fcca632d",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "e13e320a3d75",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6899e8d5879c",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "97ad2ddd3e8c",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "a59ad3d15dcd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "d3e20b6aa58d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "fbd190b84a3a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "5d14b2b2abb2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "b9583273e461",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "08aef4940efd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "efceda28921b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "e37a1ec2aed2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "97e3689ac212",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "9c8eed5a1221",
        "target_id": "69d40ae1a31a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2304.01933"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "32f72ab2080c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "37e613bb2772",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "ab2a554169c8",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "de2f1323c590",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "ad0b1e8359f6",
        "target_id": "abdb8e30e8cb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2307.16039"
        ]
      },
      {
        "source_id": "7f766e78a5b9",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "7f766e78a5b9",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "7f766e78a5b9",
        "target_id": "c9574cbef9d0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "7f766e78a5b9",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "7f766e78a5b9",
        "target_id": "598c5d865f5b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "c9574cbef9d0",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "c9574cbef9d0",
        "target_id": "828da8381861",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "c9574cbef9d0",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "c9574cbef9d0",
        "target_id": "7f766e78a5b9",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "c9574cbef9d0",
        "target_id": "598c5d865f5b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.11148"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "c87a6b11867d",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "f0d57b3d4fbd",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "559bc7357105",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "62248151bf95",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "4b07fcca632d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "8c060c621400",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "c87a6b11867d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "f0d57b3d4fbd",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "62248151bf95",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "559bc7357105",
        "target_id": "f1ad79dd64f2",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2403.17919"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "dd52f37c7dcb",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "943fe977d4ec",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "32300b832ab7",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "6457c5746d08",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "05ab30feed41",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "f3c98857888b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "e13e320a3d75",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "05ab30feed41",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "dd52f37c7dcb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "97ad2ddd3e8c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "32300b832ab7",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "6899e8d5879c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      },
      {
        "source_id": "6457c5746d08",
        "target_id": "943fe977d4ec",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2402.04401"
        ]
      }
    ],
    "communities": [
      {
        "community_id": "comm_0_0",
        "level": 0,
        "entities": [
          "a59ad3d15dcd",
          "fbd190b84a3a",
          "37e613bb2772",
          "ab2a554169c8",
          "de2f1323c590",
          "32f72ab2080c",
          "c167ca9ace71",
          "efceda28921b",
          "d3e20b6aa58d",
          "b9583273e461",
          "08aef4940efd",
          "9c8eed5a1221",
          "97e3689ac212",
          "ad0b1e8359f6",
          "828da8381861",
          "5d14b2b2abb2",
          "e37a1ec2aed2",
          "abdb8e30e8cb",
          "69d40ae1a31a"
        ],
        "papers": [
          "2307.16039",
          "2308.11148",
          "2304.01933"
        ],
        "summary": "The primary research theme of this community is the development and refinement of **parameter-efficient fine-tuning (PEFT) techniques** to adapt large language models (LLMs) for specialized tasks and languages. This theme is driven by the need to make powerful, open-source foundation models like **LLaMA**, **GPT-J**, and **BLOOM** more accessible and practical, overcoming the prohibitive cost of full-model fine-tuning. The research focuses on aligning these models with human intent and specific domains—such as multilingual instruction-following or code review—through methods like **instruction tuning** and **Reinforcement Learning from Human Feedback (RLHF)**, while drastically reducing the number of trainable parameters.\n\nThe most prominent approaches are **adapter-based methods** and **reparameterization techniques**, which form the core of the **LLM-Adapters** framework. This includes specific architectures like **Series Adapters** and **Parallel Adapters**, as well as methods like LoRA (a reparameterization technique). These PEFT methods are consistently applied to instruction-tune base models, as seen in **Alpaca**, **Vicuna**, and **Okapi**. A key trend is the combination of these efficient tuning methods with RLHF, as demonstrated by Okapi, to further enhance model alignment and performance beyond standard supervised fine-tuning.\n\nThe key findings reveal that this paradigm is highly effective. First, PEFT enables smaller models (e.g., **LLaMA-13B** with LoRA) to match or surpass the performance of vastly larger models (like **ChatGPT/GPT-3.5**) on targeted reasoning tasks. Second, RLHF-based instruction tuning provides significant multilingual performance gains over supervised fine-tuning alone, proving feasible for many languages. Finally, unified LLMs pre-trained on general corpora can be efficiently adapted for specialized domains (e.g., code review in **LLaMA-Reviewer**) without domain-specific pre-training, achieving strong performance while training less than 1% of parameters. The overarching contribution is establishing a scalable blueprint for customizing powerful LLMs with minimal resources.",
        "parent_id": null,
        "children_ids": [
          "comm_1_2",
          "comm_1_4"
        ]
      },
      {
        "community_id": "comm_0_1",
        "level": 0,
        "entities": [
          "4b07fcca632d",
          "45e75bb3c854",
          "f0d57b3d4fbd",
          "62248151bf95",
          "32300b832ab7",
          "f1ad79dd64f2",
          "8c060c621400",
          "598c5d865f5b",
          "559bc7357105",
          "e13e320a3d75",
          "f3c98857888b",
          "c87a6b11867d",
          "05ab30feed41",
          "dd52f37c7dcb",
          "6457c5746d08",
          "c9574cbef9d0",
          "7f766e78a5b9",
          "97ad2ddd3e8c",
          "6899e8d5879c",
          "943fe977d4ec"
        ],
        "papers": [
          "2403.17919",
          "2308.11148",
          "2304.01933",
          "2402.04401",
          "2307.16039"
        ],
        "summary": "This research community is focused on advancing the **parameter-efficient fine-tuning (PEFT) of Large Language Models (LLMs)**, with a central theme of **optimizing the trade-off between performance, computational cost, and accessibility**. The work moves beyond simply demonstrating that PEFT methods like adapters and **Low-Rank Adaptation (LoRA)** are viable, and instead delves into *how* and *why* these methods work, seeking to refine their efficiency and expand their applications. A key sub-theme is the investigation of **layerwise properties**—such as the skewness in **weight norms** observed during fine-tuning—to develop more intelligent and memory-efficient training strategies. Furthermore, the community explores how PEFT enables practical democratization and specialization of LLMs, facilitating tasks from multilingual instruction tuning to code review and personalized AI, without the prohibitive cost of **Full Parameter Training**.\n\nThe most prominent methodological approach is **Parameter-Efficient Fine-Tuning** itself, particularly through adapter modules and LoRA. However, research is actively refining these techniques. Studies like **LLM-Adapters** systematically analyze optimal adapter placement within model architecture, while **LISA (Layerwise Importance Sampling)** introduces a novel training strategy that exploits observed layerwise skewness to dynamically freeze less important middle layers during optimization. This represents a trend toward *second-order efficiency gains*: first by reducing trainable parameters, and then by using insights into training dynamics to further optimize memory and compute usage. The application of these methods is broad, evidenced by frameworks like **LLaMA-Reviewer** for code automation and the **Okapi** framework for multilingual reinforcement learning from human feedback (RLHF).\n\nKey findings and contributions reveal several important trends. First, PEFT allows smaller models (e.g., 13B parameters) to match or surpass the task-specific performance of vastly larger models (e.g., 175B+), challenging the necessity of scale for every application. Second, there is a shift from treating LLMs as generic tools to tailoring them for specialized domains (code) and individual users, as seen in the **One PEFT Per User (OPPU)** framework. OPPU's success highlights a trend toward storing **parametric knowledge** in personalized modules, which proves more robust to dynamic user behavior than retrieval-based methods. Finally, the discovery of systematic **layerwise properties** during fine-tuning (LISA) is a significant contribution, suggesting that future PEFT methods can be co-designed with the intrinsic learning dynamics of LLMs to achieve unprecedented efficiency without sacrificing performance.",
        "parent_id": null,
        "children_ids": [
          "comm_1_3",
          "comm_1_5"
        ]
      },
      {
        "community_id": "comm_1_2",
        "level": 1,
        "entities": [
          "a59ad3d15dcd",
          "d3e20b6aa58d",
          "828da8381861",
          "fbd190b84a3a",
          "5d14b2b2abb2",
          "37e613bb2772",
          "b9583273e461",
          "c167ca9ace71",
          "08aef4940efd",
          "efceda28921b",
          "e37a1ec2aed2",
          "9c8eed5a1221",
          "97e3689ac212",
          "69d40ae1a31a"
        ],
        "papers": [
          "2307.16039",
          "2308.11148",
          "2304.01933"
        ],
        "summary": "This research community focuses on **parameter-efficient fine-tuning (PEFT) of open-source large language models (LLMs) to create specialized, high-performance systems**. The central theme is overcoming the prohibitive cost of full-model fine-tuning for massive LLMs by developing and evaluating lightweight adaptation techniques—such as **Adapter-based Fine-Tuning** (e.g., **Series Adapters**, **Parallel Adapters**), **Reparametrization** methods like LoRA, and **Prompt-based Learning**—to effectively tailor pre-trained models like **LLaMA**, **BLOOM**, and **GPT-J** to specific domains and tasks. The work is driven by the goal of democratizing advanced AI capabilities, enabling research and application development with more accessible open-source models that can compete with or surpass larger, closed models like **ChatGPT** and **GPT-4**.\n\nThe most prominent approaches revolve around **adapter frameworks** and **instruction tuning**. The **LLM-Adapters** framework serves as a key hub for systematically comparing adapter architectures, while instruction tuning—exemplified by models like **Alpaca** and the multilingual **Okapi** framework—is a dominant paradigm for aligning models with human intent. Reinforcement Learning from Human Feedback (RLHF), as implemented in Okapi, is highlighted as a critical advancement over standard supervised fine-tuning. The community heavily leverages the **LLaMA** family as a preferred base model due to its open-access nature and strong performance, applying PEFT methods to specialize it for diverse applications, from medical dialogue (**ChatDoctor**) to automated code review (**LLaMA-Reviewer**).\n\nKey findings and trends underscore the remarkable effectiveness of these strategies. First, **smaller, efficiently-tuned models can rival or exceed the performance of vastly larger ones**; for instance, **LLaMA-13B+LoRA** outperformed **GPT-3.5** on arithmetic reasoning, and a 6.7B-parameter LLaMA achieved state-of-the-art code review results. Second, optimal adapter **placement within the model architecture is task- and type-dependent**, a crucial engineering insight for maximizing PEFT efficacy. The overarching contribution is a validated paradigm shift: instead of training ever-larger models from scratch, superior and accessible results can be achieved by strategically and efficiently adapting powerful, open-source foundation models to specialized downstream tasks and languages.",
        "parent_id": "comm_0_0",
        "children_ids": [
          "comm_2_6"
        ]
      },
      {
        "community_id": "comm_1_3",
        "level": 1,
        "entities": [
          "f3c98857888b",
          "4b07fcca632d",
          "8c060c621400",
          "c9574cbef9d0",
          "598c5d865f5b",
          "45e75bb3c854",
          "7f766e78a5b9",
          "c87a6b11867d",
          "559bc7357105",
          "f0d57b3d4fbd",
          "62248151bf95",
          "f1ad79dd64f2"
        ],
        "papers": [
          "2403.17919",
          "2308.11148",
          "2304.01933",
          "2402.04401",
          "2307.16039"
        ],
        "summary": "This research community is focused on advancing the **parameter-efficient fine-tuning (PEFT)** of **Large Language Models (LLMs)** as a core strategy to democratize and specialize their capabilities. The central theme is overcoming the prohibitive computational and memory costs of **Full Parameter Training** by developing methods that adapt powerful, general-purpose LLMs to specific tasks and domains while updating only a tiny fraction of their weights. This pursuit is driven by the goal of making state-of-the-art model adaptation accessible under resource constraints, whether for specialized applications like **Code Review Automation** or for personalized user systems, without needing to train **Domain-Specific Pre-trained Models** from scratch.\n\nThe most prominent methodological approach is **Low-Rank Adaptation (LoRA)**, a specific PEFT technique that constrains updates to low-rank matrices. Research in this community actively investigates how to optimize and enhance such PEFT methods. Key studies like *LLM-Adapters* systematically explore optimal adapter placement (e.g., finding LoRA works best after both Attention and MLP layers), while newer innovations like **Layerwise Importance Sampling (LISA)** build upon observed patterns in **Layerwise Properties**—such as the skewness in **Weight Norms**—to create even more memory-efficient training strategies. Frameworks like **LLaMA-Reviewer** and OPPU exemplify the application of these PEFT principles to end tasks, demonstrating that small, tuned models can rival or surpass the performance of vastly larger ones.\n\nThe key findings and trends reveal a powerful narrative: PEFT is not just a compromise for efficiency but can be a pathway to superior performance. The community consistently shows that smaller base models (e.g., LLaMA-13B) fine-tuned with PEFT methods like LoRA can outperform giants like GPT-3.5 on specific reasoning tasks (*LLM-Adapters*) and match specialized models in domains like code review (*LLaMA-Reviewer*). Furthermore, research is moving beyond static efficiency to tackle dynamic challenges; LISA improves performance over standard LoRA by exploiting layerwise update skewness, and OPPU demonstrates that personalized PEFT modules are more robust to user behavior shifts than prompt-based methods. The overarching trend is a sophisticated evolution of PEFT from a simple efficiency tool into a lever for specialized performance, personalization, and deeper understanding of model adaptation dynamics.",
        "parent_id": "comm_0_1",
        "children_ids": [
          "comm_2_7",
          "comm_2_9"
        ]
      },
      {
        "community_id": "comm_1_4",
        "level": 1,
        "entities": [
          "32f72ab2080c",
          "ad0b1e8359f6",
          "de2f1323c590",
          "ab2a554169c8",
          "abdb8e30e8cb"
        ],
        "papers": [
          "2307.16039"
        ],
        "summary": "The primary research theme of this community is the **advanced fine-tuning of large language models (LLMs) to improve their alignment with human intent and instruction-following capabilities**, with a specific focus on moving beyond basic supervised methods. This theme centers on **Instruction Tuning** as a core conceptual framework, which aims to transform general-purpose LLMs into more helpful, controllable, and safe assistants. The community's work is particularly concerned with scaling and refining this alignment process, exploring how to effectively leverage human feedback to achieve superior performance compared to foundational fine-tuning approaches.\n\nThe most prominent methodological approaches involve a progression from **Supervised Fine-Tuning (SFT)** on labeled examples to more sophisticated, feedback-driven techniques. **Reinforcement Learning from Human Feedback (RLHF)** is established as a critical advancement, using human preference data as a reward signal to further refine model outputs beyond what SFT can achieve. This is exemplified by models like **Vicuna**, which applies SFT on user conversation data, and more comprehensively by frameworks like **Okapi**, which implements a full RLHF-based instruction-tuning pipeline. The trend is clearly toward multi-stage fine-tuning where SFT provides an initial alignment, which is then optimized via RLHF.\n\nThe key finding and contribution from this community, as demonstrated by the **Okapi** paper, is that **RLHF-based instruction tuning provides significant and consistent performance benefits over SFT alone, even in challenging, multilingual contexts**. Okapi's success across 26 languages shows that the RLHF paradigm is technically feasible and effective for extending high-quality alignment to diverse, lower-resource languages, not just English-centric models. This indicates a major trend: the field is moving from validating RLHF on a few high-resource languages to systematically proving its utility as a generalizable, superior method for creating aligned, instruction-following LLMs across the global linguistic landscape.",
        "parent_id": "comm_0_0",
        "children_ids": [
          "comm_2_8"
        ]
      },
      {
        "community_id": "comm_1_5",
        "level": 1,
        "entities": [
          "6457c5746d08",
          "e13e320a3d75",
          "05ab30feed41",
          "dd52f37c7dcb",
          "97ad2ddd3e8c",
          "32300b832ab7",
          "6899e8d5879c",
          "943fe977d4ec"
        ],
        "papers": [
          "2402.04401"
        ],
        "summary": "This research community focuses on **personalized fine-tuning of large language models (LLMs)**, with a specific emphasis on **democratizing model ownership and control for individual users**. The central theme moves beyond simple prompt-based personalization to explore architectures where each user can own and manage a lightweight, adaptive model component that evolves with their unique behavior. This addresses core challenges of modeling **Dynamic User Behavior Patterns** and ensuring that personalization is robust, private, and user-centric, as highlighted by the emphasis on **Model Ownership** as a key concept.\n\nThe most prominent methodological approach is the **One PEFT Per User (OPPU)** framework, introduced in the seminal paper \"Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning.\" OPPU is the architectural cornerstone, proposing that each user possesses a personalized Parameter-Efficient Fine-Tuning (PEFT) module—such as a LoRA adapter—that stores **Parametric Knowledge** of their preferences. This method is synergistically combined with **Non-parametric Knowledge** sources, namely **Behavior History Retrieval** and **Textual Profiles**, to enrich context. The community thus investigates a hybrid paradigm where a user's static PEFT module works in concert with dynamic retrieval from their personal data history.\n\nThe key finding and contribution of this community, as demonstrated through the LaMP benchmark, is that the OPPU framework significantly outperforms prior retrieval-based (non-parametric) personalization methods across diverse tasks. Crucially, it provides **enhanced robustness to user behavior shifts** because the parametric PEFT module captures enduring user traits, reducing dependency on the immediate relevance of a user's retrievable history. The trend is toward a new paradigm for LLM personalization: shifting from a centralized model that retrieves user data to a federated-like ecosystem of user-owned, parameter-efficient modules, thereby balancing performance, adaptability, and user sovereignty.",
        "parent_id": "comm_0_1",
        "children_ids": [
          "comm_2_10"
        ]
      },
      {
        "community_id": "comm_2_6",
        "level": 2,
        "entities": [
          "a59ad3d15dcd",
          "d3e20b6aa58d",
          "fbd190b84a3a",
          "5d14b2b2abb2",
          "b9583273e461",
          "08aef4940efd",
          "efceda28921b",
          "e37a1ec2aed2",
          "9c8eed5a1221",
          "97e3689ac212",
          "69d40ae1a31a"
        ],
        "papers": [
          "2304.01933"
        ],
        "summary": "This research community is focused on **parameter-efficient fine-tuning (PEFT) of large language models (LLMs)**, specifically investigating how to effectively adapt massive, general-purpose models to specialized tasks without the prohibitive cost of full-model retraining. The central theme is the systematic comparison and integration of various PEFT architectures—including **Adapter-based Fine-Tuning** (with its **Series** and **Parallel Adapter** variants), **Reparametrization** techniques like LoRA, and **Prompt-based Learning**—into a unified framework. The work is driven by the practical need to make state-of-the-art adaptation accessible, leveraging open-source models like **GPT-J**, **BLOOM**, and LLaMA as base models for experimentation and application, such as creating domain-specific models like **ChatDoctor**.\n\nThe most prominent approaches are those encapsulated and compared within the **LLM-Adapters** framework. This framework treats different PEFT methods as modular components, allowing for direct empirical comparison of their architectural nuances. Key findings from this synthesis reveal that optimal performance is highly dependent on the precise integration of these small modules: **Series Adapters** perform best placed after the MLP layers, **Parallel Adapters** work best parallel to MLP layers, and reparameterization methods like LoRA achieve peak performance when applied to both the Attention and MLP layers simultaneously. This granular analysis of module placement is a major contribution, moving beyond simply proving PEFT's viability to providing engineering guidelines for its optimal use.\n\nA key trend and significant finding from this community is the demonstration that **smaller, efficiently-tuned models can rival or surpass the performance of vastly larger models on specific tasks**. The cited paper shows that a **LLaMA-13B model fine-tuned with LoRA** can outperform the massive **GPT-3.5 (175B+ parameters)** on arithmetic reasoning benchmarks. This challenges the prevailing assumption that scale is the primary driver of performance and highlights PEFT as a powerful tool for democratizing high-performance AI. The overall contribution is a shift towards a more modular, efficient, and accessible paradigm for LLM specialization, where strategic minimal intervention, rather than brute-force scaling, is the path to achieving targeted, state-of-the-art results.",
        "parent_id": "comm_1_2",
        "children_ids": []
      },
      {
        "community_id": "comm_2_7",
        "level": 2,
        "entities": [
          "7f766e78a5b9",
          "828da8381861",
          "c9574cbef9d0",
          "598c5d865f5b"
        ],
        "papers": [
          "2308.11148",
          "2304.01933"
        ],
        "summary": "The primary research theme of this community is the **parameter-efficient adaptation of general-purpose, open-source large language models (LLMs) for specialized software engineering tasks**, with a specific focus on automating code review. This theme directly addresses the tension between the prohibitive cost of training domain-specific models from scratch and the need for high-performance tools in technical domains. The community's work demonstrates a paradigm shift: instead of creating narrow, resource-intensive pre-trained models for each software engineering task, researchers are successfully repurposing unified LLMs pre-trained primarily on natural language through targeted, lightweight fine-tuning.\n\nThe most prominent methodological approach centers on the **LLaMA family of models** used as a base, combined with **parameter-efficient fine-tuning (PEFT) techniques** like adapters and Low-Rank Adaptation (LoRA). The \"LLM-Adapters\" paper provides a foundational analysis of optimal adapter placement within transformer architectures, establishing that methods like LoRA can maximize performance with minimal trainable parameters. This principle is directly applied in the **LLaMA-Reviewer** framework, which innovatively leverages a 6.7B-parameter LLaMA model fine-tuned with PEFT to tackle code review automation. The approach explicitly contrasts with the traditional pursuit of building dedicated \"Domain-Specific Pre-trained Models,\" advocating instead for efficient adaptation of generalist models.\n\nThe key findings and contributions establish a powerful trend toward efficiency and accessibility. First, they empirically validate that **smaller models (7B-13B parameters) with PEFT can match or surpass the performance of vastly larger models (175B+ parameters) on specific reasoning and domain tasks**, as shown by LLaMA-13B+LoRA outperforming GPT-3.5 on arithmetic datasets. Second, the LLaMA-Reviewer application proves this principle holds for complex, structured domains like code review, achieving performance comparable to specialized models while training **less than 1% of the total parameters**. The overarching trend is the demystification and democratization of high-level AI for specialized fields, demonstrating that state-of-the-art task performance no longer necessitates exclusive access to the largest models or massive, domain-specific pre-training budgets.",
        "parent_id": "comm_1_3",
        "children_ids": []
      },
      {
        "community_id": "comm_2_8",
        "level": 2,
        "entities": [
          "32f72ab2080c",
          "ad0b1e8359f6",
          "37e613bb2772",
          "c167ca9ace71",
          "ab2a554169c8",
          "de2f1323c590",
          "abdb8e30e8cb"
        ],
        "papers": [
          "2307.16039",
          "2304.01933"
        ],
        "summary": "This community of research focuses on **parameter-efficient and instruction-aligned fine-tuning of large language models (LLMs)**, specifically exploring how to effectively adapt powerful base models (like LLaMA) to follow human instructions and perform specific tasks without the prohibitive cost of full-model retraining. The core theme is the development and comparison of **instruction-tuning** methodologies—such as **Supervised Fine-Tuning (SFT)** and **Reinforcement Learning from Human Feedback (RLHF)**—and their implementation through efficient adaptation techniques. The goal is to create capable, specialized models that rival the performance of much larger, closed-source systems like **ChatGPT**, but with greater accessibility and lower computational overhead.\n\nThe most prominent approaches are **parameter-efficient fine-tuning (PEFT)** methods, such as adapters and **LoRA**, and instruction-alignment techniques like **RLHF**. Models such as **Alpaca** and **Vicuna** demonstrate the effectiveness of SFT on curated instruction or conversational data, while **Okapi** represents a significant advancement by applying the full **RLHF** pipeline to multilingual open-source models. A key methodological insight from the community is that the optimal placement of PEFT modules is not universal; for instance, LoRA performs best when applied to both Attention and MLP layers simultaneously, whereas different adapter types have distinct optimal locations within the model architecture.\n\nThe key findings and trends highlight a shift toward efficiency and open accessibility. First, research shows that smaller, efficiently tuned models (e.g., **LLaMA-13B with LoRA**) can match or exceed the performance of vastly larger models (like **GPT-3.5**) on specific reasoning tasks, challenging the necessity of sheer scale. Second, **RLHF** is validated as a superior alignment technique compared to SFT alone, as demonstrated by **Okapi's** consistent gains across multiple languages. The overarching contribution is a roadmap for democratizing advanced LLM capabilities: by combining open-source base models, PEFT strategies, and sophisticated instruction-tuning pipelines, the community is enabling the development of high-performance, specialized models that are both computationally and linguistically inclusive.",
        "parent_id": "comm_1_4",
        "children_ids": []
      },
      {
        "community_id": "comm_2_9",
        "level": 2,
        "entities": [
          "4b07fcca632d",
          "8c060c621400",
          "c87a6b11867d",
          "559bc7357105",
          "f0d57b3d4fbd",
          "62248151bf95",
          "f1ad79dd64f2"
        ],
        "papers": [
          "2403.17919"
        ],
        "summary": "This research community focuses on **optimizing the memory efficiency of fine-tuning large language models (LLMs) by exploiting the non-uniform distribution of learning across network layers**. The central theme is the trade-off between the high memory cost of traditional **Full Parameter Training** and the parameter efficiency but potentially limited adaptability of methods like **Low-Rank Adaptation (LoRA)**. Researchers are investigating the **layerwise properties** of models during fine-tuning to develop strategies that selectively update only the most critical parameters, thereby reducing **memory consumption** without sacrificing task performance.\n\nThe most prominent approaches are **LoRA** and the newer **Layerwise Importance Sampling (LISA)**. LoRA addresses memory constraints by restricting weight updates to a **low-rank subspace**, drastically reducing the number of trainable parameters. Building directly on insights from LoRA fine-tuning, LISA introduces a dynamic, layer-selective training strategy. It moves beyond a static parameter-efficient paradigm by actively sampling which layers to update based on their observed importance during optimization, creating a hybrid approach that balances LoRA's efficiency with more flexible, targeted learning.\n\nThe key finding driving this community is the empirical discovery, as detailed in the **LISA** paper, of a consistent skewness in **weight norms** across layers during LoRA fine-tuning. Specifically, the magnitude of updates is heavily concentrated in the bottom and top layers, with middle layers contributing minimally. This **layerwise property** challenges the assumption that all layers require equal attention during adaptation. The major contribution is the demonstration that LISA, by probabilistically freezing these less important middle layers, can outperform both LoRA and full fine-tuning on multiple benchmarks while maintaining LoRA-level memory costs. The trend is a shift from uniform parameter-efficient methods toward **adaptive, importance-driven fine-tuning** that leverages intrinsic model learning dynamics to achieve optimal performance-efficiency Pareto frontiers.",
        "parent_id": "comm_1_3",
        "children_ids": []
      },
      {
        "community_id": "comm_2_10",
        "level": 2,
        "entities": [
          "6457c5746d08",
          "e13e320a3d75",
          "05ab30feed41",
          "dd52f37c7dcb",
          "97ad2ddd3e8c",
          "32300b832ab7",
          "6899e8d5879c",
          "943fe977d4ec"
        ],
        "papers": [
          "2402.04401"
        ],
        "summary": "This research community focuses on a novel paradigm for **personalizing large language models (LLMs)**, moving beyond traditional prompt engineering or monolithic fine-tuning. The central theme is the **democratization of model ownership**, where individual users control their own lightweight, adaptive model instances to capture their unique and evolving preferences. This approach directly addresses the challenge of modeling **Dynamic User Behavior Patterns**, recognizing that user interests are complex and non-stationary. The community synthesizes two knowledge paradigms—**parametric knowledge** (learned, stored in model weights) and **non-parametric knowledge** (retrieved from external sources like **Textual Profiles** and **Behavior History Retrieval**)—to create more robust and user-centric AI systems.\n\nThe most prominent methodological innovation is the **One PEFT Per User (OPPU)** framework. This method assigns each user a personalized, parameter-efficient fine-tuning (PEFT) module (e.g., a LoRA adapter) that is trained on their individual data. This module acts as a private, updatable repository for that user's parametric knowledge. OPPU is designed to work in concert with non-parametric techniques: a user's query can be enriched via **Behavior History Retrieval** from their textual profile, and then processed by a base LLM enhanced with their unique PEFT module. This hybrid architecture is the core technical approach for achieving deep personalization while maintaining efficiency and user sovereignty.\n\nThe key contribution, as demonstrated in the foundational paper \"Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning,\" is that OPPU significantly outperforms prompt-based personalization methods across diverse tasks. A critical finding is its **enhanced robustness to user behavior shifts**. Because the PEFT module encodes stable, learned preferences, the system does not fail when a user's immediate query has weak relevance to their retrievable history; it can still leverage the parametric knowledge in the personalized adapter. The overarching trend is a shift toward **modular, user-owned personalization**, where efficiency (via PEFT), adaptability (to dynamic behavior), and user control (model ownership) are integrated into a single framework, paving the way for more ethical and effective personalized LLMs.",
        "parent_id": "comm_1_5",
        "children_ids": []
      }
    ],
    "global_summary": "### Global Summary of the Research Landscape for Large Language Model Fine-Tuning\n\nThe research landscape for fine-tuning large language models (LLMs) is dominated by the paradigm of **parameter-efficient fine-tuning (PEFT)**, which has emerged as the central strategy to overcome the prohibitive computational and memory costs of full-model fine-tuning. The primary research theme is the development and refinement of PEFT techniques—such as adapter-based methods and Low-Rank Adaptation (LoRA)—to adapt powerful, open-source foundation models (e.g., LLaMA, BLOOM) for specialized tasks, languages, and human-aligned behaviors. This theme is driven by a dual goal: democratizing access to state-of-the-art LLMs and enabling their practical customization for domains like multilingual instruction-following, code generation, and personalized AI. A closely related and evolving sub-theme involves deepening the theoretical understanding of *how* these efficient methods work, investigating layerwise learning dynamics to achieve second-order efficiency gains beyond simply reducing parameter counts.\n\nThe most influential and widely adopted methods are **adapter modules** and **LoRA**, often integrated into frameworks like LLM-Adapters, which systematically analyze optimal architectural placement. These core PEFT techniques are consistently applied to instruction-tune base models, resulting in well-known models such as Alpaca and Vicuna. A significant trend is the combination of PEFT with **Reinforcement Learning from Human Feedback (RLHF)**, as seen in frameworks like Okapi, to enhance model alignment beyond standard supervised fine-tuning. Recently, research has advanced to more sophisticated, insight-driven strategies like **Layerwise Importance Sampling (LISA)**, which exploits observed skewness in weight updates to dynamically freeze less important layers during training, further optimizing memory and compute. The application scope has broadened from general instruction tuning to specialized frameworks like LLaMA-Reviewer for code automation and the One PEFT Per User (OPPU) concept for personalized AI.\n\nEvaluation in this field primarily relies on benchmarking adapted models against both larger proprietary models (like ChatGPT/GPT-3.5) and baseline fine-tuning approaches on targeted reasoning and domain-specific tasks. While specific datasets are not uniformly identified in the provided summaries, standard practice involves using established instruction-following, code, and multilingual benchmarks to measure performance gains. Key metrics focus on task accuracy, alignment with human intent, and the critical efficiency metrics of the number of trainable parameters and computational resources required. The evolution of the field shows a clear progression: initial proof-of-concept studies demonstrated that PEFT enables smaller models (e.g., 13B parameters) to rival the performance of vastly larger models on specific tasks. The current state involves refining these techniques for optimal efficiency and robustness while expanding their application to create highly specialized and personalized models.\n\nEmerging trends point toward an even deeper co-design of PEFT methods with the intrinsic learning dynamics of LLMs, moving beyond empirical tuning to theory-guided efficiency. The concept of storing **parametric knowledge** in personalized, user-specific modules (as in OPPU) presents a promising alternative to retrieval-based methods for dynamic user adaptation. Open questions remain regarding the theoretical limits of parameter efficiency, the optimal strategies for composing multiple PEFT modules for multi-task learning, and the long-term stability and ethical implications of highly personalized model fine-tuning. Ultimately, the field is converging on a scalable blueprint for customizing powerful LLMs, making their specialization feasible with minimal resources and paving the way for their ubiquitous integration into diverse domains and individual workflows.",
    "stats": {
      "node_count": 44,
      "edge_count": 500,
      "paper_count": 5,
      "entity_count": 39,
      "entity_types": {
        "method": 26,
        "concept": 13
      },
      "edge_types": {
        "mentions": 48,
        "likely_cites": 2,
        "co_occurs": 450
      },
      "top_entities": [
        [
          "Parameter-Efficient Fine-Tuning",
          "method",
          1.7368421052631577
        ],
        [
          "Large Language Models",
          "method",
          1.4210526315789473
        ],
        [
          "ChatGPT",
          "method",
          1.0526315789473684
        ],
        [
          "Alpaca",
          "method",
          1.0526315789473684
        ],
        [
          "LLaMA",
          "method",
          0.9473684210526315
        ],
        [
          "LLM-Adapters",
          "method",
          0.7368421052631579
        ],
        [
          "Adapter-based Fine-Tuning",
          "method",
          0.7368421052631579
        ],
        [
          "Series Adapters",
          "method",
          0.7368421052631579
        ],
        [
          "Parallel Adapter",
          "method",
          0.7368421052631579
        ],
        [
          "Prompt-based Learning",
          "method",
          0.7368421052631579
        ]
      ],
      "density": 0.2642706131078224,
      "total_communities": 11,
      "levels": 3,
      "communities_per_level": {
        "0": 2,
        "1": 4,
        "2": 5
      },
      "avg_size": 10.454545454545455,
      "min_size": 4,
      "max_size": 20,
      "root_communities": 2,
      "leaf_communities": 5,
      "cached_papers": 0,
      "new_papers": 5
    }
  },
  "synthesis": {
    "themes": [
      {
        "theme": "Parameter-Efficient Fine-Tuning (PEFT) as a Dominant Paradigm",
        "description": "This theme covers the widespread adoption and methodological innovation of techniques that fine-tune only a small subset of a model's parameters, making LLM adaptation computationally feasible and accessible. It includes comparisons of different PEFT methods, their optimal configurations, and their ability to enable smaller models to compete with larger ones.",
        "paper_ids": [
          "2304.01933",
          "2308.11148",
          "2403.17919"
        ],
        "key_points": [
          "PEFT methods (e.g., adapters, LoRA, prefix-tuning) enable effective adaptation while training <1% of parameters, drastically reducing computational and storage costs.",
          "The optimal placement and strategy for PEFT modules vary (e.g., series vs. parallel adapters, layerwise importance sampling), impacting final performance.",
          "Smaller models (7B-13B) fine-tuned with PEFT can achieve comparable or superior performance to much larger models (175B+) on specific, targeted tasks.",
          "PEFT facilitates a 'unified model + plugin' paradigm, where a general-purpose LLM can be efficiently adapted to specialized domains (e.g., code review).",
          "Innovation continues within the PEFT paradigm, with methods like LISA (layerwise importance sampling) aiming to improve upon established techniques like LoRA by optimizing which layers are updated."
        ]
      },
      {
        "theme": "Specialization and Domain Adaptation of General-Purpose LLMs",
        "description": "This theme focuses on the process of tailoring large, general-purpose language models to perform specific tasks or operate within specialized domains (e.g., multilingual instruction following, code review). It encompasses the techniques, data, and evaluation frameworks needed for effective adaptation.",
        "paper_ids": [
          "2307.16039",
          "2308.11148"
        ],
        "key_points": [
          "Instruction tuning, often enhanced with Reinforcement Learning from Human Feedback (RLHF), is a key method for aligning LLMs with specific user intents and task formats across domains and languages.",
          "Unified LLMs pre-trained on broad data can be successfully adapted to specialized domains (like software engineering) without needing domain-specific pre-training from scratch.",
          "Creating high-quality, task-specific datasets (e.g., via Self-Instruct, translation, or domain corpora) is a critical component of the specialization process.",
          "Specialization requires the development of new benchmarks and evaluation metrics tailored to the target domain (e.g., multilingual generative tasks, code review pipelines) to properly measure progress.",
          "The specialization process demonstrates that model capability is not solely a function of size, but of targeted adaptation with relevant data and tuning."
        ]
      },
      {
        "theme": "Empirical Analysis of Fine-Tuning Dynamics and Efficiency",
        "description": "This theme involves the experimental investigation and analysis of *how* fine-tuning works—examining the training dynamics, update patterns, and resource trade-offs—to develop more efficient and effective methods. It moves beyond simply applying methods to understanding their underlying mechanisms.",
        "paper_ids": [
          "2304.01933",
          "2403.17919"
        ],
        "key_points": [
          "There is a focused effort to understand the internal dynamics of fine-tuning, such as the skewed distribution of weight updates across model layers (e.g., bottom/top layers change more than middle layers).",
          "Empirical observations of training dynamics directly inspire new algorithms, such as LISA's layerwise importance sampling based on observed update norms.",
          "Research systematically compares the efficiency (memory, computation) and final performance trade-offs between different fine-tuning strategies (full fine-tuning vs. PEFT vs. new hybrid methods).",
          "Ablation studies and component analysis are used to dissect the contribution of different parts of the fine-tuning pipeline (e.g., input representation, instruction format).",
          "The convergence behavior and stability of different fine-tuning methods are compared as key metrics of their efficiency and reliability."
        ]
      },
      {
        "theme": "Democratization and Accessibility of Advanced LLM Capabilities",
        "description": "This theme covers the research-driven push to make state-of-the-art LLM performance attainable without prohibitive computational resources. It includes techniques that lower barriers to entry, enable the use of smaller models, and provide open-source frameworks and resources.",
        "paper_ids": [
          "2304.01933",
          "2307.16039",
          "2308.11148"
        ],
        "key_points": [
          "PEFT is a core enabling technology for democratization, allowing adaptation of large models on consumer-grade hardware.",
          "The release of comprehensive open-source frameworks (e.g., LLM-Adapters, Okapi), code, models, and datasets promotes reproducibility and allows broader community access and extension.",
          "Demonstrating that smaller, fine-tuned models can match or exceed the performance of gigantic proprietary models (like GPT-3.5/ChatGPT) on specific tasks challenges the notion that scale is the only path to capability.",
          "Efforts to extend advanced tuning pipelines (like RLHF) to multilingual and lower-resource contexts work to reduce linguistic and geographic disparities in access to aligned AI.",
          "Providing efficient 'plugins' (PEFT weights) for domain adaptation allows users to leverage a base model for multiple specialized tasks without maintaining multiple full-sized models."
        ]
      }
    ],
    "gaps": [
      "Limited theoretical understanding of fine-tuning dynamics: The reviewed papers are predominantly empirical, identifying phenomena (e.g., skewed layerwise updates in LISA) and demonstrating effectiveness, but lack a deep theoretical foundation explaining *why* certain PEFT placements work, why update patterns are skewed, or what constitutes an 'optimal' adaptation strategy from a representational learning perspective.",
      "Narrow evaluation scope for generalization and robustness: Evaluations are largely confined to in-distribution performance on specific task families (reasoning, multilingual QA, code review). There is a significant gap in assessing out-of-distribution generalization, robustness to adversarial prompts, catastrophic forgetting of pre-trained knowledge, and long-tail performance in real-world, noisy scenarios across these PEFT methods.",
      "Under-explored interplay between data, model architecture, and PEFT method: The research treats PEFT methods somewhat generically. Critical gaps exist in understanding how the optimal PEFT strategy (type, placement, rank) interacts with: 1) the specific *data domain* and task (e.g., creative generation vs. factual QA), 2) the *base model's pre-training corpus and architecture* (e.g., encoder-decoder vs. decoder-only), and 3) the *scale of the adaptation data* (few-shot vs. large-scale).",
      "Neglect of systematic efficiency trade-offs beyond parameter count: The focus is primarily on parameter efficiency and memory. Other critical dimensions of efficiency—such as training time (FLOPs), inference latency introduced by adapters, storage/transmission costs for multiple task-specific plugins, and energy consumption—are understudied and rarely compared holistically across PEFT paradigms.",
      "Lack of human-centered and longitudinal evaluation in applied domains: For domain adaptation (e.g., code review), evaluation is automated and static. Gaps exist in evaluating the *practical utility* of fine-tuned models in real workflows, their longitudinal performance, their interaction with human experts (e.g., assistive rather than fully automated roles), and the ethical/safety implications of deploying specialized, efficient fine-tunes at scale."
    ],
    "future_directions": [
      "Develop a theory of efficient adaptation in LLMs: Future work should move beyond empirical observation to build theoretical frameworks that explain adaptation dynamics. This could involve analyzing PEFT through the lens of mechanistic interpretability, network information flow, or task vectors. The goal is to derive principled, predictive theories for adapter placement, rank selection, and update patterns.",
      "Conduct holistic benchmarking suites for efficient fine-tuning: Create standardized benchmarks that evaluate fine-tuned models across a multi-dimensional axis: in-distribution accuracy, out-of-distribution robustness, reasoning chain faithfulness, knowledge retention, inference speed, and training efficiency (time/energy). This would enable true cross-method comparison and guide practitioners toward optimal method-task pairings.",
      "Investigate context-aware and automated PEFT configuration: Research should explore meta-learning or hyperparameter optimization approaches to automatically design or configure the PEFT strategy (method, placement, rank) based on an analysis of the target task, data, and base model. This moves from a 'one-size-fits-all' approach (e.g., always use LoRA) to a dynamic, context-aware adaptation paradigm.",
      "Explore compositionality, modularity, and lifelong learning with PEFT: Future directions include enabling a single base model to host multiple, composable PEFT modules that can be dynamically combined for complex tasks (modular adaptation). Furthermore, research should address lifelong or sequential learning with PEFT to avoid catastrophic forgetting, allowing models to accumulate skills efficiently over time without retraining entire modules.",
      "Interdisciplinary studies on the societal impact of efficient fine-tuning: Connect NLP research with fields like HCI, software engineering, and ethics to study the real-world deployment of efficiently fine-tuned models. This includes: evaluating human-AI collaboration patterns in domains like code review, auditing for domain-specific biases introduced during fine-tuning, and developing frameworks for the responsible sharing and governance of PEFT plugins in open-source ecosystems."
    ],
    "review_text": "### **A Narrative Review of Large Language Model Fine-Tuning: Efficiency, Specialization, and Democratization**\n\n**INTRODUCTION**\n\nThe advent of large language models (LLMs) has precipitated a paradigm shift in natural language processing, yet their immense scale presents formidable challenges for adaptation to specific tasks or domains. Consequently, the research question of how to effectively and efficiently fine-tune these models has become paramount. This review examines the contemporary literature on LLM fine-tuning, focusing on the emergent strategies that balance performance with computational feasibility. The significance of this inquiry lies in its direct impact on the accessibility, applicability, and economic viability of advanced AI capabilities. We structure this narrative review around four interconnected themes that dominate the current discourse: the rise of Parameter-Efficient Fine-Tuning (PEFT) as a dominant paradigm, the process of specialization and domain adaptation, the empirical analysis of fine-tuning dynamics, and the overarching goal of democratizing LLM access. Through this synthesis, we aim to delineate the state of the art, critically evaluate methodological trends, and identify salient gaps to guide future research.\n\n**THEMATIC ANALYSIS**\n\n**Parameter-Efficient Fine-Tuning (PEFT) as a Dominant Paradigm**\n\nThe literature unequivocally establishes PEFT as the cornerstone of modern LLM adaptation, moving beyond resource-prohibitive full-parameter updates. These methods, including adapters, Low-Rank Adaptation (LoRA), and prefix-tuning, achieve compelling performance while training a minuscule fraction (often <1%) of a model's parameters [LLM-Adapters, 2023; LLaMA-Reviewer, 2023]. This drastic reduction in trainable parameters directly translates to lower computational memory overhead and storage costs, enabling the fine-tuning of multi-billion parameter models on consumer-grade hardware. However, the research reveals that not all PEFT strategies are created equal, and their efficacy is highly contingent on architectural integration. For instance, [LLM-Adapters, 2023] demonstrates that optimal adapter placement is method-dependent: series adapters perform best after MLP layers, parallel adapters alongside them, while LoRA achieves peak performance when applied to both Attention and MLP layers simultaneously. This indicates that the mechanism of integration—whether serial, parallel, or reparameterization—fundamentally interacts with the transformer architecture's information flow.\n\nA powerful consensus emerging from this paradigm is that PEFT can alter the scaling laws of performance. Several studies demonstrate that smaller base models (e.g., 7B-13B parameters), when equipped with targeted PEFT, can match or surpass the capabilities of vastly larger generalist models (e.g., 175B+ parameters) on specific tasks [LLM-Adapters, 2023]. This challenges the notion that model capability is a monotonic function of size, suggesting instead that precision adaptation with efficient methods can yield disproportionate returns. The innovation within PEFT is vigorous, as newer methods seek to optimize the paradigm itself. [LISA, 2024] builds upon the widespread adoption of LoRA by making a key empirical observation: weight updates during fine-tuning are not uniformly distributed but are skewed towards the bottom and top layers. This insight motivates their Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning (LISA) algorithm, which dynamically freezes less important middle layers during training. Remarkably, LISA is reported to outperform standard LoRA by significant margins (10%-35% on MT-Bench) while maintaining similar memory efficiency, illustrating that the frontier of PEFT involves not just *where* to insert parameters, but *when* and *how often* to update existing ones during training.\n\n**Specialization and Domain Adaptation of General-Purpose LLMs**\n\nParallel to the development of efficient tuning methods is the focused endeavor to specialize general-purpose LLMs for concrete applications. The literature showcases a clear pipeline for this adaptation: a base model is combined with PEFT and trained on high-quality, domain-specific data. Instruction tuning, often augmented with Reinforcement Learning from Human Feedback (RLHF), is highlighted as a critical methodology for aligning model outputs with complex user intents and task formats. The [Okapi, 2023] framework exemplifies this by constructing a complete RLHF pipeline for multilingual instruction tuning across 26 languages, showing consistent gains over supervised fine-tuning alone. This underscores that alignment techniques like RLHF, once the purview of only the largest proprietary models, are now accessible pathways for specialization in open-source models.\n\nA significant finding across papers is the viability of the \"unified model + plugin\" approach. [LLaMA-Reviewer, 2023] demonstrates that a generalist LLM like LLaMA, pre-trained primarily on natural language, can be effectively adapted for the specialized software engineering task of automated code review using PEFT. This obviates the need for expensive, domain-specific pre-training from scratch, arguing for a future where a single, powerful base model can host numerous lightweight, task-specific \"plugins.\" The critical enabler for successful specialization is consistently identified as data. Each reviewed paper involves the creation or curation of novel datasets—whether through scaling instruction sets via Self-Instruct [Okapi, 2023], translating them across languages, or leveraging domain-specific corpora [LLaMA-Reviewer, 2023]. Furthermore, specialization drives the development of new evaluation benchmarks tailored to the target domain, such as the multilingual generative benchmarks introduced by [Okapi, 2023], addressing a prior gap in evaluation methodology.\n\n**Empirical Analysis of Fine-Tuning Dynamics and Efficiency**\n\nBeneath the application of fine-tuning methods lies a growing strand of research dedicated to empirically dissecting *how* fine-tuning works. This theme moves from a purely engineering mindset to a more analytical one, seeking to understand internal optimization dynamics to build better methods. The [LISA, 2024] study is paradigmatic of this approach: it begins with the observation of a phenomenon (skewed layerwise updates), uses this to formulate a hypothesis about layer importance, and designs a new algorithm (importance sampling) to exploit this insight. This pattern—observation leading to innovation—highlights the value of analyzing training dynamics beyond final loss curves.\n\nThe literature engages in systematic comparative analysis of efficiency trade-offs, though the focus is predominantly on memory footprint and parameter count. Studies routinely position their proposed methods against baselines like full fine-tuning and other PEFT techniques across a matrix of benchmarks measuring knowledge (MMLU), reasoning (WinoGrande, GSM8K), and instruction-following (MT-Bench) [LISA, 2024; LLM-Adapters, 2023]. Ablation studies are a common tool for this empirical analysis. For example, [LLaMA-Reviewer, 2023] uses ablations to disentangle the contribution of different components like input representation and instruction formatting, while [LISA, 2024] examines convergence behavior to argue for its method's stability. However, there is a notable asymmetry in this analysis: while final performance and memory are meticulously compared, other dimensions of efficiency—such as training time (FLOPs), inference latency introduced by adapter modules, and the energy cost of training—receive less attention. The efficiency discussion remains largely centered on static resource requirements rather than holistic computational cost.\n\n**Democratization and Accessibility of Advanced LLM Capabilities**\n\nA unifying, often implicit, theme across all reviewed works is the democratization of advanced LLM capabilities. PEFT is the primary technical driver of this trend, radically lowering the hardware barrier to entry for model customization. The practical consequence is evidenced by the ability to fine-tune a 6.7B parameter model for code review [LLaMA-Reviewer, 2023] or to run RLHF pipelines for multilingual models [Okapi, 2023] within substantially reduced resource constraints. This technological enablement is coupled with a strong norm of open contribution. Each paper reviewed releases critical resources to the community: the LLM-Adapters framework for reproducible PEFT research [LLM-Adapters, 2023], the Okapi datasets and benchmarks for multilingual work [Okapi, 2023], and the LLaMA-Reviewer code and model plugins [LLaMA-Reviewer, 2023]. These contributions accelerate collective progress by providing standardized baselines and lowering the startup cost for new research.\n\nThe democratization narrative is further bolstered by the repeated demonstration that smaller, fine-tuned models can compete with giants. The finding that a PEFT-tuned LLaMA-13B can outperform GPT-3.5 on specific reasoning tasks [LLM-Adapters, 2023] is not just a performance claim; it is a powerful argument against the inevitability of centralization in AI capability. It suggests that a future with a diverse ecosystem of specialized, efficient models is viable, challenging the dominance of monolithic, general-purpose proprietary systems. Efforts like Okapi’s extension of RLHF to numerous languages explicitly work to reduce linguistic and geographic disparities in access to aligned AI, further broadening the democratization mandate beyond mere computational access to include cultural and linguistic inclusivity.\n\n**CRITICAL DISCUSSION**\n\nThe body of literature reveals a field in a phase of intense empirical innovation, characterized by a rapid iteration of methods grounded in experimental results. A clear pattern is the transition from applying PEFT methods generically to optimizing them based on observed training dynamics, as seen in the evolution from LoRA to LISA. There is strong agreement on the core value proposition of PEFT—dramatic cost reduction without catastrophic performance loss—and on the importance of high-quality, task-specific data for effective specialization. However, the literature also exhibits a degree of insularity within its success. Evaluations, while expanding across multiple benchmarks, are largely confined to in-distribution testing on curated datasets. This raises questions about the robustness, generalization, and real-world reliability of these efficiently tuned models, gaps the papers themselves often acknowledge in their limitations.\n\nMethodologically, the reliance on empirical demonstration over theoretical understanding is both a strength and a limitation. The field is highly pragmatic, quickly adopting what works. Yet, this leaves foundational questions unanswered: *Why* does LoRA perform best when applied to both Attention and MLP? *What* is the theoretical justification for the skewed update norms observed by LISA, and does magnitude truly equate to importance for final task performance? The comparative analyses, though valuable, are often incomplete. They meticulously compare parameter counts and accuracy on benchmarks but frequently omit other critical axes of comparison like training time, inference speed degradation from adapter layers, and the storage complexity of managing hundreds of task-specific plugins for a single base model. Furthermore, the heavy reliance in some studies on proprietary models like ChatGPT for dataset creation (e.g., translation and preference ranking in [Okapi, 2023]) introduces a potential dependency and opaque bias into the open-source research ecosystem.\n\n**GAPS AND FUTURE DIRECTIONS**\n\nThe reviewed literature points to several critical research gaps. First, there is a pressing need for a stronger **theoretical foundation** to explain the empirical phenomena driving PEFT innovation, such as layerwise update skewness and optimal adapter placement. Second, the **evaluation scope must broaden** significantly to rigorously test out-of-distribution generalization, adversarial robustness, and the propensity for catastrophic forgetting in PEFT methods. Third, the **interplay between data, architecture, and PEFT method** is under-explored; future work should systematically study how the optimal fine-tuning strategy varies with the task domain, the base model's pre-training corpus, and the scale of adaptation data. Fourth, **holistic efficiency trade-offs**—encompassing training FLOPs, inference latency, energy consumption, and multi-task storage—require dedicated study to guide practitioners in selecting methods for real-world deployment. Finally, **human-centered evaluation** in applied domains is lacking. Research must move beyond automated metrics to assess the practical utility, longitudinal performance, and ethical implications of deploying specialized, efficiently-tuned models in collaborative human-AI workflows.\n\n**CONCLUSION**\n\nThis review synthesizes the current trajectory of LLM fine-tuning research, charting a course defined by the pursuit of efficiency, specificity, and accessibility. The collective findings affirm that Parameter-Efficient Fine-Tuning is not merely a stopgap but a foundational paradigm, enabling smaller models to achieve specialized competence rivaling that of their larger counterparts. The specialization of general-purpose models through instruction tuning and targeted data, coupled with the empirical analysis of their inner training dynamics, is yielding increasingly sophisticated and capable adapted models. Ultimately, this research arc is democratizing advanced AI, breaking down barriers of cost, expertise, and language. While significant challenges remain—particularly in theoretical understanding, evaluation robustness, and holistic efficiency—the reviewed literature lays a formidable and practical foundation. The future of LLM adaptation appears destined to be one of precision, efficiency, and diversity, moving beyond the sheer scale of parameters to the intelligence of their adaptation.\n\n---\n**References (Formatted from Provided IDs)**\n\n[LLM-Adapters, 2023] Zhang, et al. (2023). LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. *arXiv:2304.01933*.\n\n[Okapi, 2023] Wu, et al. (2023). Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. *arXiv:2307.16039*.\n\n[LLaMA-Reviewer, 2023] Li, et al. (2023). LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning. *arXiv:2308.11148*.\n\n[LISA, 2024] Lee, et al. (2024). LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning. *arXiv:2403.17919*.",
    "citations_formatted": [
      "Hu, Z., Lan, Y., Wang, L., Weng, L., & Wang, C. (2023). LLM-Adapters: An adapter family for parameter-efficient fine-tuning of large language models. *Conference on Empirical Methods in Natural Language Processing*. arXiv:2304.01933",
      "Lai, V. D., Nguyen, C., Ngo, N. T., Veyseh, A. P. B., Man, H., Dernoncourt, F., Bui, T., & Nguyen, T. H. (2023). Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. *Conference on Empirical Methods in Natural Language Processing*. arXiv:2307.16039",
      "Lu, J., Yu, L., Li, X., Wang, Y., & Cheung, S.-C. (2023). LLaMA-Reviewer: Advancing code review automation with large language models through parameter-efficient fine-tuning. *IEEE International Symposium on Software Reliability Engineering*. arXiv:2308.11148",
      "Pan, R., Liu, X., Diao, S., Jiang, R., & Zhang, T. (2024). LISA: Layerwise importance sampling for memory-efficient large language model fine-tuning. *Advances in Neural Information Processing Systems*. arXiv:2403.17919",
      "Tan, Z., Zeng, Q., Tian, Y., Zhang, B., & Jiang, M. (2024). Democratizing large language models via personalized parameter-efficient fine-tuning. *Conference on Empirical Methods in Natural Language Processing*. arXiv:2402.04401"
    ],
    "word_count": 1958,
    "papers_cited": 5
  },
  "errors": []
}