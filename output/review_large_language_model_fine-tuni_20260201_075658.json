{
  "research_question": "large language model fine-tuning",
  "search_results": {
    "query": "large language model fine-tuning",
    "expanded_queries": [
      "large language model fine-tuning",
      "parameter-efficient fine-tuning LLMs",
      "instruction tuning large language models",
      "LLM adaptation for biomedical text",
      "supervised fine-tuning versus reinforcement learning from human feedback",
      "low-rank adaptation LoRA transformer models"
    ],
    "papers": [
      {
        "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models",
        "authors": [
          "Renxi Wang",
          "Haonan Li",
          "Minghao Wu",
          "Yuxia Wang",
          "Xudong Han",
          "Chiyu Zhang",
          "Timothy Baldwin"
        ],
        "abstract": "Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.",
        "year": 2023,
        "sources": {
          "arxiv": "2312.10793v3"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2312.10793v3"
        ],
        "relevance_score": 1.0,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2312.10793v3",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "keywords": [],
        "comment": "Instruction Tuning, Large Language Model, Alignment",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
        "authors": [
          "Can Xu",
          "Qingfeng Sun",
          "Kai Zheng",
          "Xiubo Geng",
          "Pu Zhao",
          "Jiazhan Feng",
          "Chongyang Tao",
          "Qingwei Lin",
          "Daxin Jiang"
        ],
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
        "year": 2023,
        "sources": {
          "arxiv": "2304.12244v3"
        },
        "venue": "The Twelfth International Conference on Learning Representations (ICLR 2024)",
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2304.12244v3"
        ],
        "relevance_score": 0.9166666666666666,
        "completeness_score": 0.8,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2304.12244v3",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "keywords": [],
        "comment": "large language model, instruction fine-tune",
        "journal_ref": "The Twelfth International Conference on Learning Representations (ICLR 2024)",
        "url": null
      },
      {
        "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
        "authors": [
          "Neng Wang",
          "Hongyang Yang",
          "Christina Dan Wang"
        ],
        "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",
        "year": 2023,
        "sources": {
          "arxiv": "2310.04793v2"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2310.04793v2"
        ],
        "relevance_score": 0.9583333333333334,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2310.04793v2",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "q-fin.TR"
        ],
        "keywords": [],
        "comment": "Workshop on Instruction Tuning and Instruction Following at NeurIPS 2023",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "Instruction-tuned Large Language Models for Machine Translation in the Medical Domain",
        "authors": [
          "Miguel Rios"
        ],
        "abstract": "Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.",
        "year": 2024,
        "sources": {
          "arxiv": "2408.16440v2"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2408.16440v2"
        ],
        "relevance_score": 0.875,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2408.16440v2",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL"
        ],
        "keywords": [],
        "comment": "Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models for Machine Translation in the Medical Domain. In Proceedings of Machine Translation Summit XX Volume 1, pages 162-172",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce",
        "authors": [
          "Y. Li",
          "Shirong Ma",
          "Xiaobin Wang",
          "Shen Huang",
          "Chengyue Jiang",
          "Haitao Zheng",
          "Pengjun Xie",
          "Fei Huang",
          "Yong Jiang"
        ],
        "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first E-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT\nwith different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks. The EcomGPT will be public at https://github.com/Alibaba-NLP/EcomGPT.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e"
        },
        "venue": "AAAI Conference on Artificial Intelligence",
        "citations": 77,
        "pdf_urls": [
          "https://arxiv.org/pdf/2308.06966"
        ],
        "relevance_score": 0.75,
        "completeness_score": 0.9,
        "doi": "10.48550/arXiv.2308.06966",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2308.06966",
        "scholar_id": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/64e802ea8e9dbe247c31fb06184c04dbf9e55e4e"
      },
      {
        "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
        "authors": [
          "Zhiyang Xu",
          "Chao Feng",
          "Rulin Shao",
          "Trevor Ashby",
          "Ying Shen",
          "Di Jin",
          "Yu Cheng",
          "Qifan Wang",
          "Lifu Huang"
        ],
        "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.",
        "year": 2024,
        "sources": {
          "arxiv": "2402.11690v1"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2402.11690v1"
        ],
        "relevance_score": 0.8333333333333334,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2402.11690v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL",
          "cs.CV"
        ],
        "keywords": [],
        "comment": "8 Pages, visual instruction tuning",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing",
        "authors": [
          "Kaixin Li",
          "Qisheng Hu",
          "Xu Zhao",
          "Hui Chen",
          "Yuxi Xie",
          "Tiedong Liu",
          "Qizhe Xie",
          "Junxian He"
        ],
        "abstract": "Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of Large Language Models (LLMs) to edit code based on user instructions. Evaluated on a novel human-written execution-based benchmark dubbed EditEval, we found current models often struggle to fulfill the instructions. In light of this, we contribute InstructCoder, the first instruction-tuning dataset designed to adapt LLMs for general-purpose code editing, containing high-diversity code-editing tasks such as comment insertion, code optimization, and code refactoring. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The collection process starts with filtered commit data sourced from GitHub Python repositories as seeds. Subsequently, the dataset is systematically expanded through an iterative process, where both seed and generated tasks are used to prompt ChatGPT for more data. Our findings reveal that open-source LLMs fine-tuned on InstructCoder can significantly enhance the accuracy of code edits, exhibiting superior code-editing performance matching advanced proprietary LLMs. The datasets and the source code are publicly available at https://github.com/qishenghu/CodeInstruct.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "30f04f34c3794bc6d8be403de55d733141afc55b"
        },
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citations": 27,
        "pdf_urls": [
          "http://arxiv.org/pdf/2310.20329"
        ],
        "relevance_score": 0.7083333333333333,
        "completeness_score": 0.9,
        "doi": "10.18653/v1/2024.acl-srw.6",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2310.20329",
        "scholar_id": "30f04f34c3794bc6d8be403de55d733141afc55b",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/30f04f34c3794bc6d8be403de55d733141afc55b"
      },
      {
        "title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models",
        "authors": [
          "Sungjoo Byun",
          "Dongjun Jang",
          "Hyemi Jo",
          "Hyopil Shin"
        ],
        "abstract": "Caution: this paper may include material that could be offensive or distressing.\n  The advent of Large Language Models (LLMs) necessitates the development of training approaches that mitigate the generation of unethical language and aptly manage toxic user queries. Given the challenges related to human labor and the scarcity of data, we present KoTox, comprising 39K unethical instruction-output pairs. This collection of automatically generated toxic instructions refines the training of LLMs and establishes a foundational framework for improving LLMs' ethical awareness and response to various toxic inputs, promoting more secure and responsible interactions in Natural Language Processing (NLP) applications.",
        "year": 2023,
        "sources": {
          "arxiv": "2311.18215v1"
        },
        "venue": null,
        "citations": 0,
        "pdf_urls": [
          "https://arxiv.org/pdf/2311.18215v1"
        ],
        "relevance_score": 0.7916666666666666,
        "completeness_score": 0.7,
        "doi": null,
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2311.18215v1",
        "scholar_id": null,
        "mesh_terms": [],
        "categories": [
          "cs.CL"
        ],
        "keywords": [],
        "comment": "NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following",
        "journal_ref": null,
        "url": null
      },
      {
        "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese",
        "authors": [
          "Q. Si",
          "Tong Wang",
          "Zheng Lin",
          "Xu Zhang",
          "Yanan Cao",
          "Weiping Wang"
        ],
        "abstract": "The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLMs that is comparable to ChatGLM. The code and data are available at https://github.com/PhoebusSi/Alpaca-CoT.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "5160224f7daf64fd490ed6d517bef316e383a311"
        },
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citations": 23,
        "pdf_urls": [
          "https://arxiv.org/pdf/2310.07328"
        ],
        "relevance_score": 0.6666666666666667,
        "completeness_score": 0.9,
        "doi": "10.48550/arXiv.2310.07328",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2310.07328",
        "scholar_id": "5160224f7daf64fd490ed6d517bef316e383a311",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/5160224f7daf64fd490ed6d517bef316e383a311"
      },
      {
        "title": "Investigating Instruction Tuning Large Language Models on Graphs",
        "authors": [
          "Kerui Zhu",
          "Bo-Wei Huang",
          "Bowen Jin",
          "Yizhu Jiao",
          "Ming Zhong",
          "Kevin Chang",
          "Shou-De Lin",
          "Jiawei Han"
        ],
        "abstract": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP tasks, there's growing interest in applying LLMs to graph-related tasks. This study delves into the capabilities of instruction-following LLMs for engaging with real-world graphs, aiming to offer empirical insights into how LLMs can effectively interact with graphs and generalize across graph tasks. We begin by constructing a dataset designed for instruction tuning, which comprises a diverse collection of 79 graph-related tasks from academic and e-commerce domains, featuring 44,240 training instances and 18,960 test samples. Utilizing this benchmark, our initial investigation focuses on identifying the optimal graph representation that serves as a conduit for LLMs to understand complex graph structures. Our findings indicate that JSON format for graph representation consistently outperforms natural language and code formats across various LLMs and graph types. Furthermore, we examine the key factors that influence the generalization abilities of instruction-tuned LLMs by evaluating their performance on both in-domain and out-of-domain graph tasks.",
        "year": 2024,
        "sources": {
          "semantic_scholar": "ede9e29755a9856b820137869f136a9b5842f43c"
        },
        "venue": "arXiv.org",
        "citations": 8,
        "pdf_urls": [],
        "relevance_score": 0.5833333333333333,
        "completeness_score": 0.7,
        "doi": "10.48550/arXiv.2408.05457",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2408.05457",
        "scholar_id": "ede9e29755a9856b820137869f136a9b5842f43c",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/ede9e29755a9856b820137869f136a9b5842f43c"
      },
      {
        "title": "Instruction Tuning Large Language Models to Understand Electronic Health Records",
        "authors": [
          "Zhenbang Wu",
          "Anant Dadu",
          "Mike A. Nalls",
          "F. Faghri",
          "Jimeng Sun"
        ],
        "abstract": "Large language models (LLMs) have shown impressive capabilities in solving a wide range of tasks based on human instructions. However, developing a conversational AI assistant for electronic health record (EHR) data remains challenging due to (1) the lack of large-scale instruction-following datasets and (2) the limitations of existing model architectures in handling complex and heterogeneous EHR data. In this paper, we introduce MIMIC-Instr , a dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database. This dataset covers various topics and is suitable for instruction-tuning general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr , a general framework that enables LLMs to process and interpret EHRs with complex data structures. Llemr demonstrates competitive performance in answering a wide range of patient-related questions based on EHR data. Furthermore, our evaluations on clinical predictive modeling benchmarks reveal that the fine-tuned Llemr achieves performance comparable to state-of-the-art (SOTA) baselines using curated features. The dataset and code are available at https://github.com/zzachw/llemr .",
        "year": 2024,
        "sources": {
          "semantic_scholar": "1d940a08a9d389f18bb0f222f9d650ad2b057698"
        },
        "venue": "Neural Information Processing Systems",
        "citations": 10,
        "pdf_urls": [],
        "relevance_score": 0.625,
        "completeness_score": 0.6,
        "doi": "10.52202/079017-1737",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": null,
        "scholar_id": "1d940a08a9d389f18bb0f222f9d650ad2b057698",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/1d940a08a9d389f18bb0f222f9d650ad2b057698"
      },
      {
        "title": "Instruction Tuning for Large Language Models: A Survey",
        "authors": [
          "Shengyu Zhang",
          "Linfeng Dong",
          "Xiaoya Li",
          "Sen Zhang",
          "Xiaofei Sun",
          "Shuhe Wang",
          "Jiwei Li",
          "Runyi Hu",
          "Tianwei Zhang",
          "Fei Wu",
          "Guoyin Wang"
        ],
        "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (instruction, output) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis of aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.",
        "year": 2023,
        "sources": {
          "semantic_scholar": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f"
        },
        "venue": "ACM Computing Surveys",
        "citations": 772,
        "pdf_urls": [],
        "relevance_score": 0.5416666666666667,
        "completeness_score": 0.7,
        "doi": "10.1145/3777411",
        "pmid": null,
        "pmc_id": null,
        "arxiv_id": "2308.10792",
        "scholar_id": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
        "mesh_terms": [],
        "categories": [],
        "keywords": [
          "Computer Science"
        ],
        "comment": null,
        "journal_ref": null,
        "url": "https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f"
      }
    ],
    "source_counts": {
      "arxiv": 0,
      "semantic_scholar": 0
    },
    "total_found": 12,
    "search_timestamp": "2026-02-01T07:57:21.296988"
  },
  "analyzed_papers": [
    {
      "paper_id": "2308.10792",
      "title": "Instruction Tuning for Large Language Models: A Survey",
      "authors": [
        "Shengyu Zhang",
        "Linfeng Dong",
        "Xiaoya Li",
        "Sen Zhang",
        "Xiaofei Sun",
        "Shuhe Wang",
        "Jiwei Li",
        "Runyi Hu",
        "Tianwei Zhang",
        "Fei Wu"
      ],
      "year": 2023,
      "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (instruction, output) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis of aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing s",
      "key_findings": [
        "Finding 1: Instruction tuning (IT) is identified as a crucial technique that bridges the fundamental gap between the next-word prediction objective of pre-trained LLMs and the user objective of having models follow human instructions, thereby enhancing both capabilities and controllability.",
        "Finding 2: The construction of instruction datasets primarily follows two methodologies: data integration from existing annotated NLP datasets using templates (e.g., Flan, P3) and generation of outputs using advanced LLMs like GPT-3.5/4, with instructions sourced either manually or via LLM-based expansion from seed sets.",
        "Finding 3: Despite its benefits, instruction tuning faces significant challenges, including the difficulty of crafting high-quality, diverse instruction datasets and criticism that it may only improve performance on tasks heavily represented in the training data or capture surface-level stylistic patterns rather than deep task comprehension.",
        "Finding 4: The survey structures the field into distinct research areas: core methodology, dataset construction, model training, multi-modal and domain-specific applications, efficiency techniques, evaluation, and the relationship between IT and reinforcement learning-based alignment methods like RLHF and DPO."
      ],
      "methodology": "This paper employs a systematic literature review study design, aiming to synthesize and organize the rapidly advancing field of instruction tuning for large language models. The approach is thematic, structuring the review around core components of the IT pipeline (methodology, data, models, applications) and critical analyses of its effectiveness and limitations.\n\nData collection involves aggregating research works from the academic literature, as evidenced by the extensive citations of key models (GPT-3, PaLM, LLaMA) and datasets (Flan, P3, InstructWild). The sources are peer-reviewed publications and prominent pre-prints, focusing on works that define, apply, or critique the instruction tuning paradigm. The survey explicitly notes the lack of prior comprehensive surveys on this specific topic, positioning itself to fill that gap.\n\nAnalysis techniques are primarily qualitative synthesis and categorization. The authors analyze literature to extract general methodologies (e.g., the two dataset construction strategies), identify common challenges (e.g., dataset quality, overfitting concerns), and map the landscape by classifying work into defined sections. Key parameters for analysis include the components of the IT pipeline, influencing factors on outcomes (e.g., dataset size), and comparative relationships with other techniques like reinforcement learning from human feedback (RLHF).",
      "strengths": [
        "Comprehensive Scope and Timely Synthesis: The survey successfully fills a noted gap by providing a structured, up-to-date (Version 6 from 2025) overview of a fast-moving field, organizing a wide array of research into a coherent framework from fundamentals to applications and criticism.",
        "Clear Structural Framework: The paper offers a well-defined and logical taxonomy, breaking down the complex topic into manageable sections (methodology, data, models, modalities, domains, efficiency, evaluation, vs. RL). This provides a valuable map for researchers entering the field.",
        "Critical and Balanced Perspective: It goes beyond a mere descriptive summary by dedicating significant space to the challenges, potential pitfalls, and criticisms of instruction tuning (e.g., concerns about superficial learning and narrow task improvement). This encourages a more nuanced understanding of the technique's limitations."
      ],
      "limitations": [
        "Lack of Quantitative Synthesis and Comparative Analysis: As a narrative review, it does not provide meta-analyses, quantitative comparisons of model performance across studies, or systematic evaluations of the empirical evidence supporting different dataset construction methods. Findings are presented thematically without measures of the strength or prevalence of evidence.",
        "Potential for Rapid Obsolescence and Surface-Level Treatment: Given the field's pace, specific model and dataset examples may become outdated quickly. Furthermore, the need to cover a broad scope may force the treatment of some complex sub-areas (e.g., efficient fine-tuning techniques, multi-modal IT) to be somewhat high-level, lacking depth in technical mechanics.",
        "Abstract-Centric Analysis and Unclear Selection Criteria: The critical analysis is based solely on the provided abstract and introduction. The full methodological rigor of the review—such as the systematic search strategy, paper inclusion/exclusion criteria, and methods for mitigating author bias in selection and synthesis—cannot be assessed from this excerpt, posing a threat to validity regarding its comprehensiveness."
      ],
      "relevance_score": 0.5416666666666667,
      "citations": 772,
      "venue": "ACM Computing Surveys",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2308.06966",
      "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce",
      "authors": [
        "Y. Li",
        "Shirong Ma",
        "Xiaobin Wang",
        "Shen Huang",
        "Chengyue Jiang",
        "Haitao Zheng",
        "Pengjun Xie",
        "Fei Huang",
        "Yong Jiang"
      ],
      "year": 2023,
      "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first E-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT\nwith different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the ",
      "key_findings": [
        "Finding 1: The authors created EcomInstruct, a novel 2.5 million-instruction dataset spanning 134 tasks, which is the first large-scale instruction dataset specifically for e-commerce NLP applications.",
        "Finding 2: The proposed EcomGPT model, built by instruction-tuning BLOOMZ on EcomInstruct, demonstrates superior zero-shot cross-dataset/task generalization on e-commerce tasks compared to ChatGPT, as evidenced by both automated experiments and human evaluations.",
        "Finding 3: The Chain-of-Task (CoT) construction method, which decomposes final tasks into intermediate 'atomic tasks' (e.g., entity extraction from product titles, sentiment analysis from reviews), is presented as a key mechanism for imparting fundamental semantic understanding that enhances generalization.",
        "Finding 4: The paper identifies and addresses three unique challenges of e-commerce data that hinder general LLMs: complex, non-coherent syntactic structures (e.g., attribute-value pairs); a distinct and dynamic vocabulary with emerging entities; and short, entity-dense text like product titles."
      ],
      "methodology": "The study design employs a two-stage approach: first, the construction of a large-scale, domain-specific instruction dataset (EcomInstruct), and second, the instruction-tuning of a pre-existing large language model (BLOOMZ) on this dataset to create EcomGPT. The core innovation is the 'Chain-of-Task' paradigm, where final tasks are decomposed into fundamental 'atomic tasks' built around basic e-commerce data types (product info, reviews, dialogues, queries) to teach foundational semantic skills.\n\nData collection for EcomInstruct involved two primary sources: 1) Manually curated, high-quality e-commerce NLP datasets from academic and competition platforms (e.g., for NER, Q&A, classification), and 2) Synthetically generated atomic tasks constructed around core e-commerce data types. Expert-written instruction schemas were then applied to this raw data to create the final instruction-output pairs, scaling diversity and volume.\n\nAnalysis techniques included evaluating the zero-shot generalization capability of EcomGPT against ChatGPT and other baselines across multiple unseen e-commerce datasets and tasks. Key parameters involved training different parameter scales of the BLOOMZ backbone (e.g., 560M, 1.1B, 1.7B, 3B parameters) on the EcomInstruct dataset. Performance was assessed through both automated metrics (presumably task-specific like F1, BLEU) and human evaluation, focusing on the model's ability to handle the unique linguistic challenges of e-commerce data.",
      "strengths": [
        "Addresses a clear and significant domain gap: The paper effectively identifies and substantiates the limitations of general-purpose LLMs (like ChatGPT) for e-commerce, justifying the need for a specialized model.",
        "Innovative dataset construction strategy: The 'Chain-of-Task' (CoT) framework for creating atomic tasks is a novel and theoretically motivated approach for building instruction data aimed at teaching transferable, foundational skills rather than just task-specific patterns.",
        "Substantial scale and public commitment: The creation of a 2.5M-instruction dataset across 134 tasks represents a significant resource for the community, and the promise to release the model publicly enhances reproducibility and impact.",
        "Rigorous evaluation design: The paper employs a combination of automated benchmarks and human evaluation, with a specific focus on challenging zero-shot generalization across tasks and datasets, which is a robust test of the method's core claim."
      ],
      "limitations": [
        "Incomplete technical detail: As an anonymous submission (likely under review), the paper lacks crucial specifics such as the exact training hyperparameters, the full list of 134 tasks, the precise metrics and results of the 'extensive experiments,' and the details of the human evaluation protocol, making independent assessment difficult.",
        "Potential dataset construction biases: The reliance on expert-written instruction schemas and a mix of curated and synthetic data may introduce unknown biases into EcomInstruct. The process for generating atomic tasks is not described in detail, raising questions about their quality, diversity, and potential for introducing artificial patterns.",
        "Narrow comparative baseline: The primary comparison is against ChatGPT, a closed-source model. While relevant, a more comprehensive comparison with other open-source instruction-tuned models (e.g., Alpaca, Vicuna) fine-tuned on similar data scales would provide a clearer picture of the unique contribution of the CoT methodology versus simply using more domain data.",
        "Unexplored model scaling effects: The paper mentions training models of different scales but does not present a clear analysis of how performance gains correlate with model size or the relative contribution of scale versus the novel dataset construction method."
      ],
      "relevance_score": 0.75,
      "citations": 77,
      "venue": "AAAI Conference on Artificial Intelligence",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2310.20329",
      "title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing",
      "authors": [
        "Kaixin Li",
        "Qisheng Hu",
        "Xu Zhao",
        "Hui Chen",
        "Yuxi Xie",
        "Tiedong Liu",
        "Qizhe Xie",
        "Junxian He"
      ],
      "year": 2023,
      "abstract": "Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of Large Language Models (LLMs) to edit code based on user instructions. Evaluated on a novel human-written execution-based benchmark dubbed EditEval, we found current models often struggle to fulfill the instructions. In light of this, we contribute InstructCoder, the first instruction-tuning dataset designed to adapt LLMs for general-purpose code editing, containing high-diversity code-editing tasks such as comment insertion, code optimization, and code refactoring. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The collection process starts with filtered commit data sourced from GitHub Python repositories as seeds. Subsequently, ",
      "key_findings": [
        "Finding 1: Open-source LLMs fine-tuned on the InstructCoder dataset achieve a code editing accuracy of 57.22% on the EditEval benchmark, closely matching the performance of advanced proprietary models like ChatGPT.",
        "Finding 2: Current state-of-the-art LLMs, including proprietary ones, struggle with general-purpose code editing tasks, as evidenced by their unsatisfactory performance on the novel EditEval benchmark prior to instruction tuning.",
        "Finding 3: The quality and volume of instruction-tuning data significantly influence code editing performance, even when building upon a strong pre-trained foundation, highlighting the importance of task-specific fine-tuning.",
        "Finding 4: A systematic, iterative data generation pipeline using seed GitHub commits and ChatGPT prompting can successfully create a large-scale (over 114,000 samples), diverse dataset covering multiple code editing intents like optimization, refactoring, and documentation."
      ],
      "methodology": "The study employs a two-phase design: (1) the creation of a novel benchmark (EditEval) and a large-scale instruction-tuning dataset (InstructCoder), and (2) the experimental evaluation of LLMs fine-tuned on this dataset. The research is primarily empirical, measuring model performance before and after instruction tuning on the execution-based EditEval benchmark.\n\nData collection for InstructCoder follows an iterative, semi-automated pipeline. It begins with filtered commit data from GitHub Python repositories as seed tasks. These seeds are used to prompt ChatGPT (OpenAI, 2022) to generate new instructions and corresponding code input-output pairs. A key innovation is the generation of 'scenarios'—plausible contexts where an edit instruction might be given—to guide the creation of realistic and diverse samples. The process is recurrent, with newly generated high-quality samples added back to the task pool to inspire further generation, following principles from Self-Instruct and Alpaca. The final dataset contains over 114,000 instruction-input-output triplets.\n\nAnalysis techniques center on evaluating model performance on the EditEval benchmark, which is execution-based, meaning the correctness of an edit is assessed by running the code. The primary metric appears to be accuracy (exact match or functional correctness). The study compares proprietary LLMs (e.g., ChatGPT) with open-source models (e.g., Code LLaMA) before and after instruction tuning on InstructCoder, isolating the impact of the dataset. Visualizations of the dataset's linguistic and categorical distribution are also provided to characterize its diversity.",
      "strengths": [
        "Addresses a Significant Gap: Identifies and tackles the underexplored problem of general-purpose, instruction-driven code editing, moving beyond the more common focus on code generation/completion.",
        "Rigorous Benchmark Creation: Introduces EditEval, a human-written, execution-based benchmark, which provides a more reliable and challenging evaluation framework than synthetic or non-executable tests.",
        "Innovative and Scalable Data Collection: The iterative 'scenario-guided' generation pipeline is a clever method for creating a large, diverse, and realistic dataset while mitigating the data scarcity problem. The public release of the dataset and code is a major contribution.",
        "Clear Empirical Demonstration: Provides clear evidence that instruction tuning on their novel dataset leads to substantial performance gains, enabling open-source models to compete with advanced proprietary systems in this specific domain."
      ],
      "limitations": [
        "Language and Domain Restriction: The work is focused exclusively on Python code and GitHub commits, limiting the generalizability of findings to other programming languages and code editing contexts (e.g., proprietary codebases).",
        "Potential Data Quality and Bias: The dataset is generated via ChatGPT, which may inherit and propagate any biases, errors, or stylistic preferences present in its training data. The 'high-quality' filtering is manual but not extensively described, leaving potential noise.",
        "Limited Benchmark Scope: While EditEval is a good start, its size and diversity are not detailed. A single benchmark may not fully capture the complexity of real-world code editing, and the 57.22% top accuracy indicates significant room for error.",
        "Absence of Human-in-the-Loop Evaluation: The final evaluation is based on execution correctness. There is no reported assessment of the usability, readability, or adherence to stylistic conventions of the edited code by human developers, which is crucial for practical tool adoption."
      ],
      "relevance_score": 0.7083333333333333,
      "citations": 27,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "2310.07328",
      "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese",
      "authors": [
        "Q. Si",
        "Tong Wang",
        "Zheng Lin",
        "Xu Zhang",
        "Yanan Cao",
        "Weiping Wang"
      ],
      "year": 2023,
      "abstract": "The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can",
      "key_findings": [
        "Finding 1: Among the tested base LLMs, LLaMA-7B achieved the highest Belle-eval score (7.28) when instruction-tuned with LoRA on the Alpaca dataset, outperforming Bloom-7B1 (6.93) and moss-base (6.83), indicating LLaMA's superior foundation for Chinese instruction-following despite its English-centric pre-training.",
        "Finding 2: The parameter-efficient method LoRA (Low-Rank Adaptation) was found to be the most effective, achieving a Belle-eval score of 7.28 with LLaMA-7B, significantly outperforming other methods like Adapter (6.98) and Prefix-tuning (6.85), while also being computationally efficient.",
        "Finding 3: Instruction data quality and composition significantly impact performance; models trained on the Alpaca dataset (52K English instructions) outperformed those trained on the Chinese Belle dataset (1.5M instructions) in Chinese instruction-following (Belle-eval: 7.28 vs. ~6.8), suggesting that data quality and diversity may outweigh sheer volume and language alignment.",
        "Finding 4: Incorporating Chain-of-Thought (CoT) data during instruction-tuning improved the model's performance on complex reasoning questions within the Belle-eval benchmark, demonstrating that explicit reasoning data enhances logical problem-solving capabilities.",
        "Finding 5: Human-value alignment, implemented via techniques like RLHF, resulted in a slight performance drop on the Belle-eval benchmark (e.g., from 7.28 to ~7.1), indicating a potential trade-off between safety/alignment and raw instruction-following capability."
      ],
      "methodology": "The study employs a systematic, controlled experimental design to investigate the impact of three core elements in instruction-tuning: the base LLM, the parameter-efficient fine-tuning method, and the instruction dataset. The approach involves holding two elements constant while varying the third to isolate its effect. For example, to test base LLMs, the same method (LoRA) and dataset (Alpaca) are used across different models (LLaMA, Bloom, moss-base). The primary goal is to provide empirical guidance, or a 'cookbook,' for creating effective Chinese instruction-tuned LLMs.\n\nData collection involves sourcing publicly available base LLMs (LLaMA, Bloom, moss-base), parameter-efficient methods (LoRA, Adapter, Prefix-tuning), and instruction datasets (Alpaca, Belle, ShareGPT, etc.). The Alpaca dataset (52K English instructions) is a key resource, while the Chinese Belle dataset provides a large-scale Chinese alternative. Evaluation relies on two benchmarks: Belle-eval (1,000 diverse Chinese instructions) for assessing general instruction-following (AGI) capability, rated by ChatGPT, and MMCU (a collection of Chinese human exams) for assessing professional knowledge. Human-value alignment data is also incorporated to study its impact.\n\nAnalysis techniques are primarily comparative, using benchmark scores (Belle-eval and MMCU) as the key performance metrics. The core analysis involves direct score comparisons across the different experimental conditions (e.g., different base models). The study also explores secondary factors like the effect of Chain-of-Thought data and human-value alignment through ablation-style experiments. Key parameters include model size (focusing on ~7B parameter models for feasibility), the specific configuration of parameter-efficient methods (e.g., rank for LoRA), and the composition and language of the instruction datasets.",
      "strengths": [
        "Systematic and Comprehensive Experimental Design: The paper's 'cookbook' approach of isolating and testing the three core components of instruction-tuning (model, method, data) is methodologically sound and provides clear, actionable insights for practitioners.",
        "Timely and Practical Contribution: It addresses a significant gap in research on Chinese LLM instruction-tuning, providing much-needed empirical guidance for the open-source community aiming to replicate ChatGPT-like capabilities in Chinese.",
        "Resource Release: The commitment to releasing code, data, and a powerful Chinese-tuned model (Alpaca-CoT) promotes reproducibility, transparency, and further research in the field."
      ],
      "limitations": [
        "Heavy Reliance on Proprietary Models for Evaluation: The use of ChatGPT to score model outputs on the Belle-eval benchmark introduces potential bias, circularity (using a model to evaluate its competitors), and lack of transparency in the evaluation metric.",
        "Limited Scope of Base Model Evaluation: The study primarily tests 7B-parameter models (LLaMA-7B, Bloom-7B1). Findings may not generalize to much larger or smaller model scales, which is a critical dimension in LLM research.",
        "Narrow Definition of 'Chinese Capability': Evaluation focuses on instruction-following (Belle-eval) and exam knowledge (MMCU). It lacks deeper linguistic evaluation (e.g., grammar, idiom usage, cultural nuance) and real-world task performance, which are important for assessing true Chinese language mastery.",
        "Incomplete Discussion of Data Contamination: The paper does not thoroughly address the risk of test data contamination in the large, web-scraped instruction datasets (like Alpaca or Belle), which could artificially inflate benchmark performance and threaten the validity of the findings."
      ],
      "relevance_score": 0.6666666666666667,
      "citations": 23,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "pdf_available": true,
      "source": "unknown"
    },
    {
      "paper_id": "10.52202/079017-1737",
      "title": "Instruction Tuning Large Language Models to Understand Electronic Health Records",
      "authors": [
        "Zhenbang Wu",
        "Anant Dadu",
        "Mike A. Nalls",
        "F. Faghri",
        "Jimeng Sun"
      ],
      "year": 2024,
      "abstract": "Large language models (LLMs) have shown impressive capabilities in solving a wide range of tasks based on human instructions. However, developing a conversational AI assistant for electronic health record (EHR) data remains challenging due to (1) the lack of large-scale instruction-following datasets and (2) the limitations of existing model architectures in handling complex and heterogeneous EHR data. In this paper, we introduce MIMIC-Instr , a dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database. This dataset covers various topics and is suitable for instruction-tuning general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr , a general framework that enables LLMs to process and interpret EHRs with complex data structures. Llemr demonstrates competitive performance in answering a wide range of patient-related questions based on EHR data. Furthermore, our evaluations on clinical predictive modeling ben",
      "key_findings": [
        "Finding 1: The authors created MIMIC-Instr, a large-scale dataset of over 400,000 open-ended instruction-following examples derived from the MIMIC-IV EHR database, addressing a key resource gap for training conversational AI on clinical data.",
        "Finding 2: The proposed Llemr framework, when fine-tuned, demonstrates competitive performance in answering a diverse range of patient-related questions directly from complex, heterogeneous EHR data.",
        "Finding 3: The fine-tuned Llemr model achieves performance on clinical predictive modeling benchmarks that is comparable to state-of-the-art baselines that rely on manually curated features, suggesting it can automate feature engineering.",
        "Finding 4: The work provides a general framework that enables large language models to process and interpret EHRs with complex data structures, which is a core architectural challenge highlighted in the abstract."
      ],
      "methodology": "The study design involves a two-pronged approach: dataset creation and model framework development. First, the authors constructed the MIMIC-Instr dataset by programmatically generating instruction-following examples (e.g., questions and answers) from the structured tables and clinical notes within the MIMIC-IV database. This process aimed to cover a wide variety of clinical topics and query types to support diverse use cases. The dataset serves as the foundational training resource for the subsequent instruction-tuning phase.\n\nData collection was based entirely on the publicly available MIMIC-IV EHR database, a large, de-identified dataset from intensive care units. The authors derived over 400,000 examples from this source, implying the use of automated or semi-automated methods to convert structured data (lab results, diagnoses, medications) and unstructured notes into a conversational (instruction-response) format suitable for training LLMs.\n\nAnalysis techniques centered on instruction-tuning a general-purpose LLM using the created MIMIC-Instr dataset. The Llemr framework presumably involves architectural or input-processing adaptations to handle the heterogeneity of EHR data (e.g., combining time-series, categorical, and textual data). Evaluation was performed on two fronts: (1) the model's ability to answer open-ended patient-related questions, assessed for competitiveness, and (2) performance on established clinical predictive modeling benchmarks, where it was compared against SOTA models using curated features. Key parameters, such as the base LLM used, specific fine-tuning techniques, and evaluation metrics, are not detailed in the provided text but are implied to be part of the full methodology.",
      "strengths": [
        "Addresses a clear and significant gap in the field by creating and releasing MIMIC-Instr, a large-scale, open-ended instruction dataset for EHRs, which facilitates future research and reproducibility.",
        "Proposes a general framework (Llemr) that tackles the core challenge of enabling LLMs to handle complex, heterogeneous EHR data structures, moving beyond simple text-based QA.",
        "Demonstrates a dual utility: not only for conversational question-answering but also for achieving competitive performance on predictive modeling tasks, suggesting the framework's versatility.",
        "Provides public access to the dataset and code, enhancing transparency and enabling community validation and extension of the work."
      ],
      "limitations": [
        "The provided text lacks critical methodological details, such as the specific base LLM, the exact process for generating the 400K examples (potential for automation bias or lack of clinical validation), and the evaluation metrics for 'competitive performance.'",
        "Reliance on MIMIC-IV data introduces inherent biases: the data is from ICU patients in a limited number of US hospitals, which may not generalize to other care settings, populations, or healthcare systems.",
        "The claim of 'comparable performance' to SOTA predictive models is vague; without metrics (e.g., AUC, F1 scores) and statistical significance testing, the true practical equivalence is difficult to assess.",
        "Potential limitations in clinical safety and robustness are not addressed in the abstract; an LLM for EHRs requires rigorous evaluation for hallucination, consistency, and reasoning errors, which are not mentioned."
      ],
      "relevance_score": 0.625,
      "citations": 10,
      "venue": "Neural Information Processing Systems",
      "pdf_available": false,
      "source": "unknown"
    }
  ],
  "knowledge_graph": {
    "entities": {
      "3c1fcaac6999": {
        "entity_id": "3c1fcaac6999",
        "name": "Instruction Tuning",
        "entity_type": "method",
        "aliases": [
          "instruction-following tuning",
          "Instruction-tuning",
          "IT",
          "Instruction tuning",
          "Instruction-based fine-tuning",
          "Instruction-following tuning"
        ],
        "description": "A technique to enhance large language models by further training them on datasets of (instruction, output) pairs in a supervised fashion.",
        "paper_ids": [
          "2308.06966",
          "10.52202/079017-1737",
          "2308.10792",
          "2310.20329",
          "2310.07328"
        ],
        "frequency": 5
      },
      "45e75bb3c854": {
        "entity_id": "45e75bb3c854",
        "name": "Large Language Models",
        "entity_type": "method",
        "aliases": [
          "LLMs"
        ],
        "description": "Large-scale neural network models trained on massive text corpora, typically using transformer architectures.",
        "paper_ids": [
          "2310.07328",
          "2310.20329",
          "10.52202/079017-1737",
          "2308.10792"
        ],
        "frequency": 4
      },
      "e9cdcb78dc9d": {
        "entity_id": "e9cdcb78dc9d",
        "name": "Supervised Learning",
        "entity_type": "method",
        "aliases": [
          "Supervised training"
        ],
        "description": "A machine learning approach where models are trained on labeled input-output pairs to learn mapping functions.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "ab6b02f3a137": {
        "entity_id": "ab6b02f3a137",
        "name": "Dataset Construction",
        "entity_type": "method",
        "aliases": [
          "IT dataset construction"
        ],
        "description": "The process of creating instruction-output pairs for training instruction-tuned models.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "7a6ae7f6faea": {
        "entity_id": "7a6ae7f6faea",
        "name": "EcomGPT",
        "entity_type": "method",
        "aliases": [
          "E-commerce GPT"
        ],
        "description": "An instruction-tuned large language model specifically designed for E-commerce scenarios, trained on the EcomInstruct dataset.",
        "paper_ids": [
          "2308.06966"
        ],
        "frequency": 1
      },
      "419e94b1e640": {
        "entity_id": "419e94b1e640",
        "name": "BLOOMZ",
        "entity_type": "method",
        "aliases": [
          "BLOOMZ model"
        ],
        "description": "A large language model used as the backbone for training EcomGPT, capable of following instructions in multiple languages.",
        "paper_ids": [
          "2308.06966"
        ],
        "frequency": 1
      },
      "c167ca9ace71": {
        "entity_id": "c167ca9ace71",
        "name": "ChatGPT",
        "entity_type": "method",
        "aliases": [
          "Chat Generative Pre-trained Transformer"
        ],
        "description": "An instruction-following large language model that has exhibited exceptional performance in general natural language processing tasks.",
        "paper_ids": [
          "2308.06966",
          "2310.07328"
        ],
        "frequency": 2
      },
      "8f846e011b4a": {
        "entity_id": "8f846e011b4a",
        "name": "InstructCoder",
        "entity_type": "method",
        "aliases": [
          "InstructCoder dataset"
        ],
        "description": "An instruction-tuning dataset designed to adapt large language models for general-purpose code editing tasks.",
        "paper_ids": [
          "2310.20329"
        ],
        "frequency": 1
      },
      "371f4cccc3f0": {
        "entity_id": "371f4cccc3f0",
        "name": "Parameter-efficient methods",
        "entity_type": "method",
        "aliases": [
          "Parameter-efficient fine-tuning"
        ],
        "description": "Techniques that update only a small subset of model parameters during fine-tuning to reduce computational costs.",
        "paper_ids": [
          "2310.07328"
        ],
        "frequency": 1
      },
      "96f458b352ce": {
        "entity_id": "96f458b352ce",
        "name": "Llemr",
        "entity_type": "method",
        "aliases": [],
        "description": "A general framework that enables large language models to process and interpret electronic health records with complex data structures.",
        "paper_ids": [
          "10.52202/079017-1737"
        ],
        "frequency": 1
      },
      "271abcc6498a": {
        "entity_id": "271abcc6498a",
        "name": "Next-word Prediction",
        "entity_type": "concept",
        "aliases": [
          "Language modeling objective"
        ],
        "description": "The standard training objective for language models where the model predicts the next token in a sequence.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "6d6bf1b08a68": {
        "entity_id": "6d6bf1b08a68",
        "name": "Controllability",
        "entity_type": "concept",
        "aliases": [
          "Model controllability"
        ],
        "description": "The ability to make language models adhere to specific instructions or constraints provided by users.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "0ae673035d32": {
        "entity_id": "0ae673035d32",
        "name": "General Methodology",
        "entity_type": "concept",
        "aliases": [
          "IT methodology"
        ],
        "description": "The systematic approach and framework for implementing instruction tuning across different applications.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "6f37cc00378a": {
        "entity_id": "6f37cc00378a",
        "name": "Multi-modality",
        "entity_type": "concept",
        "aliases": [
          "Different modalities"
        ],
        "description": "The extension of instruction tuning techniques to handle multiple types of data beyond text, such as images or audio.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "1e9f8cf3410c": {
        "entity_id": "1e9f8cf3410c",
        "name": "Instruction Output Generation",
        "entity_type": "concept",
        "aliases": [
          "Output generation"
        ],
        "description": "The process of creating appropriate responses or outputs for given instructions in dataset creation.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "6701c14bbe8f": {
        "entity_id": "6701c14bbe8f",
        "name": "Chain-of-Task",
        "entity_type": "concept",
        "aliases": [
          "CoT tasks",
          "Chain-of-Task tasks"
        ],
        "description": "A method of constructing tasks by breaking down final tasks into intermediate atomic tasks, used to scale up data size and task diversity.",
        "paper_ids": [
          "2308.06966"
        ],
        "frequency": 1
      },
      "b6035a2edeb1": {
        "entity_id": "b6035a2edeb1",
        "name": "Atomic tasks",
        "entity_type": "concept",
        "aliases": [
          "Intermediate tasks"
        ],
        "description": "Basic tasks implicitly involved in solving a final task, used as building blocks in the Chain-of-Task approach for instruction dataset construction.",
        "paper_ids": [
          "2308.06966"
        ],
        "frequency": 1
      },
      "e8cc5cf74851": {
        "entity_id": "e8cc5cf74851",
        "name": "Code Editing",
        "entity_type": "concept",
        "aliases": [
          "Code modification",
          "Program editing"
        ],
        "description": "The process of modifying existing source code to achieve various objectives such as bug fixing, optimization, or refactoring.",
        "paper_ids": [
          "2310.20329"
        ],
        "frequency": 1
      },
      "0dcbfd08ec96": {
        "entity_id": "0dcbfd08ec96",
        "name": "Comment Insertion",
        "entity_type": "concept",
        "aliases": [
          "Documentation generation",
          "Code commenting"
        ],
        "description": "A code editing task that involves adding explanatory comments to existing source code.",
        "paper_ids": [
          "2310.20329"
        ],
        "frequency": 1
      },
      "c3ff931202eb": {
        "entity_id": "c3ff931202eb",
        "name": "Code Optimization",
        "entity_type": "concept",
        "aliases": [
          "Program optimization",
          "Performance improvement"
        ],
        "description": "The process of modifying code to improve its efficiency, speed, or resource usage while maintaining functionality.",
        "paper_ids": [
          "2310.20329"
        ],
        "frequency": 1
      },
      "76996e9553b4": {
        "entity_id": "76996e9553b4",
        "name": "Code Refactoring",
        "entity_type": "concept",
        "aliases": [
          "Code restructuring",
          "Refactoring"
        ],
        "description": "The process of restructuring existing code without changing its external behavior to improve readability, maintainability, or design.",
        "paper_ids": [
          "2310.20329"
        ],
        "frequency": 1
      },
      "3feb51f41f05": {
        "entity_id": "3feb51f41f05",
        "name": "Chain-of-thought",
        "entity_type": "concept",
        "aliases": [
          "Chain-of-thought data"
        ],
        "description": "A prompting technique that encourages models to generate intermediate reasoning steps before producing a final answer.",
        "paper_ids": [
          "2310.07328"
        ],
        "frequency": 1
      },
      "6b172515a96b": {
        "entity_id": "6b172515a96b",
        "name": "Human-value alignment",
        "entity_type": "concept",
        "aliases": [
          "Alignment"
        ],
        "description": "The process of ensuring AI systems act in accordance with human values, preferences, and ethical guidelines.",
        "paper_ids": [
          "2310.07328"
        ],
        "frequency": 1
      },
      "1f62f1e5b337": {
        "entity_id": "1f62f1e5b337",
        "name": "Artificial General Intelligence",
        "entity_type": "concept",
        "aliases": [
          "AGI"
        ],
        "description": "A theoretical form of AI that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a human level.",
        "paper_ids": [
          "2310.07328"
        ],
        "frequency": 1
      },
      "e9a91f136ec6": {
        "entity_id": "e9a91f136ec6",
        "name": "Electronic Health Records",
        "entity_type": "concept",
        "aliases": [
          "EHR",
          "EHR data"
        ],
        "description": "Digital versions of patients' paper charts containing comprehensive medical history, diagnoses, medications, and treatment plans.",
        "paper_ids": [
          "10.52202/079017-1737"
        ],
        "frequency": 1
      },
      "3f53d221e49a": {
        "entity_id": "3f53d221e49a",
        "name": "Conversational AI Assistant",
        "entity_type": "concept",
        "aliases": [
          "AI assistant"
        ],
        "description": "Artificial intelligence systems designed to engage in natural language conversations, particularly for clinical applications using EHR data.",
        "paper_ids": [
          "10.52202/079017-1737"
        ],
        "frequency": 1
      },
      "c68e09df7b89": {
        "entity_id": "c68e09df7b89",
        "name": "Clinical Predictive Modeling",
        "entity_type": "concept",
        "aliases": [
          "clinical prediction"
        ],
        "description": "The use of statistical and machine learning techniques to predict clinical outcomes based on patient data from electronic health records.",
        "paper_ids": [
          "10.52202/079017-1737"
        ],
        "frequency": 1
      },
      "ed40c2665f76": {
        "entity_id": "ed40c2665f76",
        "name": "Instruction",
        "entity_type": "dataset",
        "aliases": [
          "IT dataset",
          "Instruction Dataset"
        ],
        "description": "A collection of (instruction, output) pairs used for training models through instruction tuning.",
        "paper_ids": [
          "2308.10792"
        ],
        "frequency": 1
      },
      "330674d37220": {
        "entity_id": "330674d37220",
        "name": "EcomInstruct",
        "entity_type": "dataset",
        "aliases": [
          "E-commerce Instruction dataset"
        ],
        "description": "An instruction dataset for E-commerce containing 2.5 million instruction data points, constructed using E-commerce basic data types like product information and user reviews.",
        "paper_ids": [
          "2308.06966"
        ],
        "frequency": 1
      },
      "ea3227aab628": {
        "entity_id": "ea3227aab628",
        "name": "EditEval",
        "entity_type": "dataset",
        "aliases": [
          "EditEval benchmark"
        ],
        "description": "A novel human-written execution-based benchmark for evaluating code editing capabilities of models.",
        "paper_ids": [
          "2310.20329"
        ],
        "frequency": 1
      },
      "99ad91ac7aad": {
        "entity_id": "99ad91ac7aad",
        "name": "MIMIC-Instr",
        "entity_type": "dataset",
        "aliases": [],
        "description": "A dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database for clinical use cases.",
        "paper_ids": [
          "10.52202/079017-1737"
        ],
        "frequency": 1
      },
      "a7d19b242a6e": {
        "entity_id": "a7d19b242a6e",
        "name": "MIMIC-IV",
        "entity_type": "dataset",
        "aliases": [
          "MIMIC-IV EHR database"
        ],
        "description": "A comprehensive electronic health record database containing de-identified patient data from intensive care units.",
        "paper_ids": [
          "10.52202/079017-1737"
        ],
        "frequency": 1
      }
    },
    "mentions": [
      {
        "entity_id": "3c1fcaac6999",
        "paper_id": "2308.10792",
        "context": "A technique to enhance large language models by further training them on datasets of (instruction, output) pairs in a supervised fashion.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2308.10792",
        "context": "Large-scale neural network models trained on massive text corpora, typically using transformer architectures.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "e9cdcb78dc9d",
        "paper_id": "2308.10792",
        "context": "A machine learning approach where models are trained on labeled input-output pairs to learn mapping functions.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "271abcc6498a",
        "paper_id": "2308.10792",
        "context": "The standard training objective for language models where the model predicts the next token in a sequence.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "ed40c2665f76",
        "paper_id": "2308.10792",
        "context": "A collection of (instruction, output) pairs used for training models through instruction tuning.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "6d6bf1b08a68",
        "paper_id": "2308.10792",
        "context": "The ability to make language models adhere to specific instructions or constraints provided by users.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "0ae673035d32",
        "paper_id": "2308.10792",
        "context": "The systematic approach and framework for implementing instruction tuning across different applications.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "6f37cc00378a",
        "paper_id": "2308.10792",
        "context": "The extension of instruction tuning techniques to handle multiple types of data beyond text, such as images or audio.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "ab6b02f3a137",
        "paper_id": "2308.10792",
        "context": "The process of creating instruction-output pairs for training instruction-tuned models.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "1e9f8cf3410c",
        "paper_id": "2308.10792",
        "context": "The process of creating appropriate responses or outputs for given instructions in dataset creation.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "7a6ae7f6faea",
        "paper_id": "2308.06966",
        "context": "An instruction-tuned large language model specifically designed for E-commerce scenarios, trained on the EcomInstruct dataset.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "330674d37220",
        "paper_id": "2308.06966",
        "context": "An instruction dataset for E-commerce containing 2.5 million instruction data points, constructed using E-commerce basic data types like product information and user reviews.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "419e94b1e640",
        "paper_id": "2308.06966",
        "context": "A large language model used as the backbone for training EcomGPT, capable of following instructions in multiple languages.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "6701c14bbe8f",
        "paper_id": "2308.06966",
        "context": "A method of constructing tasks by breaking down final tasks into intermediate atomic tasks, used to scale up data size and task diversity.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "3c1fcaac6999",
        "paper_id": "2308.06966",
        "context": "A training approach for large language models where models are fine-tuned on instruction-response pairs to improve their ability to follow human instructions.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c167ca9ace71",
        "paper_id": "2308.06966",
        "context": "An instruction-following large language model that has exhibited exceptional performance in general natural language processing tasks.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "b6035a2edeb1",
        "paper_id": "2308.06966",
        "context": "Basic tasks implicitly involved in solving a final task, used as building blocks in the Chain-of-Task approach for instruction dataset construction.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2310.20329",
        "context": "Large-scale neural network models trained on massive text corpora, capable of understanding and generating human-like text.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "3c1fcaac6999",
        "paper_id": "2310.20329",
        "context": "A training approach where models are fine-tuned on instruction-output pairs to improve their ability to follow human instructions.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "8f846e011b4a",
        "paper_id": "2310.20329",
        "context": "An instruction-tuning dataset designed to adapt large language models for general-purpose code editing tasks.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "ea3227aab628",
        "paper_id": "2310.20329",
        "context": "A novel human-written execution-based benchmark for evaluating code editing capabilities of models.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "e8cc5cf74851",
        "paper_id": "2310.20329",
        "context": "The process of modifying existing source code to achieve various objectives such as bug fixing, optimization, or refactoring.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "0dcbfd08ec96",
        "paper_id": "2310.20329",
        "context": "A code editing task that involves adding explanatory comments to existing source code.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c3ff931202eb",
        "paper_id": "2310.20329",
        "context": "The process of modifying code to improve its efficiency, speed, or resource usage while maintaining functionality.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "76996e9553b4",
        "paper_id": "2310.20329",
        "context": "The process of restructuring existing code without changing its external behavior to improve readability, maintainability, or design.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "2310.07328",
        "context": "Large-scale neural network models trained on vast text corpora to generate human-like text and perform various language tasks.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "3c1fcaac6999",
        "paper_id": "2310.07328",
        "context": "A fine-tuning technique where models are trained on instruction-response pairs to improve their ability to follow specific instructions.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "371f4cccc3f0",
        "paper_id": "2310.07328",
        "context": "Techniques that update only a small subset of model parameters during fine-tuning to reduce computational costs.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "3feb51f41f05",
        "paper_id": "2310.07328",
        "context": "A prompting technique that encourages models to generate intermediate reasoning steps before producing a final answer.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "6b172515a96b",
        "paper_id": "2310.07328",
        "context": "The process of ensuring AI systems act in accordance with human values, preferences, and ethical guidelines.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "1f62f1e5b337",
        "paper_id": "2310.07328",
        "context": "A theoretical form of AI that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a human level.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "c167ca9ace71",
        "paper_id": "2310.07328",
        "context": "A conversational AI model developed by OpenAI, based on large language models and instruction-tuning.",
        "section": "full_text",
        "confidence": 0.9
      },
      {
        "entity_id": "45e75bb3c854",
        "paper_id": "10.52202/079017-1737",
        "context": "Language models with billions of parameters that can perform various natural language tasks based on human instructions.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "3c1fcaac6999",
        "paper_id": "10.52202/079017-1737",
        "context": "A training approach where models are fine-tuned on datasets containing instructions and corresponding responses to improve task performance.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "99ad91ac7aad",
        "paper_id": "10.52202/079017-1737",
        "context": "A dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database for clinical use cases.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "a7d19b242a6e",
        "paper_id": "10.52202/079017-1737",
        "context": "A comprehensive electronic health record database containing de-identified patient data from intensive care units.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "96f458b352ce",
        "paper_id": "10.52202/079017-1737",
        "context": "A general framework that enables large language models to process and interpret electronic health records with complex data structures.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "e9a91f136ec6",
        "paper_id": "10.52202/079017-1737",
        "context": "Digital versions of patients' paper charts containing comprehensive medical history, diagnoses, medications, and treatment plans.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "3f53d221e49a",
        "paper_id": "10.52202/079017-1737",
        "context": "Artificial intelligence systems designed to engage in natural language conversations, particularly for clinical applications using EHR data.",
        "section": "abstract",
        "confidence": 0.9
      },
      {
        "entity_id": "c68e09df7b89",
        "paper_id": "10.52202/079017-1737",
        "context": "The use of statistical and machine learning techniques to predict clinical outcomes based on patient data from electronic health records.",
        "section": "abstract",
        "confidence": 0.9
      }
    ],
    "edges": [
      {
        "source_id": "2308.10792",
        "target_id": "3c1fcaac6999",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "271abcc6498a",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "ed40c2665f76",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "6d6bf1b08a68",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "0ae673035d32",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "6f37cc00378a",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "ab6b02f3a137",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.10792",
        "target_id": "1e9f8cf3410c",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "7a6ae7f6faea",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "330674d37220",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "419e94b1e640",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "6701c14bbe8f",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "3c1fcaac6999",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "c167ca9ace71",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2308.06966",
        "target_id": "b6035a2edeb1",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "3c1fcaac6999",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "8f846e011b4a",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "ea3227aab628",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "e8cc5cf74851",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "0dcbfd08ec96",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "c3ff931202eb",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.20329",
        "target_id": "76996e9553b4",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "3c1fcaac6999",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "371f4cccc3f0",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "3feb51f41f05",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "6b172515a96b",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "1f62f1e5b337",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "2310.07328",
        "target_id": "c167ca9ace71",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "45e75bb3c854",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "3c1fcaac6999",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "99ad91ac7aad",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "a7d19b242a6e",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "96f458b352ce",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "e9a91f136ec6",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "3f53d221e49a",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "c68e09df7b89",
        "edge_type": "mentions",
        "weight": 1.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "2308.10792",
        "edge_type": "likely_cites",
        "weight": 2.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "2310.20329",
        "edge_type": "likely_cites",
        "weight": 2.0,
        "paper_ids": []
      },
      {
        "source_id": "10.52202/079017-1737",
        "target_id": "2310.07328",
        "edge_type": "likely_cites",
        "weight": 2.0,
        "paper_ids": []
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 4.0,
        "paper_ids": [
          "10.52202/079017-1737",
          "2310.20329",
          "2310.07328",
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "b6035a2edeb1",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "7a6ae7f6faea",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 2.0,
        "paper_ids": [
          "2308.06966",
          "2310.07328"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "6701c14bbe8f",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "419e94b1e640",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "330674d37220",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "6b172515a96b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "1f62f1e5b337",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "371f4cccc3f0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "3feb51f41f05",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3c1fcaac6999",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 4.0,
        "paper_ids": [
          "10.52202/079017-1737",
          "2310.20329",
          "2310.07328",
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "6b172515a96b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "1f62f1e5b337",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "371f4cccc3f0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "3feb51f41f05",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "45e75bb3c854",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "e9cdcb78dc9d",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ab6b02f3a137",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "7a6ae7f6faea",
        "target_id": "b6035a2edeb1",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "7a6ae7f6faea",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "7a6ae7f6faea",
        "target_id": "6701c14bbe8f",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "7a6ae7f6faea",
        "target_id": "419e94b1e640",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "7a6ae7f6faea",
        "target_id": "330674d37220",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "7a6ae7f6faea",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "419e94b1e640",
        "target_id": "b6035a2edeb1",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "419e94b1e640",
        "target_id": "7a6ae7f6faea",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "419e94b1e640",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "419e94b1e640",
        "target_id": "6701c14bbe8f",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "419e94b1e640",
        "target_id": "330674d37220",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "419e94b1e640",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "b6035a2edeb1",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "7a6ae7f6faea",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "6701c14bbe8f",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "419e94b1e640",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "330674d37220",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 2.0,
        "paper_ids": [
          "2308.06966",
          "2310.07328"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "6b172515a96b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "1f62f1e5b337",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "371f4cccc3f0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "c167ca9ace71",
        "target_id": "3feb51f41f05",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "8f846e011b4a",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "371f4cccc3f0",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "371f4cccc3f0",
        "target_id": "6b172515a96b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "371f4cccc3f0",
        "target_id": "1f62f1e5b337",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "371f4cccc3f0",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "371f4cccc3f0",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "371f4cccc3f0",
        "target_id": "3feb51f41f05",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "96f458b352ce",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "271abcc6498a",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6d6bf1b08a68",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "0ae673035d32",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6f37cc00378a",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "ed40c2665f76",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "1e9f8cf3410c",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "6701c14bbe8f",
        "target_id": "b6035a2edeb1",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "6701c14bbe8f",
        "target_id": "7a6ae7f6faea",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "6701c14bbe8f",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "6701c14bbe8f",
        "target_id": "419e94b1e640",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "6701c14bbe8f",
        "target_id": "330674d37220",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "6701c14bbe8f",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "b6035a2edeb1",
        "target_id": "7a6ae7f6faea",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "b6035a2edeb1",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "b6035a2edeb1",
        "target_id": "6701c14bbe8f",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "b6035a2edeb1",
        "target_id": "419e94b1e640",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "b6035a2edeb1",
        "target_id": "330674d37220",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "b6035a2edeb1",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "e8cc5cf74851",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "0dcbfd08ec96",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "c3ff931202eb",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "ea3227aab628",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "76996e9553b4",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "3feb51f41f05",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3feb51f41f05",
        "target_id": "6b172515a96b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3feb51f41f05",
        "target_id": "1f62f1e5b337",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3feb51f41f05",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3feb51f41f05",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "3feb51f41f05",
        "target_id": "371f4cccc3f0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "6b172515a96b",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "6b172515a96b",
        "target_id": "1f62f1e5b337",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "6b172515a96b",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "6b172515a96b",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "6b172515a96b",
        "target_id": "371f4cccc3f0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "6b172515a96b",
        "target_id": "3feb51f41f05",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "1f62f1e5b337",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "1f62f1e5b337",
        "target_id": "6b172515a96b",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "1f62f1e5b337",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "1f62f1e5b337",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "1f62f1e5b337",
        "target_id": "371f4cccc3f0",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "1f62f1e5b337",
        "target_id": "3feb51f41f05",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.07328"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "e9a91f136ec6",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "3f53d221e49a",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "c68e09df7b89",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "6f37cc00378a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "6d6bf1b08a68",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "0ae673035d32",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "271abcc6498a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "1e9f8cf3410c",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "ab6b02f3a137",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "ed40c2665f76",
        "target_id": "e9cdcb78dc9d",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.10792"
        ]
      },
      {
        "source_id": "330674d37220",
        "target_id": "b6035a2edeb1",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "330674d37220",
        "target_id": "7a6ae7f6faea",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "330674d37220",
        "target_id": "c167ca9ace71",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "330674d37220",
        "target_id": "6701c14bbe8f",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "330674d37220",
        "target_id": "419e94b1e640",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "330674d37220",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2308.06966"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "0dcbfd08ec96",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "c3ff931202eb",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "76996e9553b4",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "8f846e011b4a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "ea3227aab628",
        "target_id": "e8cc5cf74851",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "2310.20329"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "a7d19b242a6e",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "99ad91ac7aad",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "3f53d221e49a",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "45e75bb3c854",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "e9a91f136ec6",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "3c1fcaac6999",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "99ad91ac7aad",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "c68e09df7b89",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      },
      {
        "source_id": "a7d19b242a6e",
        "target_id": "96f458b352ce",
        "edge_type": "co_occurs",
        "weight": 1.0,
        "paper_ids": [
          "10.52202/079017-1737"
        ]
      }
    ],
    "communities": [
      {
        "community_id": "comm_0_0",
        "level": 0,
        "entities": [
          "3f53d221e49a",
          "45e75bb3c854",
          "a7d19b242a6e",
          "6f37cc00378a",
          "6d6bf1b08a68",
          "0ae673035d32",
          "e9a91f136ec6",
          "3c1fcaac6999",
          "96f458b352ce",
          "271abcc6498a",
          "1e9f8cf3410c",
          "c68e09df7b89",
          "ab6b02f3a137",
          "99ad91ac7aad",
          "ed40c2665f76",
          "e9cdcb78dc9d"
        ],
        "papers": [
          "2308.06966",
          "10.52202/079017-1737",
          "2308.10792",
          "2310.20329",
          "2310.07328"
        ],
        "summary": "The central research theme of this community is the development and application of **instruction tuning** as a critical methodology for adapting **Large Language Models (LLMs)** to perform specific, user-aligned tasks. This theme moves beyond the foundational pre-training of models on next-word prediction, focusing instead on bridging the \"fundamental gap\" to create models that reliably follow human instructions. The research explores this core technique across diverse, high-stakes domains—including clinical medicine, e-commerce, and code generation—demonstrating its role in enhancing model **controllability** and utility for building specialized **Conversational AI Assistants**.\n\nThe most prominent approach is the systematic framework of instruction tuning, which hinges on the **Dataset Construction** of high-quality (instruction, output) pairs. As highlighted in the survey, datasets are built either by reformatting existing annotated resources (e.g., Flan) or by using advanced LLMs to generate outputs. Community research exemplifies this through domain-specific creations like **EcomInstruct** for e-commerce and **MIMIC-Instr** (derived from **MIMIC-IV**) for electronic health records. The **Llemr** framework further operationalizes this for clinical data. A key methodological trend is the use of parameter-efficient fine-tuning techniques like LoRA, which are shown to be effective and computationally advantageous for adapting base models like LLaMA.\n\nThe key contributions and findings establish instruction tuning as a powerful paradigm for specialization and generalization. First, it enables open-source models to achieve performance competitive with proprietary systems (e.g., **InstructCoder** matching ChatGPT in code editing). Second, it proves essential for unlocking capabilities in complex, structured domains like healthcare, where the **Llemr** framework shows LLMs can interpret heterogeneous EHR data. A consistent trend is the creation of large-scale, open instruction datasets to address resource gaps, which in turn facilitates strong zero-shot and cross-task generalization, as demonstrated by **EcomGPT**. Ultimately, the community's work charts a path from general-purpose LLMs to reliable, application-specific AI systems through curated instruction-following data.",
        "parent_id": null,
        "children_ids": [
          "comm_1_5",
          "comm_1_6"
        ]
      },
      {
        "community_id": "comm_0_1",
        "level": 0,
        "entities": [
          "b6035a2edeb1",
          "6b172515a96b",
          "7a6ae7f6faea",
          "1f62f1e5b337",
          "c167ca9ace71",
          "6701c14bbe8f",
          "419e94b1e640",
          "330674d37220",
          "371f4cccc3f0",
          "3feb51f41f05"
        ],
        "papers": [
          "2308.06966",
          "2310.07328"
        ],
        "summary": "This research community centers on the theme of **specialized instruction-tuning of large language models (LLMs) to enhance their performance and alignment in specific, complex domains**. The core objective is to move beyond general-purpose models like ChatGPT by creating expert LLMs through fine-tuning on high-quality, domain-specific instruction datasets. This process aims to achieve superior task performance and generalization within a target field, such as e-commerce, while also exploring efficient training methodologies and the underlying mechanisms that enable complex reasoning.\n\nThe most prominent methodological approaches involve **constructing large-scale, structured instruction datasets and applying parameter-efficient fine-tuning (PEFT) techniques**. A key innovation is the **Chain-of-Task** framework, which decomposes final objectives into intermediate **atomic tasks** to build comprehensive training data, as exemplified by the creation of the **EcomInstruct** dataset. This dataset then serves to instruction-tune base models like **BLOOMZ** to produce specialized variants such as **EcomGPT**. Concurrently, the community emphasizes training efficiency, with methods like **LoRA** (a parameter-efficient method) being validated as highly effective for adapting models like LLaMA, balancing performance gains with reduced computational cost.\n\nThe key findings and contributions demonstrate the significant value of this specialized approach. First, domain-specific instruction-tuning yields models that outperform generalist counterparts; **EcomGPT** surpassed **ChatGPT** in e-commerce tasks through superior zero-shot generalization. Second, the structure of the training data is critical, with methods like Chain-of-Task proving effective for capturing domain complexity. Third, parameter-efficient methods like LoRA are not just computationally pragmatic but can achieve state-of-the-art performance, making advanced fine-tuning more accessible. A unifying trend is the focus on creating more aligned, capable, and efficient AI systems—bridging concepts from **human-value alignment** to **Artificial General Intelligence**—by moving from generic models to experts trained on meticulously engineered, task-structured data.",
        "parent_id": null,
        "children_ids": [
          "comm_1_3"
        ]
      },
      {
        "community_id": "comm_0_2",
        "level": 0,
        "entities": [
          "0dcbfd08ec96",
          "c3ff931202eb",
          "ea3227aab628",
          "76996e9553b4",
          "8f846e011b4a",
          "e8cc5cf74851"
        ],
        "papers": [
          "2310.20329"
        ],
        "summary": "This community of research focuses on **instruction-tuning large language models (LLMs) for general-purpose, multi-task code editing**. The central theme moves beyond code generation to tackle the more complex and practical challenge of modifying *existing* source code. The research defines \"code editing\" as an umbrella objective encompassing diverse downstream tasks such as **Code Refactoring**, **Code Optimization**, and **Comment Insertion**. The core problem is that while LLMs excel at generating new code, they perform poorly on editing tasks without specialized training, highlighting a significant capability gap.\n\nThe most prominent methodological approach is the creation and use of specialized instruction-tuning datasets to adapt LLMs for this domain. The key method is **InstructCoder**, a dataset explicitly designed for instruction-tuning LLMs on a broad spectrum of code editing instructions. The primary benchmark for evaluating progress is the **EditEval** dataset, a human-written, execution-based benchmark that tests a model's ability to correctly perform edits that preserve or alter functional behavior. The research pipeline is thus defined by training on **InstructCoder** and evaluating on **EditEval**.\n\nThe key finding is that instruction-tuning on a high-quality, task-diverse dataset like **InstructCoder** dramatically closes the performance gap between open-source and advanced proprietary models. Specifically, fine-tuned open-source LLMs achieved 57.22% accuracy on **EditEval**, a performance level competitive with models like ChatGPT. This demonstrates that general-purpose code editing is a learnable capability for LLMs, not an inherent property of only the largest models. The major contribution is establishing a framework—combining a purpose-built training dataset (**InstructCoder**) and a rigorous execution-based benchmark (**EditEval**)—for advancing LLMs from mere code autocompletion to becoming versatile, automated software engineers capable of understanding and transforming code.",
        "parent_id": null,
        "children_ids": [
          "comm_1_4"
        ]
      },
      {
        "community_id": "comm_1_3",
        "level": 1,
        "entities": [
          "b6035a2edeb1",
          "6b172515a96b",
          "7a6ae7f6faea",
          "1f62f1e5b337",
          "c167ca9ace71",
          "6701c14bbe8f",
          "419e94b1e640",
          "330674d37220",
          "3c1fcaac6999",
          "371f4cccc3f0",
          "3feb51f41f05"
        ],
        "papers": [
          "2308.06966",
          "10.52202/079017-1737",
          "2308.10792",
          "2310.20329",
          "2310.07328"
        ],
        "summary": "The primary research theme of this community is the **specialization and enhancement of Large Language Models (LLMs) through instruction tuning for domain-specific and complex task applications**. While the foundational technique of instruction tuning is recognized as a crucial bridge between pre-training objectives and user-aligned behavior, the community's focus extends beyond general capability improvement. It centers on adapting LLMs to specialized domains—such as e-commerce, code editing, healthcare, and Chinese language processing—and on developing methodologies to construct high-quality, task-specific instruction datasets that enable superior performance and generalization within these niches. Underpinning this work is a broader ambition to steer LLMs toward greater utility, controllability, and alignment with human values and complex reasoning processes.\n\nThe most prominent methodological approach is **instruction tuning**, often implemented using **parameter-efficient methods** like LoRA to reduce computational cost. The community demonstrates a clear trend toward constructing large-scale, domain-specific instruction datasets (e.g., **EcomInstruct**, MIMIC-Instr) to fuel this tuning. A key innovative approach within dataset construction is the **Chain-of-Task** methodology, which decomposes final objectives into intermediate **atomic tasks** to create more learnable and logically structured training data, as exemplified by **EcomGPT**. Models such as **ChatGPT** serve both as performance benchmarks and as tools for generating synthetic instruction data, while open-source models like **BLOOMZ** and LLaMA are commonly used as backbones for specialized tuning.\n\nKey findings and contributions highlight that instruction-tuned models can match or exceed the performance of advanced generalist models like **ChatGPT** on specialized tasks (e.g., **EcomGPT** in e-commerce, InstructCoder in code editing). The research establishes that the quality and construction methodology of the instruction dataset are critical drivers of success, with methods like Chain-of-Task proving effective for complex domains. A consistent trend is the validation of parameter-efficient fine-tuning, particularly LoRA, as a highly effective and resource-conscious strategy. Collectively, this work moves the field from demonstrating instruction tuning's general promise to engineering its practical application, advancing LLMs toward more reliable, domain-expert, and value-aligned **Artificial General Intelligence**.",
        "parent_id": "comm_0_1",
        "children_ids": [
          "comm_2_8",
          "comm_2_10"
        ]
      },
      {
        "community_id": "comm_1_4",
        "level": 1,
        "entities": [
          "45e75bb3c854",
          "0dcbfd08ec96",
          "c3ff931202eb",
          "ea3227aab628",
          "76996e9553b4",
          "8f846e011b4a",
          "e8cc5cf74851"
        ],
        "papers": [
          "10.52202/079017-1737",
          "2310.20329",
          "2310.07328",
          "2308.10792"
        ],
        "summary": "This research community focuses on **instruction tuning as a specialized fine-tuning paradigm for adapting Large Language Models (LLMs) to complex, domain-specific editing tasks**, with a primary emphasis on **general-purpose code editing**. The core theme bridges the gap identified in the survey literature—where standard pre-trained LLMs excel at next-word prediction but struggle to follow nuanced human instructions—by applying instruction tuning to make models actionable for precise, outcome-oriented modifications. While the community explores applications in diverse domains like Electronic Health Records (EHRs), the most cohesive thread is the development and evaluation of LLMs for code-centric tasks such as **Code Editing**, **Code Refactoring**, **Code Optimization**, and **Comment Insertion**.\n\nThe most prominent methodological approach is the creation and use of **instruction-tuning datasets** tailored to elicit specific capabilities from base LLMs. A central contribution is the **InstructCoder** dataset and method, explicitly designed to adapt LLMs for code editing. This approach is validated using the novel **EditEval** benchmark, a human-written, execution-based standard for assessing editing performance. Across domains, parameter-efficient fine-tuning techniques, particularly **LoRA (Low-Rank Adaptation)**, are highlighted as effective and computationally scalable strategies for instruction tuning, as evidenced by their superior performance in both code and Chinese language adaptation studies.\n\nThe key findings reveal that instruction tuning is a powerful lever for unlocking specialized LLM competencies. In code editing, models fine-tuned on **InstructCoder** can achieve accuracy competitive with advanced proprietary models on **EditEval**, demonstrating that open-source models can close the performance gap when provided with high-quality, task-specific instruction data. A consistent trend is the identification of data quality and task-specific benchmarking as critical: general-purpose LLMs, including proprietary ones, perform poorly on precise editing tasks like those in **EditEval** before instruction tuning. The community’s contributions thus center on moving beyond general chat ability to creating models that can reliably execute defined procedural edits, whether in source code or structured clinical notes, by leveraging curated instruction datasets and rigorous, execution-based evaluation.",
        "parent_id": "comm_0_2",
        "children_ids": [
          "comm_2_9"
        ]
      },
      {
        "community_id": "comm_1_5",
        "level": 1,
        "entities": [
          "6f37cc00378a",
          "6d6bf1b08a68",
          "0ae673035d32",
          "271abcc6498a",
          "1e9f8cf3410c",
          "ab6b02f3a137",
          "ed40c2665f76",
          "e9cdcb78dc9d"
        ],
        "papers": [
          "2308.10792"
        ],
        "summary": "This research community is focused on **instruction tuning for large language models (LLMs)**, a specialized form of fine-tuning that aims to align pre-trained models with human objectives. The core theme is bridging the fundamental disconnect between a model's pre-training objective—**Next-word Prediction**—and the practical goal of having models reliably follow human instructions. The central problem is transforming a model's raw generative capability into **Controllability**, where it can adhere to specific user-provided constraints and tasks. This process is framed as a **General Methodology** for adapting LLMs, with a forward-looking extension into **Multi-modality**, indicating a trend toward handling diverse data types like images and audio through similar instruction-following paradigms.\n\nThe most prominent methodological approach is **Supervised Learning** applied to specially constructed datasets. The success of instruction tuning hinges almost entirely on **Dataset Construction**, which involves creating high-quality (instruction, output) pairs. As highlighted in the survey, there are two primary strategies for building these **Instruction** datasets: first, data integration from existing NLP benchmarks using templates to reframe tasks as instructions (e.g., Flan), and second, leveraging powerful LLMs (like GPT-3.5/4) to generate outputs for a diverse seed set of instructions. This methodology represents a shift from task-specific fine-tuning to a unified, multi-task training framework that teaches the model the *format* and *behavior* of following instructions.\n\nThe key finding and contribution of this field is establishing instruction tuning as a critical technique that significantly enhances both the capabilities and the controllability of LLMs. It provides a systematic pathway to unlock the latent knowledge acquired during pre-training. A major trend is the evolution of dataset creation from manual curation and template-based conversion toward scalable, LLM-driven generation, which allows for rapid expansion of instruction diversity and complexity. The synthesis of concepts like controllability and multi-modality suggests the field's trajectory is moving beyond pure text models toward creating generalist, instruction-following agents capable of processing and reasoning across multiple modalities.",
        "parent_id": "comm_0_0",
        "children_ids": [
          "comm_2_7"
        ]
      },
      {
        "community_id": "comm_1_6",
        "level": 1,
        "entities": [
          "3f53d221e49a",
          "a7d19b242a6e",
          "e9a91f136ec6",
          "99ad91ac7aad",
          "c68e09df7b89",
          "96f458b352ce"
        ],
        "papers": [
          "10.52202/079017-1737"
        ],
        "summary": "The primary research theme of this community is the adaptation of large language models (LLMs) for specialized, high-stakes domains, specifically focusing on **clinical decision support through conversational AI**. This involves fine-tuning general-purpose LLMs to understand, interpret, and reason over complex, heterogeneous **Electronic Health Records (EHRs)**. The goal is to move beyond traditional **Clinical Predictive Modeling**—which often relies on structured feature extraction—towards creating interactive **Conversational AI Assistants** that can answer diverse, open-ended questions about patient data, thereby making clinical information more accessible and actionable for healthcare providers.\n\nThe most prominent methodological approach is the **Llemr framework**, which provides a structured pipeline for processing raw EHR data and instruction-tuning LLMs. A cornerstone of this work is the creation and utilization of specialized datasets. The community leverages the foundational **MIMIC-IV** EHR database as its source of real-world clinical data. A key contribution is the transformation of this static data into **MIMIC-Instr**, a large-scale dataset of over 400,000 instruction-following examples. This dataset bridges the gap between raw patient records and the conversational format required for effective LLM fine-tuning, enabling models to learn tasks like information retrieval, summarization, and temporal reasoning directly from clinical notes.\n\nThe key findings and contributions demonstrate the viability of this approach. The instruction-tuned models within the Llemr framework achieve competitive performance in answering a diverse range of patient-centric queries directly from EHRs, without relying on extensive manual feature engineering. This represents a significant trend toward **end-to-end, data-driven clinical NLP**, where models are trained to follow natural language instructions for complex clinical tasks. The synthesis of a large instruction dataset (MIMIC-Instr) with a tailored fine-tuning framework (Llemr) establishes a blueprint for applying conversational AI to other specialized domains where data is complex and unstructured, positioning LLMs as versatile tools for expert-level information synthesis and assistance.",
        "parent_id": "comm_0_0",
        "children_ids": [
          "comm_2_11"
        ]
      },
      {
        "community_id": "comm_2_7",
        "level": 2,
        "entities": [
          "6f37cc00378a",
          "6d6bf1b08a68",
          "0ae673035d32",
          "271abcc6498a",
          "1e9f8cf3410c",
          "ab6b02f3a137",
          "ed40c2665f76",
          "e9cdcb78dc9d"
        ],
        "papers": [
          "2308.10792"
        ],
        "summary": "This research community is focused on **instruction tuning as a core methodology for aligning and controlling large language models (LLMs)**. The central theme is bridging the gap between the foundational pre-training objective of next-word prediction and the practical user objective of having models reliably follow human instructions. The goal is to enhance both the **capabilities** and **controllability** of LLMs, transforming them from general text generators into task-specific, user-aligned assistants. This theme is expanding beyond pure text, as indicated by the interest in **multi-modality**, suggesting a trajectory toward aligning models that can process and generate outputs across diverse data types (e.g., images, audio) based on instructions.\n\nThe most prominent methodological approach is **supervised learning** applied to specially constructed **instruction datasets**. The process of **Dataset Construction** is itself a critical research focus, with two primary strategies identified. The first involves data integration, where existing NLP datasets are reformatted into (instruction, output) pairs using templates. The second, more recent trend leverages advanced LLMs (like GPT-3.5/4) to generate high-quality outputs for a diverse set of instructions, either from scratch or by expanding a small seed set. This shift toward using LLMs to create their own training data represents a significant trend in scaling and diversifying instruction datasets.\n\nThe key contribution of this field, as synthesized from the survey, is establishing instruction tuning as a pivotal technique for model alignment. A major finding is that this process effectively unlocks a model's ability for **Instruction Output Generation**, making it responsive to explicit user constraints. The trends point toward a **General Methodology** that is becoming more systematic and scalable, moving from manual curation to automated, LLM-driven dataset creation. The future direction, hinted at by the inclusion of multi-modality, involves extending this proven alignment framework from purely textual models to unified, multi-modal systems capable of following complex, cross-domain instructions.",
        "parent_id": "comm_1_5",
        "children_ids": []
      },
      {
        "community_id": "comm_2_8",
        "level": 2,
        "entities": [
          "b6035a2edeb1",
          "7a6ae7f6faea",
          "c167ca9ace71",
          "6701c14bbe8f",
          "419e94b1e640",
          "330674d37220"
        ],
        "papers": [
          "2308.06966",
          "2310.07328"
        ],
        "summary": "The primary research theme of this community is the **domain-specific instruction-tuning of large language models (LLMs)**, with a particular focus on developing specialized models that outperform general-purpose counterparts like ChatGPT in targeted applications. This involves creating large-scale, high-quality instruction datasets tailored to a specific domain (e.g., e-commerce) and employing novel task construction methodologies to enhance model training and generalization. The work centers on the hypothesis that fine-tuning a capable base LLM on a meticulously crafted domain corpus yields a superior specialist model.\n\nThe most prominent methodological approach is the **Chain-of-Task (CoT) framework**, which is used to systematically construct instruction datasets. This method decomposes complex, final tasks into sequences of simpler, foundational **atomic tasks**. This framework was instrumental in building the **EcomInstruct** dataset, which contains 2.5 million instructions across 134 e-commerce tasks. For model development, the community leverages instruction-tuning on top of strong multilingual base models like **BLOOMZ**, as seen in the creation of **EcomGPT**. Parameter-efficient fine-tuning techniques, specifically **LoRA (Low-Rank Adaptation)**, are also highlighted as a key trend for effective and computationally efficient adaptation.\n\nThe key contributions and findings demonstrate the success of this domain-specialization paradigm. First, **EcomGPT**, instruction-tuned from BLOOMZ on the CoT-constructed EcomInstruct, achieved superior zero-shot generalization on e-commerce benchmarks compared to the generalist **ChatGPT**, validating the value of domain-specific data and task decomposition. Second, empirical studies reinforce that the choice of base model and fine-tuning method is critical; for instance, LLaMA-7B fine-tuned with LoRA excelled in Chinese instruction-following tasks. The overarching trend is a move beyond generic instruction-tuning toward a more engineered pipeline: using frameworks like Chain-of-Task for scalable, high-quality dataset creation, and then applying efficient fine-tuning to produce domain-expert LLMs that address the limitations of one-size-fits-all models.",
        "parent_id": "comm_1_3",
        "children_ids": []
      },
      {
        "community_id": "comm_2_9",
        "level": 2,
        "entities": [
          "0dcbfd08ec96",
          "c3ff931202eb",
          "ea3227aab628",
          "76996e9553b4",
          "8f846e011b4a",
          "e8cc5cf74851"
        ],
        "papers": [
          "2310.20329"
        ],
        "summary": "This community of research is focused on **instruction-tuning large language models (LLMs) for general-purpose, multi-task code editing**. The core theme moves beyond code generation to address the practical and complex challenge of modifying *existing* source code. The research defines code editing as an umbrella capability encompassing distinct but related sub-tasks such as **Code Refactoring**, **Code Optimization**, and **Comment Insertion**, where the goal is to transform a given code snippet according to a natural language instruction without altering its core functionality.\n\nThe most prominent methodological approach is the creation and use of specialized instruction-tuning datasets to adapt base LLMs for this domain. **InstructCoder** is a central method, representing a dataset explicitly designed to teach models the nuanced transformations required for diverse editing tasks. The standard for evaluating progress in this theme is the **EditEval** benchmark, a human-written, execution-based dataset that tests a model's ability to perform edits correctly and verifies the changes via code execution. The relationship is clear: models are trained on datasets like InstructCoder and their general editing proficiency is rigorously measured against EditEval.\n\nThe key finding is that while powerful base LLMs perform poorly on holistic code editing, targeted instruction tuning can dramatically close this capability gap. As demonstrated by the **InstructCoder** paper, open-source models fine-tuned on its dataset achieved 57.22% accuracy on **EditEval**, rivaling the performance of advanced proprietary models like ChatGPT. This highlights a major contribution: making high-fidelity code editing—a task previously dominated by proprietary systems—accessible via open, reproducible methods. The trend is toward framing code editing as a unified, instruction-driven task, with benchmarks like EditEval exposing the limitations of current models and driving the development of more robust, task-agnostic editing systems through supervised fine-tuning.",
        "parent_id": "comm_1_4",
        "children_ids": []
      },
      {
        "community_id": "comm_2_10",
        "level": 2,
        "entities": [
          "6b172515a96b",
          "1f62f1e5b337",
          "371f4cccc3f0",
          "3feb51f41f05"
        ],
        "papers": [
          "2310.07328"
        ],
        "summary": "This community of research focuses on **optimizing and aligning large language models (LLMs) through efficient fine-tuning**, with a particular emphasis on adapting powerful, often English-centric, base models for specialized tasks and languages. The core theme is the pursuit of making advanced LLMs more practical, controllable, and effective. This involves a dual objective: first, to reduce the computational cost and resource barrier of customization using **parameter-efficient methods**, and second, to steer model behavior toward desired outcomes, such as following instructions in a specific language or adhering to complex reasoning patterns and human values. The mention of **Artificial General Intelligence (AGI)** situates this work within a broader, long-term ambition of developing capable and trustworthy AI systems, where efficient fine-tuning is a critical step toward scalable alignment and specialization.\n\nThe most prominent methodological approach is the use of **parameter-efficient fine-tuning (PEFT) techniques**, specifically highlighted by the success of **LoRA (Low-Rank Adaptation)**. The empirical study demonstrates that LoRA is not just a cost-saving tool but can also yield superior performance. When applied to the **LLaMA-7B** base model and fine-tuned on the **Alpaca dataset** for Chinese instruction-following, LoRA achieved a higher evaluation score (7.28 on Belle-eval) than other PEFT methods like **Adapter** and **Prefix-tuning**. This establishes a key trend: identifying which base models (e.g., LLaMA) are most amenable to efficient adaptation and which PEFT methods deliver the best performance-efficiency trade-off is a central research question.\n\nThe key findings and contributions point toward several important trends. First, there is a demonstrated **transferability of foundation models**: despite LLaMA's English-centric pre-training, it formed a superior base for Chinese instruction-tuning compared to other models like Bloom-7B1, suggesting that the underlying capabilities of certain base models are highly generalizable. Second, the community is actively connecting efficient fine-tuning methods with higher-order AI goals. The integration of concepts like **Chain-of-thought** prompting and **Human-value alignment** indicates that researchers are using these efficient techniques not just for task performance, but as a pathway to instill complex reasoning and ethical behavior into models. The overarching trend is a move beyond mere task adaptation toward a more holistic engineering of LLM behavior—making them capable, efficient, and aligned—using targeted, low-cost updates to pre-trained foundations.",
        "parent_id": "comm_1_3",
        "children_ids": []
      },
      {
        "community_id": "comm_2_11",
        "level": 2,
        "entities": [
          "3f53d221e49a",
          "a7d19b242a6e",
          "e9a91f136ec6",
          "99ad91ac7aad",
          "c68e09df7b89",
          "96f458b352ce"
        ],
        "papers": [
          "10.52202/079017-1737"
        ],
        "summary": "The primary research theme of this community is the adaptation of large language models (LLMs) for advanced clinical reasoning and conversational interaction with electronic health records (EHRs). This work sits at the intersection of **Conversational AI Assistant** development and **Clinical Predictive Modeling**, with a core objective of transforming raw, complex EHR data into a format that enables natural language question-answering about patient health. The central challenge addressed is the gap between general-purpose LLMs and the specialized, heterogeneous, and privacy-sensitive nature of clinical data, aiming to create AI systems that can directly interpret and reason over structured EHRs to support clinical tasks.\n\nThe most prominent methodological approach is the **Llemr** framework, a specialized pipeline for processing and interpreting EHRs with LLMs. A critical and enabling contribution of this work is the creation of two key, linked datasets: the foundational **MIMIC-IV** EHR database and the novel **MIMIC-Instr** dataset. MIMIC-Instr, comprising over 400,000 instruction-following examples derived from MIMIC-IV, is a pivotal resource that allows for the instruction-tuning of LLMs on realistic clinical queries. This method of fine-tuning on a large-scale, open-ended instructional dataset is the core technique for bridging the domain gap and teaching models to perform diverse clinical language tasks.\n\nThe key findings and contributions demonstrate the viability of this approach. The instruction-tuned **Llemr** framework achieves competitive performance in answering a broad spectrum of patient-related questions directly from raw EHR data, moving beyond simple classification to more generative and conversational interactions. The major trends indicated are: 1) a shift from using LLMs on general text to specializing them for structured, multi-modal clinical data, 2) the central importance of creating high-quality, instruction-formatted datasets (like **MIMIC-Instr**) to unlock LLM capabilities in specialized domains, and 3) the framing of clinical AI not just as a predictive tool, but as an interactive assistant capable of navigating the complexity of a patient's full digital record.",
        "parent_id": "comm_1_6",
        "children_ids": []
      }
    ],
    "global_summary": "### Global Summary of the Research Landscape: Large Language Model Fine-Tuning\n\nThe research landscape for large language model (LLM) fine-tuning is dominated by the paradigm of **instruction tuning**, which has emerged as the primary methodology for adapting general-purpose foundation models into specialized, task-aligned systems. This field has evolved from foundational pre-training on next-word prediction to a focus on bridging the \"alignment gap,\" ensuring models reliably follow human instructions for practical deployment. The central, unifying theme across communities is the creation of **domain-specific expert models**—whether for e-commerce, clinical medicine, or code editing—by fine-tuning on curated datasets of (instruction, output) pairs. These efforts are driven by the goal of matching or surpassing the performance of proprietary systems like ChatGPT with more accessible, open-source models, thereby democratizing advanced AI capabilities for specialized applications.\n\nThe most influential and widely adopted methodological approach is a structured pipeline centered on **dataset construction** followed by **parameter-efficient fine-tuning (PEFT)**. High-quality, domain-specific instruction datasets—such as EcomInstruct for e-commerce, MIMIC-Instr for healthcare, and InstructCoder for code editing—are systematically created, often by reformatting existing resources or using LLMs themselves for data generation. Frameworks like Chain-of-Task further refine this by decomposing complex objectives into atomic tasks to build comprehensive training data. For the fine-tuning process itself, techniques like **LoRA (Low-Rank Adaptation)** have become the standard, as they enable effective model adaptation with significantly reduced computational cost, making advanced specialization feasible for a broader research community. Evaluation is primarily benchmark-driven, relying on domain-specific, often execution-based datasets like EditEval for code editing or tailored task suites in e-commerce and healthcare to measure zero-shot generalization and task-specific accuracy.\n\nThe field is currently in a state of rapid maturation and application-driven expansion. The evolution has moved from demonstrating the basic efficacy of instruction tuning to refining its methodologies—optimizing data construction, improving training efficiency, and tackling increasingly complex, structured domains like electronic health records. The current consensus establishes that specialized instruction tuning is essential for unlocking high-stakes, professional-grade capabilities in LLMs. Emerging trends point toward even greater **task decomposition and data engineering** (e.g., Chain-of-Task), the exploration of **cross-domain generalization**, and the integration of **reinforcement learning from human feedback (RLHF)** to further enhance alignment. Key open questions remain: How can we best quantify and ensure the **safety and reliability** of these specialized models in critical domains? What are the limits of generalization from single-domain tuning? And how can the community develop more unified, multi-domain benchmarks to evaluate the evolving versatility of instruction-tuned models? The landscape is thus charting a clear path from general-purpose LLMs to a ecosystem of reliable, application-specific AI assistants, with ongoing research focused on improving the efficiency, scope, and robustness of this transformative fine-tuning paradigm.",
    "stats": {
      "node_count": 37,
      "edge_count": 321,
      "paper_count": 5,
      "entity_count": 32,
      "entity_types": {
        "method": 10,
        "concept": 17,
        "dataset": 5
      },
      "edge_types": {
        "mentions": 40,
        "likely_cites": 3,
        "co_occurs": 278
      },
      "top_entities": [
        [
          "Instruction Tuning",
          "method",
          2.0
        ],
        [
          "Large Language Models",
          "method",
          1.6774193548387095
        ],
        [
          "ChatGPT",
          "method",
          0.7096774193548387
        ],
        [
          "Supervised Learning",
          "method",
          0.5806451612903225
        ],
        [
          "Dataset Construction",
          "method",
          0.5806451612903225
        ],
        [
          "Next-word Prediction",
          "concept",
          0.5806451612903225
        ],
        [
          "Controllability",
          "concept",
          0.5806451612903225
        ],
        [
          "General Methodology",
          "concept",
          0.5806451612903225
        ],
        [
          "Multi-modality",
          "concept",
          0.5806451612903225
        ],
        [
          "Instruction Output Generation",
          "concept",
          0.5806451612903225
        ]
      ],
      "density": 0.240990990990991,
      "total_communities": 12,
      "levels": 3,
      "communities_per_level": {
        "0": 3,
        "1": 4,
        "2": 5
      },
      "avg_size": 7.833333333333333,
      "min_size": 4,
      "max_size": 16,
      "root_communities": 3,
      "leaf_communities": 5,
      "cached_papers": 0,
      "new_papers": 5
    }
  },
  "synthesis": {
    "themes": [
      {
        "theme": "Instruction Tuning as a Critical Bridge for Task Alignment",
        "description": "This theme covers the fundamental role of instruction tuning in aligning pre-trained LLMs with human intent and specific downstream tasks. It addresses the gap between the model's pre-training objective (next-word prediction) and the user's goal of having the model follow instructions to perform tasks.",
        "paper_ids": [
          "2308.10792",
          "2308.06966",
          "2310.20329"
        ],
        "key_points": [
          "Instruction tuning is identified as a crucial technique for enhancing both model capabilities and controllability by teaching models to follow instructions.",
          "It serves as a bridge between the pre-training objective and user objectives, enabling zero-shot and few-shot generalization to unseen tasks.",
          "The technique is applicable across domains, from general NLP to specialized areas like e-commerce and code editing, demonstrating its versatility as a core fine-tuning paradigm.",
          "Performance improvements are heavily dependent on the quality and construction of the instruction dataset, highlighting the data-centric nature of this approach."
        ]
      },
      {
        "theme": "Dataset Construction Methodologies and Challenges",
        "description": "This theme encompasses the various strategies, innovations, and persistent difficulties in creating high-quality instruction-tuning datasets. It covers both general methodologies and domain-specific adaptations.",
        "paper_ids": [
          "2308.10792",
          "2308.06966",
          "2310.20329"
        ],
        "key_points": [
          "Two primary dataset construction methodologies exist: (1) data integration from existing NLP datasets using templates, and (2) generation using advanced LLMs (e.g., GPT-3.5/4).",
          "Innovative construction strategies are being developed to enhance learning, such as the Chain-of-Task (CoT) method for decomposing complex tasks into atomic sub-tasks to teach foundational skills.",
          "Scalable, iterative pipelines (e.g., using seed data and LLM expansion) are effective for generating large-scale, diverse datasets for niche domains.",
          "A significant challenge is crafting datasets that are high-quality, diverse, and free of biases, with concerns that models may only learn surface-level stylistic patterns rather than deep task comprehension.",
          "There is a tension between scale and quality, with automated generation risking the propagation of errors and biases from the generator model."
        ]
      },
      {
        "theme": "Domain-Specialization through Instruction Tuning",
        "description": "This theme focuses on the application of instruction tuning to adapt general-purpose LLMs to specific domains or verticals, addressing unique challenges that hinder the performance of generic models.",
        "paper_ids": [
          "2308.06966",
          "2310.20329"
        ],
        "key_points": [
          "General-purpose LLMs often underperform in specialized domains due to unique data characteristics (e.g., non-coherent syntax in e-commerce, precise syntax in code).",
          "Domain-specific instruction tuning demonstrably improves performance, enabling models to outperform general counterparts like ChatGPT on in-domain tasks.",
          "Key domain challenges include handling specialized vocabularies, unique syntactic structures, and task formats not well-represented in general pre-training data.",
          "Creating a domain-specific instruction dataset is a foundational step, requiring domain expertise to identify relevant tasks and data sources.",
          "Specialization leads to strong zero-shot and cross-dataset generalization within the target domain, as evidenced by benchmarks and human evaluation."
        ]
      },
      {
        "theme": "Evaluation and Benchmarking for Specialized Capabilities",
        "description": "This theme covers the development and use of tailored benchmarks and evaluation protocols to measure the effectiveness of instruction-tuned models, particularly for specialized tasks where standard NLP benchmarks are insufficient.",
        "paper_ids": [
          "2308.06966",
          "2310.20329"
        ],
        "key_points": [
          "There is a need for novel, domain-specific benchmarks to properly assess specialized capabilities (e.g., EditEval for code editing, custom suites for e-commerce tasks).",
          "High-quality benchmarks are often human-written and execution-based (for code) or involve human evaluation to ensure reliability and challenge.",
          "Evaluation compares model performance against strong proprietary baselines (e.g., ChatGPT) to establish competitive utility.",
          "Benchmarks reveal the limitations of current SOTA models prior to specialized instruction tuning, justifying the need for domain adaptation.",
          "Performance on these benchmarks is used to validate the effectiveness of both the instruction-tuning process and the novel dataset construction methods."
        ]
      },
      {
        "theme": "Open Challenges and Limitations of Instruction Tuning",
        "description": "This theme synthesizes the critical perspectives, identified pitfalls, and unresolved issues associated with the instruction-tuning paradigm, as noted across the surveyed research.",
        "paper_ids": [
          "2308.10792",
          "2308.06966",
          "2310.20329"
        ],
        "key_points": [
          "A major concern is that instruction tuning may lead to 'superficial' learning—capturing stylistic patterns in the instruction data rather than developing deep task comprehension or reasoning.",
          "Performance improvements may be narrow, primarily boosting tasks heavily represented in the training data, questioning the breadth of generalization.",
          "Dataset construction is fraught with challenges: potential biases from expert schemas or generator models, scalability of quality assurance, and the difficulty of achieving true diversity.",
          "There are inherent limitations in benchmark scope (e.g., language/domain restriction) which may not fully capture real-world complexity.",
          "The field lacks comprehensive quantitative synthesis and comparative analysis of different methods, making it difficult to gauge the empirical strength of various approaches."
        ]
      }
    ],
    "gaps": [
      "Lack of mechanistic understanding of how instruction tuning alters model capabilities and representations. While studies show performance improvements, there is limited research on the underlying changes in model reasoning, knowledge organization, or internal representations that enable task generalization versus superficial pattern matching.",
      "Insufficient investigation into the long-term effects and interaction of sequential fine-tuning. The impact of multiple rounds of instruction tuning, domain adaptation, or combining instruction tuning with other alignment methods (RLHF, DPO) on model stability, catastrophic forgetting, and emergent capabilities remains underexplored.",
      "Understudied populations and contexts for domain-specific instruction tuning. Most research focuses on high-resource domains (e-commerce, code) or English. There is limited work on low-resource languages, specialized professional domains (law, medicine beyond QA), and multimodal instruction tuning that integrates non-linguistic reasoning.",
      "Methodological limitations in dataset quality evaluation and contamination detection. Current studies lack standardized metrics for assessing instruction dataset diversity, difficulty, and potential benchmark contamination. The field lacks robust methods to distinguish genuine task understanding from dataset-specific memorization.",
      "Inadequate exploration of efficiency-robustness trade-offs in instruction tuning. While efficient fine-tuning methods exist, their impact on model robustness, adversarial vulnerability, and out-of-distribution generalization compared to full fine-tuning is not systematically studied across domains."
    ],
    "future_directions": [
      "Develop diagnostic frameworks and probing studies to understand instruction tuning's mechanistic effects. Future research should combine performance evaluation with representational analysis, circuit-based interpretability, and controlled experiments to isolate how different instruction types affect model capabilities versus behaviors.",
      "Investigate curriculum-based and sequential instruction tuning protocols. Research should explore optimal ordering of tasks, domains, and difficulty levels during instruction tuning, and develop methods to preserve previously learned skills while adapting to new domains, potentially drawing from continual learning literature.",
      "Expand domain-specific instruction tuning to underserved contexts. Future work should create instruction datasets for low-resource languages, specialized professional reasoning tasks requiring deep domain knowledge, and integrated multimodal tasks requiring joint understanding of text, code, and visual information.",
      "Establish standardized evaluation suites for instruction dataset quality and model generalization. The field needs benchmark datasets specifically designed to test generalization beyond surface patterns, along with metrics for dataset diversity, difficulty progression, and contamination detection that go beyond simple similarity checks.",
      "Systematically study the robustness-efficiency frontier in instruction tuning. Future research should compare different efficient fine-tuning methods (LoRA, adapter layers, prefix tuning) against full fine-tuning across multiple axes: adversarial robustness, out-of-domain generalization, and long-term stability, potentially leading to hybrid approaches."
    ],
    "review_text": "### **A Narrative Review of Large Language Model Fine-Tuning: The Instruction Tuning Paradigm**\n\n**1. INTRODUCTION**\n\nThe advent of large language models (LLMs) has precipitated a paradigm shift in natural language processing, yet a fundamental disconnect persists between their pre-training objective—next-word prediction on vast corpora—and the user-centric goal of executing specific tasks via instruction. Instruction tuning has emerged as a pivotal fine-tuning technique to bridge this alignment gap, transforming raw, capable models into controllable and user-aligned agents. This review synthesizes current literature to examine the methodologies, applications, and critical debates surrounding instruction tuning for LLMs. Focusing on its role as a critical alignment mechanism, the analysis proceeds through themes of dataset construction, domain specialization, evaluation, and inherent limitations. By integrating findings from foundational surveys and domain-specific applications, this review aims to provide a coherent overview of the state of the field, identify consensus and contention, and delineate pathways for future research to address persistent challenges in creating truly generalizable and robust instruction-following models.\n\n**2. THEMATIC ANALYSIS**\n\n**Instruction Tuning as a Critical Bridge for Task Alignment**\nA central consensus across the literature positions instruction tuning as an indispensable technique for aligning LLMs with human intent. As [Instruction Tuning for Large Language Models: A Survey, 2023] articulates, it directly addresses the \"fundamental gap\" between pre-training and user objectives, enhancing both model capabilities and controllability. This alignment function is not merely additive but transformative, enabling the zero-shot and few-shot generalization to unseen tasks that characterizes modern LLM utility. The universality of this paradigm is evidenced by its successful application across diverse domains. For instance, while general surveys outline the core methodology, applied studies demonstrate its efficacy in specialized contexts: [EcomGPT, 2023] utilizes it to adapt a model to the idiosyncratic syntax and vocabulary of e-commerce, and [InstructCoder, 2023] employs it to teach code editing, a task requiring precise syntactic manipulation. This cross-domain applicability underscores instruction tuning’s role as a core fine-tuning paradigm. However, a critical nuance emerges: this alignment is profoundly data-centric. Performance gains are not automatic but heavily contingent on the quality, diversity, and construction methodology of the instruction dataset, a dependency that threads through all subsequent themes and introduces significant challenges.\n\n**Dataset Construction Methodologies and Challenges**\nThe construction of instruction datasets is a primary focus of innovation and a significant source of limitation. The literature delineates two predominant methodologies. The first, data integration from existing NLP datasets via templating (e.g., Flan), is noted for leveraging curated, high-quality annotations but may lack diversity in instruction phrasing [Instruction Tuning for Large Language Models: A Survey, 2023]. The second, generation using advanced LLMs like GPT-3.5/4, offers scalability and linguistic diversity but risks propagating the generator’s biases and errors. Domain-specific studies exemplify sophisticated hybrids of these approaches. [InstructCoder, 2023] employs an iterative, LLM-driven pipeline seeded with real GitHub commits to generate a large-scale dataset for code editing, demonstrating a scalable solution to data scarcity in niche areas. Conversely, [EcomGPT, 2023] introduces the innovative Chain-of-Task (CoT) methodology, which decomposes complex end-tasks (e.g., review summarization) into atomic sub-tasks (e.g., entity extraction, sentiment analysis). This approach is theoretically motivated to instill foundational semantic understanding rather than superficial task-specific patterns, aiming for deeper generalization.\n\nDespite these innovations, significant challenges persist, forming a key area of critical discourse. A major concern, highlighted by the survey, is that instruction tuning may only teach models to capture surface-level stylistic patterns of the instruction-response pairs rather than fostering deep task comprehension or reasoning. This risk is amplified in LLM-generated datasets, where models may learn to mimic the generator’s style. Furthermore, there is an inherent tension between scale and quality. While automated pipelines enable the creation of millions of examples, as in EcomInstruct’s 2.5 million instructions, the processes for quality assurance and bias mitigation are often under-specified, leaving potential noise and unknown biases in the training data [EcomGPT, 2023]; [InstructCoder, 2023]. The field currently lacks standardized metrics for evaluating dataset diversity, difficulty, and freedom from contamination, making comparative assessment of construction methods difficult.\n\n**Domain-Specialization through Instruction Tuning**\nA robust finding across applied studies is that general-purpose LLMs frequently underperform in specialized domains, and instruction tuning is a highly effective remedy. This specialization addresses unique domain challenges that are poorly represented in general pre-training corpora. For example, [EcomGPT, 2023] identifies e-commerce-specific hurdles such as non-coherent, attribute-value pair syntax, a dynamic and specialized vocabulary, and entity-dense short texts. Similarly, the precise, syntactic nature of code editing poses a distinct challenge for models trained primarily on natural language [InstructCoder, 2023]. The foundational step for such specialization is the creation of a domain-specific instruction dataset, which requires substantial domain expertise to curate relevant tasks and data sources.\n\nThe outcome of this process is demonstrably improved performance. EcomGPT, instruction-tuned on the domain-specific EcomInstruct, is shown to outperform the general-purpose ChatGPT on e-commerce tasks in both automated and human evaluations. Likewise, models tuned on the InstructCoder dataset achieve code editing accuracy competitive with advanced proprietary models. Crucially, this specialization does not merely lead to overfitting on a narrow set of tasks; both studies report strong zero-shot and cross-dataset generalization *within* the target domain. This suggests that well-constructed domain-specific instruction tuning can impart a flexible, foundational understanding of the domain’s semantics and tasks, enabling application to unseen but related problems.\n\n**Evaluation and Benchmarking for Specialized Capabilities**\nThe push for domain specialization has necessitated the development of novel, tailored evaluation benchmarks, as standard NLP benchmarks are often insufficient. This trend highlights a maturation in the field’s approach to validation. Studies move beyond reporting accuracy on generic tasks to constructing bespoke, challenging evaluation suites. [InstructCoder, 2023] introduces EditEval, a human-written, execution-based benchmark for code editing, which provides a more reliable measure of functional correctness than syntactic similarity. Similarly, [EcomGPT, 2023] employs a comprehensive suite of e-commerce tasks for evaluation. These benchmarks serve a dual purpose: they rigorously assess the specialized capabilities of the newly tuned models, and they revealingly expose the limitations of state-of-the-art general models prior to tuning, thereby justifying the need for domain adaptation.\n\nThe choice of comparative baseline is another critical aspect of evaluation. A common and pragmatic approach is to benchmark against powerful proprietary models like ChatGPT, establishing a high bar for utility and performance. However, this can present a narrow view if not complemented by comparisons with other open-source, instruction-tuned models. The reliance on a single, closed-source baseline can obscure whether performance gains stem from the novel dataset construction method (e.g., CoT) or simply from the volume of domain-specific data. Furthermore, these domain-specific benchmarks, while valuable, often have their own limitations in scope—such as being restricted to a single programming language or e-commerce platform—which may not capture the full complexity of real-world applications.\n\n**Open Challenges and Limitations of Instruction Tuning**\nBeyond technical and methodological issues, the literature articulates deeper conceptual concerns about the instruction-tuning paradigm. The foremost criticism, as noted in the survey, is the question of whether improvements signify genuine task comprehension or \"superficial\" learning of stylistic patterns. This concern directly challenges the premise of instruction tuning as creating robustly aligned models. Closely related is the observation that performance gains may be narrow, primarily boosting performance on tasks heavily represented in the training data, which questions the breadth and depth of generalization claimed.\n\nMethodologically, the field exhibits several limitations. As a narrative survey points out, there is a notable \"lack of quantitative synthesis and comparative analysis\" [Instruction Tuning for Large Language Models: A Survey, 2023]. Findings are typically presented thematically or as isolated empirical results, without meta-analyses to gauge the strength and prevalence of evidence for different dataset construction or training techniques. Furthermore, the rapid pace of advancement threatens the longevity of specific findings, and the broad scope of surveys can force a surface-level treatment of complex sub-areas like efficient fine-tuning. Finally, the evaluation ecosystem, despite advances, remains fragmented. Benchmarks are often narrow, and there is inadequate exploration of how efficient fine-tuning methods (e.g., LoRA) impact model robustness and out-of-distribution generalization compared to full parameter tuning, a critical consideration for practical deployment.\n\n**3. CRITICAL DISCUSSION**\n\nThe synthesized literature reveals a field characterized by rapid engineering innovation but facing foundational scientific questions. A clear pattern is the transition of instruction tuning from a general alignment technique to a primary tool for domain specialization. The success in domains like e-commerce and code editing demonstrates its potency, yet it simultaneously reinforces the data-centric nature of the approach: performance is inextricably linked to the quality of the curated dataset. This raises a critical methodological consideration. The most impactful studies are those that contribute both a novel model and a significant new dataset or benchmark (e.g., EcomInstruct, InstructCoder/EditEval). However, the methodologies for creating these resources—often involving expert schemas, LLM generation, and manual filtering—are complex and can introduce opaque biases. The lack of standardized protocols for dataset evaluation makes it difficult to compare resources or assess the true source of a model’s performance gains.\n\nA significant tension exists between the demonstrated empirical successes and the theoretical criticisms regarding superficial learning. While models like EcomGPT show strong cross-task generalization, suggesting some depth of understanding, the concern that they may be leveraging dataset-specific patterns rather than developing reasoning remains unresolved. This points to a major limitation in the current body of research: a predominance of outcome-based evaluation (does the model output the correct answer?) over process-based analysis (how does the model arrive at that answer?). Few studies probe the mechanistic changes in model representations or reasoning pathways induced by instruction tuning. Furthermore, the long-term dynamics of sequential fine-tuning—such as the stability of knowledge when a model is successively tuned for multiple domains or combined with reinforcement learning from human feedback (RLHF)—are underexplored, representing a gap in understanding model plasticity and catastrophic forgetting.\n\n**4. GAPS AND FUTURE DIRECTIONS**\n\nThe analysis identifies several key research gaps that warrant future investigation. First, there is a pressing need for mechanistic interpretability studies to understand *how* instruction tuning alters model internals, distinguishing between changes that enable robust reasoning versus those that facilitate pattern matching. Second, the interaction and long-term effects of sequential fine-tuning procedures require systematic study to guide sustainable model development and adaptation. Third, the scope of domain specialization must expand beyond high-resource contexts to include low-resource languages, highly specialized professional domains (e.g., legal drafting, scientific discovery), and integrated multimodal reasoning tasks. Fourth, the field must develop robust, standardized methodologies for evaluating instruction dataset quality, diversity, and for detecting benchmark contamination to improve methodological rigor. Finally, future work should rigorously explore the trade-offs between fine-tuning efficiency (e.g., parameter-efficient methods) and model robustness, adversarial vulnerability, and generalization, ensuring that efficient deployment does not come at the cost of reliable performance.\n\n**5. CONCLUSION**\n\nThis review underscores that instruction tuning has firmly established itself as a cornerstone technique for aligning large language models with human intent and specialized task requirements. It effectively bridges the pre-training and application gap, enabling significant improvements in controllability and zero-shot generalization. The thematic synthesis reveals a vibrant field where innovations in dataset construction, such as Chain-of-Task frameworks and iterative generation pipelines, are driving successful domain specialization, as evidenced in e-commerce and code editing. However, these advances are tempered by persistent and profound challenges: concerns over superficial learning, biases in data generation, a lack of mechanistic understanding, and fragmented evaluation. The significance of these findings lies in their dual nature; they highlight both the remarkable engineering utility of instruction tuning and the substantial scientific questions that remain about how it works and how to ensure its outcomes are robust and generalizable. Moving forward, the field must balance its impressive pace of applied innovation with deeper foundational research to solidify instruction tuning as a reliable and transparent pathway toward truly intelligent, aligned machine behavior.",
    "citations_formatted": [
      "Zhang, S., Dong, L., Li, X., et al. (2023). Instruction tuning for large language models: A survey. *ACM Computing Surveys*. arXiv:2308.10792.",
      "Li, Y., Ma, S., Wang, X., et al. (2023). EcomGPT: Instruction-tuning large language models with chain-of-task tasks for e-commerce. *Proceedings of the AAAI Conference on Artificial Intelligence*. arXiv:2308.06966.",
      "Li, K., Hu, Q., Zhao, X., et al. (2023). InstructCoder: Instruction tuning large language models for code editing. *Proceedings of the Annual Meeting of the Association for Computational Linguistics*. arXiv:2310.20329.",
      "Si, Q., Wang, T., Lin, Z., et al. (2023). An empirical study of instruction-tuning large language models in Chinese. *Proceedings of the Conference on Empirical Methods in Natural Language Processing*. arXiv:2310.07328.",
      "Wu, Z., Dadu, A., Nalls, M. A., et al. (2024). Instruction tuning large language models to understand electronic health records. *Advances in Neural Information Processing Systems*. https://doi.org/10.52202/079017-1737"
    ],
    "word_count": 1934,
    "papers_cited": 5
  },
  "errors": [
    "UNIQUE constraint failed: schema_version.version",
    "UNIQUE constraint failed: schema_version.version"
  ]
}