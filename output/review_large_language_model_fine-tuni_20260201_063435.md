# Literature Review: large language model fine-tuning

### **A Narrative Review of Large Language Model Fine-Tuning: Innovations, Challenges, and Future Trajectories**

The fine-tuning of Large Language Models (LLMs) represents a critical bridge between general-purpose pre-training and specialized, high-performance applications. As the scale of foundational models grows exponentially, the computational cost and methodological complexity of adapting them to downstream tasks have spurred a vibrant field of research. This review synthesizes recent literature to address the central question: **How can we effectively and efficiently adapt large language models to specific tasks and domains while preserving their general capabilities?** The surveyed work reveals a shift from simple full-parameter updates towards sophisticated strategies that are parameter-efficient, data-aware, and stability-preserving. This narrative will explore these developments thematically, analyzing innovations in memory-efficient tuning, techniques to mitigate catastrophic forgetting, data-centric training strategies, domain-specialization pipelines, and the empirical diagnostics that underpin them. It will conclude by critiquing methodological trends and outlining pivotal research gaps that must be addressed to advance the field from a collection of empirical successes to a more principled engineering and scientific discipline.

### **Thematic Analysis**

**Memory and Parameter-Efficient Fine-Tuning (PEFT) Innovations**
A dominant theme in contemporary research is the development of PEFT methods that drastically reduce memory and computational overhead without sacrificing performance. While Low-Rank Adaptation (LoRA) established a popular baseline by updating low-rank matrices injected into model layers, recent innovations challenge the assumption of uniform update importance across parameters. A key diagnostic insight driving new methods is the observation of skewed update distributions. For instance, [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning, 2024] found that during LoRA fine-tuning, weight update norms are consistently concentrated in the bottom and/or top layers of transformer models. This empirical finding motivated LISA’s approach of importance sampling, where most middle layers are randomly frozen during optimization. This simple strategy achieves memory costs comparable to LoRA while reportedly outperforming it on benchmarks like MT-Bench, suggesting that not all layers require simultaneous updating for effective adaptation.

This principle of selective updating extends beyond layers to individual parameters. In the multimodal context, [Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning, 2024] introduces the Parameter Importance Discrepancy (PID) framework, which measures the conflict between a parameter’s importance for upstream (pre-trained) knowledge versus downstream task performance. Their proposed SPIDER method selectively updates parameters by consolidating those with high importance for generalization (measured via pre-trained weight magnitude) and optimizing those important for specialization (measured via accumulated gradient norms). Both LISA and SPIDER exemplify a next generation of PEFT that moves beyond uniform low-rank updates towards more intelligent, empirically-grounded update allocation. The area of agreement here is that the over-parameterized nature of LLMs allows for sparse or selective updates without performance loss, and that identifying the *right* parameters to update is more valuable than simply reducing the update count. A point of contrast lies in the granularity: LISA operates at the layer level, which is coarse but extremely simple, while SPIDER operates at the parameter level, offering finer control but requiring more complex importance metrics.

**Mitigating Catastrophic Forgetting and Balancing Generalization**
Closely related to PEFT is the challenge of catastrophic forgetting, where adapting a model to a new task degrades its performance on original capabilities. This stability-plasticity dilemma has become a central focus, particularly for multimodal LLMs (MLLMs) where aligning visual and textual representations can severely disrupt linguistic knowledge. The literature converges on the view that forgetting is not an inevitable byproduct of fine-tuning but a manageable consequence of parameter interference. The PID concept from [SPIDER, 2024] provides a measurable mechanism for this interference, showing a higher discrepancy for unseen downstream distributions. The proposed solution—partitioning parameters into those to be consolidated for general knowledge and those to be adapted for new tasks—directly addresses this interference.

This theme highlights a critical area of agreement: successful fine-tuning must explicitly balance specialization and generalization. Methods that update all parameters (full fine-tuning) or even a fixed, task-agnostic subset (like standard LoRA adapters) risk excessive interference. The emerging paradigm, as seen in SPIDER, is to make this trade-off explicit and data-driven. The literature suggests that the goal is not merely to remember old tasks but to preserve the core representational and reasoning structures acquired during pre-training. While the discussed papers primarily address this in a two-distribution (upstream vs. one downstream) setting, they lay the groundwork for more complex continual learning scenarios. The emphasis on measuring and mitigating forgetting represents a significant maturation from viewing fine-tuning as a simple task-specific optimization to treating it as a constrained adaptation problem that must respect the model’s foundational knowledge.

**Data-Centric and Sequential Training Strategies**
Beyond *how* to update parameters, a significant body of research investigates *what* data and training sequence lead to superior fine-tuned models. This theme underscores that performance is profoundly influenced by the quality, format, and orchestration of the fine-tuning process itself. [Improving Large Language Model Fine-tuning for Solving Math Problems, 2023] provides a compelling case study, demonstrating that the quality and step-by-step formatting of mathematical solutions in the fine-tuning data have a large impact on final performance. This finding elevates the importance of data curation over mere data scaling for specialized domains.

Furthermore, this work illustrates the power of sequential and multi-task training strategies. Their multi-task sequential fine-tuning, where a model is trained first as a solution generator, then as an evaluator, and finally as a generator again, yields gains over single-stage supervised fine-tuning. This indicates that auxiliary tasks (like binary evaluation) can provide complementary learning signals that refine the model’s primary generative capability. The synergy between fine-tuning and inference-time techniques is another key insight; combining solution-cluster re-ranking with majority voting was found to be more effective and efficient than either alone. These approaches coalesce into complex, holistic “recipes” that treat fine-tuning not as a one-off step but as a multi-faceted pipeline involving data engineering, staged training, and enhanced decoding. The contrast here is with simpler, one-shot fine-tuning approaches, and the literature suggests that for challenging domains like mathematical reasoning, such sophisticated recipes are necessary to achieve state-of-the-art results.

**Domain Specialization and Real-World Application Pipelines**
A practical driver of fine-tuning research is the need to equip LLMs with deep, reliable expertise for professional domains. This theme focuses on building end-to-end pipelines that transition from general models to trustworthy domain-specific tools. [Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning, 2024] exemplifies this, fine-tuning models on corpora extracted from academic papers to automate systematic literature review synthesis. Its significance lies not only in demonstrating successful domain adaptation but in directly confronting the primary barriers to real-world deployment: hallucination and lack of verifiability. The proposed integration of source attribution mechanisms directly addresses the scholarly requirement for auditability.

This application-oriented research highlights a shift in evaluation criteria from benchmark scores to real-world utility and reliability. Validation through the replication of an existing human-conducted PRISMA review provides a tangible, rigorous test of factual accuracy and synthesis capability. A notable broader implication from this work is the call to update human-centric methodological standards (e.g., PRISMA) to govern AI-assisted processes, emphasizing transparency. This theme reveals an area of growing consensus: for high-stakes domains, effective fine-tuning must be part of a larger system that includes guardrails, verification mechanisms, and interpretability tools. The focus expands from optimizing a model’s output to engineering a credible and dependable workflow.

**Empirical Analysis and Diagnosis of Fine-Tuning Dynamics**
Underpinning the methodological advances in the previous themes is a foundation of empirical analysis. Many innovative papers begin with diagnostic studies to uncover the mechanistic behaviors of fine-tuning. The discovery of skewed layer-wise updates in [LISA, 2024] and the quantification of Parameter Importance Discrepancy in [SPIDER, 2024] are prime examples of how empirical observation drives innovation. This theme encompasses research dedicated to understanding *how* and *why* fine-tuning succeeds or fails, providing the evidence base for new algorithms.

A consistent pattern across the literature, however, is a noted gap between strong empirical results and deep theoretical understanding. Multiple papers, including those on LISA and sequential math training, conclude with calls for better mechanistic explanations. Why do middle layers tolerate random freezing? What is the causal relationship between evaluation-task training and improved generation? The current body of work is rich in correlational findings but poorer in causal, theoretical models. This diagnostic theme is also characterized by a methodological strength—comprehensive evaluation across model scales (7B to 70B) and diverse benchmarks—and a corresponding limitation: findings are often tied to specific model families (LLaMA, PaLM) or task types, raising questions about generalizability across architectures like encoder-decoder models or other fine-tuning paradigms like reinforcement learning from human feedback (RLHF).

### **Critical Discussion**

The literature reveals a clear trajectory from monolithic fine-tuning towards modular, intelligent, and constrained adaptation. The prevailing trend is the rejection of the “one-size-fits-all” update in favor of methods that are selective (LISA, SPIDER), sequential [Improving Large Language Model Fine-tuning for Solving Math Problems, 2023], and system-aware [Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning, 2024]. A strong pattern is the reliance on empirical diagnostics to identify inefficiencies (like uniform updates) or problems (like PID) and then to engineer targeted solutions. This has proven highly fruitful, yielding methods that often outperform strong baselines like LoRA.

Methodologically, the field heavily favors empirical, benchmark-driven research. While this has accelerated progress, it introduces considerations. First, the reliance on aggregate benchmark scores (MMLU, MT-Bench) may obscure nuanced failures or capabilities, particularly for interactive or reasoning tasks. Second, the complexity of new “recipes” can threaten reproducibility and obscure which components are truly necessary, as ablated cost-effectiveness analyses are often lacking. Third, there is a noticeable concentration on certain model architectures (decoder-only transformers) and fine-tuning scenarios (instruction-tuning), leaving other important paradigms relatively underexplored. A critical limitation across many studies is the treatment of hyperparameters for new methods; without thorough sensitivity analyses, optimal results may appear as fortuitous configurations rather than robust evidence of a method’s superiority.

### **Gaps and Future Directions**

The reviewed literature points to several critical research gaps. First is the **lack of theoretical foundations** for many empirical successes. Future work must develop stronger theoretical models to explain phenomena like layer-wise importance skew and the efficacy of sequential training, moving the field from heuristic innovation to principled design. Second, the **scope of fine-tuning paradigms is narrow**. Research must expand beyond instruction-tuning to rigorously explore dynamics in continued pre-training, RLHF, preference optimization, and for diverse architectures (encoder-decoder, multimodal beyond vision-language). Third, there is **insufficient investigation into hyperparameter sensitivity and optimization** for new methods, requiring standardized ablation studies and perhaps meta-learning approaches for hyperparameter configuration. Fourth, research often neglects **complex, real-world deployment constraints** such as continual/multi-task learning, streaming data adaptation, and robustness to adversarial inputs or distribution shifts post-deployment. Finally, evaluation must evolve to assess **higher-order cognitive capabilities** like calibration, uncertainty estimation, reasoning faithfulness, and long-form interactive collaboration, which are crucial for trustworthy application.

### **Conclusion**

This review synthesizes a rapidly evolving field centered on the efficient and effective adaptation of large language models. The collective findings underscore that modern fine-tuning is a multifaceted challenge requiring solutions that are parameter-efficient, stability-preserving, data-aware, and integrated into reliable application pipelines. Key insights include the non-uniform importance of parameters, the necessity of explicitly balancing specialization with generalization, and the profound impact of data quality and training orchestration. While the empirical richness of current research has yielded significant performance gains, the path forward demands a deeper theoretical understanding, broader exploration of fine-tuning scenarios, and more rigorous evaluation under realistic constraints. Addressing these challenges will be essential to transition LLM fine-tuning from a specialized engineering practice into a robust and generalizable cornerstone of reliable AI system development.

## References

- Pan, R., Liu, X., Diao, S., et al. (2024). LISA: Layerwise importance sampling for memory-efficient large language model fine-tuning. *Neural Information Processing Systems*. arXiv:2403.17919.
- Liu, Y., Singh, A., Freeman, C. D., et al. (2023). Improving large language model fine-tuning for solving math problems. *arXiv.org*. arXiv:2310.10047.
- Sušnjak, T., Hwang, P., Reyes, N., et al. (2024). Automating research synthesis with domain-specific large language model fine-tuning. *ACM Transactions on Knowledge Discovery from Data*. arXiv:2404.08680.
- Zhai, Y., Tong, S., Li, X., et al. (2024). Investigating the catastrophic forgetting in multimodal large language model fine-tuning. *CPAL*.
- Huang, W., Liang, J., Shi, Z., et al. (2024). Learn from downstream and be yourself in multimodal large language model fine-tuning. *arXiv.org*. arXiv:2411.10928.
- Zou, J., Zhou, M., Li, T., et al. (2024). PromptIntern: Saving inference costs by internalizing recurrent prompt during large language model fine-tuning. *Conference on Empirical Methods in Natural Language Processing*. arXiv:2407.02211.
- Wang, R., Li, H., Wu, M., et al. (2023). Demystifying instruction mixing for fine-tuning large language models. arXiv:2312.10793v3.
- Yu, D., Naik, S., Backurs, A., et al. (2021). Differentially private fine-tuning of language models. arXiv:2110.06500v2.
- Wang, R., Li, H., Han, X., et al. (2024). Learning from failure: Integrating negative examples when fine-tuning large language models as agents. arXiv:2402.11651v2.
- Xu, C., Sun, Q., Zheng, K., et al. (2023). WizardLM: Empowering large pre-trained language models to follow complex instructions. *The Twelfth International Conference on Learning Representations (ICLR 2024)*. arXiv:2304.12244v3.
- Zhang, J., & Bottou, L. (2024). Fine-tuning with very large dropout. arXiv:2403.00946v3.
- Tian, C., Blaschko, M. B., Xing, M., et al. (2025). Large language models reasoning abilities under non-ideal conditions after RL-fine-tuning. arXiv:2508.04848v1.
- He, J., Zhou, C., Ma, X., et al. (2021). Towards a unified view of parameter-efficient transfer learning. arXiv:2110.04366v3.
- Tang, Y., Zhang, R., Guo, Z., et al. (2023). Point-PEFT: Parameter-efficient fine-tuning for 3D pre-trained models. arXiv:2310.03059v8.
- Si, C., Yang, X., & Shen, W. (2024). See further for parameter efficient fine-tuning by standing on the shoulders of decomposition. arXiv:2407.05417v2.
- Shen, Z., Liu, Z., Qin, J., et al. (2021). Partial is better than all: Revisiting fine-tuning strategy for few-shot learning. arXiv:2102.03983v1.
- Metz, Y., Lindner, D., Baur, R., et al. (2023). RLHF-Blender: A configurable interactive interface for learning from diverse human feedback. *ICML2023 Interactive Learning from Implicit Human Feedback Workshop*. arXiv:2308.04332v1.
- Xu, Z., Feng, C., Shao, R., et al. (2024). Vision-Flan: Scaling human-labeled tasks in visual instruction tuning. arXiv:2402.11690v1.
- Becktepe, J., Dierkes, J., Benjamins, C., et al. (2024). ARLBench: Flexible and efficient benchmarking for hyperparameter optimization in reinforcement learning. *17th European Workshop on Reinforcement Learning 2024*. arXiv:2409.18827v1.
- González Barman, K., Lohse, S., & de Regt, H. (2024). Reinforcement learning from human feedback: Whose culture, whose values, whose perspectives? *Philosophy & Technology, 38*(35). arXiv:2407.17482v2.
